---
title: IAT task for wiki-trained word2vec models
author: Molly Lewis 
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
    number_sections: no
    toc: yes
---
  
```{r setup, include = F}
rm(list = ls())

# load packages
library(knitr)
library(rmarkdown)
library(tidyverse)
library(langcog)
library(stringr)
library(forcats)
library(broom)
library(fastrtext)
library(Rcurl)
library(data.table)

opts_chunk$set(echo = T, message = F, warning = F, 
               error = F, tidy = F, cache = F)

source("IAT_utils2.R")
```

```{r}
countries_langs <- read_csv("../../data/other/countries_lang.csv")

translated_wiki_langs <- read_csv("../../data/other/langs_for_study1.csv")
iat_countries<- read_csv("../../data/other/iat_behavior_langs.csv") %>%
  left_join(countries_langs %>% select(country_code, language_code), by = c("countryres"= "country_code"))

iat_countries %>%
 inner_join(translated_wiki_langs) %>%
  count(language_name) %>%
  as.data.frame()
```

Pre-process text
```{r}
translated_words <- read.csv("../../data/models/translations_for_models/Clean - IATLANG STUDY 1 TRANSLATIONS.csv",
                             encoding ='UTF-8') 
# note that there are some arabic words that are not represented correctly - not sure if this is an R terminal issue or a real issue

translated_clean <- translated_words %>%
  mutate(English = ENGLISH) %>%
  gather(language_name, translation, -1) %>%
  left_join(countries_langs %>% select(language_name, language_code)) %>%
  rename(target_word = ENGLISH) %>%
  select(target_word, language_code, translation) %>%
  mutate(translation = trimws(translation),
        translation = tolower(translation),
        translation = str_replace(translation, "\b+", ""),
        translation = str_replace(translation, "/ ", "/"),
        translation = str_replace(translation, " /", "/"))

## gather multiple translations and split words
tidy_translations <- translated_clean %>%
  separate(translation, 
           c("t1", "t2", "t3", "t4", "t5", "t6", "t7"), "/") %>%
  gather("translation_id", "translation", -1:-2) %>%
  separate(translation, 
              c("w1", "w2", "w3", "w4", "w5", "w6", "w7", "w8", "w9"), " ") %>%
  gather("word_id", "translation", -1:-3) %>%
  filter(!is.na(translation))
```

Define test words
```{r}
word_list <- list(test_name = "WEAT_6", # not identical to caliskan (caliskan used proper names)
           bias_type = "gender-bias-career-family",
           category_1 = c("male", "man", "boy", "brother", "he", "him", "his", "son"),
           category_2 = c("female", "woman", "girl", "sister", "she", "her", "hers", "daughter"),
           attribute_1 = c("executive", "management", "professional", "corporation", "salary", 
                                "office", "business", "career"),
           attribute_2 = c("home", "parents", "children", "family", "cousins", "marriage", 
                            "wedding", "relatives"))
```

Loop over languages, get language models, and calculate effect sizes
```{r}
relevant_calculated_vector_path <- "../../data/models/wikipedia/subsetted_career_models/calculated/"
es_file_path <- "../../data/models/wikipedia/subsetted_career_models/career_effect_sizes.csv"

for (i in 1:length(translated_wiki_langs$language_code)){
  
  current_lang <- translated_wiki_langs$language_code[i]
  print(paste0("=====", current_lang, "====="))
  print(Sys.time())
  
  # download model into temp file from online 
  model_path <- paste0("https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.", current_lang, ".vec")
  local_temp_filename <- "temp_model_file.txt"
  f <- CFILE(local_temp_filename, mode = "wb") # wb = binary
  curlPerform(url = model_path, writedata = f@ref)
  close(f)
  
  # read in model from temp_filename
  model <- fread(local_temp_filename,    
                 skip = 1,
                 key = "V1",
                 encoding = "UTF-8",
                 data.table = TRUE)
  
  # delete local file to save memory
  # file.remove(local_temp_filename)

  # get model of the words we care about 
  translated_word_list <- tidy_translations %>%
                              filter(language_code == current_lang) 
  
  # get subsetted model (processing mutliple words, etc) and write to file
  write_subsetted_models(translated_target_words, model, current_lang)
  
  # read back in subsetted file
  subsetted_model <- read_csv(paste0(relevant_calculated_vector_path, 
                  "wiki.", lang_code, "_calculated_career.csv")) %>%
    select(-language_code)
  
  # calculate ES
  ES <- get_ES(word_list, subsetted_model) %>%
    mutate(language_code = lang_code) %>%
    select(language_code, everything())
  
  write_csv(ES, es_file_path, append = TRUE)
}

```