---
output:
  pdf_document: default
  html_document: default
---
```{r}
library(broom)
```
# Study 1: Gender bias and semantics


Are participants' implicit and explicit gender biases predictable from biases found in the semantic structure of their native languages? For example, are the semantics of the words  “woman” and “family” more similar in Spanish than in English? Both the language-as-reflection and language-as-causal hypotheses predict a positive correlation between the measured biases and biases present in language.
 

 
As a model of word meanings, we use large-scale distributional semantics
models derived from auto-encoding neural networks trained on large
text corpora. The underlying assumption of these models is that the
meaning of a word can be described by the words it tends to co-occur
with---words occurring in similar contexts, tend to have similar meanings [@firth1957synopsis]. The word like "dog", for example is represented as more similar to "hound" than to "banana" because
"dog" co-occurs with words more in common with "hound"
than "banana."Recent developments in machine learning allow the idea of distributional semantics to be implemented in a way that takes into account many
features of language structure while remaining computationally
tractable. The best known of these word embedding models is
_word2vec_ [@mikolov2013efficient]. The model takes as input a corpus of text and outputs a vector for each word corresponding to its semantics. From these vectors, we can derive a measure of the semantic similarity between two words by taking the distance between their vectors (e.g., cosine distance). 

As it turns out, many of the biases previously reported using implicit association tests can be predicted from distributional semantics models like word2vec. Caliskan, Bryson, and Narayanan (2017; henceforth _CBN_) measured the distance in vector space between the words presented to participants in the IAT task. CBN found that these distance
measures were highly correlated with reaction times in the behavioral IAT
task. For example,  CBN find a bias to associate males with career and females with family in the career-gender IAT, suggesting that the biases measured by the IAT are also found in the lexical semantics of natural language.

CBN only measured semantic biases in English, however. In Study 1, we
use the method described by CBN to examine whether the gender bias of the participants in the Project Implicit dataset is correlated with the gender bias measured in the dominant languages spoken in the countries of these participants. We begin by validating word embedding measures of gender bias by comparing them to explicit human judgements of word genderness (Study 1a). We then apply this method to models trained on text in other languages (Study 1b). To foreshadow the results, we find that the implicit gender biases reported in Study 1 for individual countries are correlated with the biases found in the distributional semantics of the language spoken in the countries of the participants.


## Study 1a: Word embeddings as a measure of psychological gender bias

To validate word embeddings as a measure of psychological gender bias, we asked whether words that were closely associated with males in the word embedding models tended to be rated by human participants as being more male biased. We found human and word-embedding estimates of gender bias to be highly correlated.

### Methods

```{r}
GENDER_NORMS <- here("data/study1a/raw/GlasgowNorms.csv")
glasgow_norms <- read_csv(GENDER_NORMS) %>%
  rename(maleness_norm = GEND_M)  %>%
  select(word, maleness_norm) %>%
  rowwise() %>%
  mutate(word =  str_split(word, " ", simplify = T)[1],
         word = tolower(word)) %>%
  group_by(word)  %>%
  summarize(maleness_norm = mean(maleness_norm))  # take the mean across multiple sense of word

EMBEDDING_BIAS_SUB  <- here("data/study1a/processed/gender_bias_by_word_english_sub.csv")
EMBEDDING_BIAS_WIKI <- here("data/study1a/processed/gender_bias_by_word_english_wiki.csv")

sub_male_bias <- read_csv(EMBEDDING_BIAS_SUB) %>%
  rename(maleness_embedding_sub = male_score)

wiki_male_bias <- read_csv(EMBEDDING_BIAS_WIKI) %>%
  rename(maleness_embedding_wiki = male_score)

embedding_ratings <- sub_male_bias %>%
  full_join(wiki_male_bias, by = "word") %>%
  select(word, contains("embedding")) %>%
  filter_at(vars(contains("embedding")), any_vars(!is.na(.)))

MALE_WORDS <- c("son", "his","him","he", "brother","boy", "man", "male")
FEMALE_WORDS <- c("daughter", "hers", "her", "she",  "sister", "girl", "woman", "female")

all_ratings <- embedding_ratings %>%
  left_join(glasgow_norms) %>%
  filter(!(word %in% c(MALE_WORDS, FEMALE_WORDS)))
```

We used an existing set of word norms in which participants were asked to rate "the gender associated with each word" on a Likert scale ranging from _very feminine_ (1) to _very masculine_  [7; @scott2018glasgow]. We compared these gender norms to estimates of gender bias  obtained from embedding models pre-trained on two different corpora of English text: Wikipedia [@bojanowski2016enriching] and subtitles from movies and TV shows [@vanparidon;@lison].  The Wikipedia corpus is a large, naturalistic corpus of written language trained using the fastText algorithm [a variant of word2vec; @bojanowski2016enriching;@joulin2016bag]; The subtitle corpus is a smaller corpus of spoken language, trained using the XX algorithm.

To calculate a gender score from the word embeddings, for each word we calculated the average cosine distance to a standard set of male "anchor" words: ("male", "man", "he", "boy", "his", "him", "son", "brother") and the average cosine similarity to a set of female words ("female",  "woman", "she", "girl", "hers", "her", "daughter", "sister"). A gender score for each word was then obtained by taking the difference of the similarity estimates (mean male similarity - mean female similarity), such that larger values indicated a stronger association with males. There were `r format(nrow(all_ratings), big.mark = ",")` words in total that overlapped between the word-embedding models and human ratings.

### Results and Discussion
```{r}
model_gender_score_corr <- cor.test(all_ratings$maleness_embedding_sub,
                              all_ratings$maleness_embedding_wiki) %>%
  tidy()

sub_gender_score_corr <- cor.test(all_ratings$maleness_embedding_sub,
                              all_ratings$maleness_norm) %>%
  tidy()

wiki_gender_score_corr <- cor.test(all_ratings$maleness_embedding_wiki,
                              all_ratings$maleness_norm) %>%
  tidy()

#poly_model <- lm(maleness_embedding ~ poly(maleness_norm,3), all_ratings) %>%
#  summary()
```

Estimates of gender bias from the Subtitle corpus (_M_ = `r round(mean(all_ratings$maleness_embedding_sub),2)`; _SD_=  `r round(sd(all_ratings$maleness_embedding_sub),2)`) and  the Wikipedia corpus (_M_ = `r round(mean(all_ratings$maleness_embedding_wiki),2)`; _SD_ =  `r round(sd(all_ratings$maleness_embedding_wiki),2)`) were highly correlated with each other (_r_ = `r round(model_gender_score_corr$estimate, 2)`; _p_ <  .0001). Critically, bias estimates from both word embedding models were also highly correlated with human judgements (_M_ = `r round(mean(all_ratings$maleness_norm),2)`; _SD_ =  `r round(sd(all_ratings$maleness_norm),2)`; _r_~subtitles~ = `r round(sub_gender_score_corr$estimate, 2)`; _p_ <  .0001; _r_~Wikipedia~ = `r round(wiki_gender_score_corr$estimate, 2)`; _p_ <  .0001; Fig. 1). This suggests that the psychological gender bias of a word can be reasonably estimated from word embeddings. 

```{r, fig.pos = "!t", fig.height = 4, fig.width = 4.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Word estimates of gender bias from the Subtitle-trained embedding model as a function of human judgments of gender bias (Study 1a).  Each point corresponds to a word. Larger numbers indicate stronger association with males. Blue line shows linear fit and the error band corresponds to a standard error (too small to be visible)."}
ggplot(all_ratings, aes(x = maleness_norm, y = maleness_embedding_sub)) +
  geom_point(size = .2, alpha = .3) +
  ggtitle("Gender Association of English Words") +
  xlab("Human judgement of gender association") +
  ylab("Word embedding estimate of gender association") +
  scale_x_continuous(breaks = 1:7, 
                     label = c("1\n(female)", "2", "3", "4", "5", "6", "7\n(male)   ")) +
  scale_y_continuous(breaks = c(-.15, -.1,-.05,  -0, .05, .1, .15), 
                     label = c("-0.15\n(female)","-.1", "-.05" , "0.0", ".05", ".1", ".15\n(male)")) +
 # geom_smooth(method = "lm", 
  #            formula=y ~ poly(x, 3, raw=TRUE)) + #third-order polynomial fit
  geom_smooth(method = "lm", color = "blue") +
  annotate("text", x = 5, y = -.12, label = 
             paste0("r = ", round(sub_gender_score_corr$estimate, 2)), 
           color = "red", size = 4) +
  theme_classic()
```

## Study 1b: Gender bias across languages
Having validated our method, we next turn toward examining the relationship between psychological and linguistic gender biases. In Study 1b, we estimate the magnitude of the gender-career bias in the dominant language spoken in the countries of the Project Implicit participants and compare it with estimates of behavioral gender bias from the Project Implicit data set. 

### Methods
```{r read_in_es_data}
LANG_IAT_PATH <- here("data/study1b/iat_es_lang.csv")
iat_lang_es <- read_csv(LANG_IAT_PATH)

LANG_FAMILY_PATH <- here("data/study0/processed/top_lang_by_country_ethnologue.csv")
lang_family <- read_csv(LANG_FAMILY_PATH) %>%
  select(wiki_language_code, family) %>%
  rename(language_code = "wiki_language_code") %>%
  distinct()

BEHAVIORAL_IAT_PATH <- here("data/study0/processed/by_language_df.csv")
iat_behavioral_es <- read_csv(BEHAVIORAL_IAT_PATH) %>%
  rename(language_code = "wiki_language_code") %>%
  select(language_code, median_country_age, 
         prop_male,log_age, es_iat_sex_age_order_explicit_resid,
         es_iat_sex_age_order_implicit_resid, per_women_stem_2012_2017, n_participants)

# combine lang and behavioral and family info
all_es <- left_join(iat_behavioral_es, iat_lang_es, by = "language_code") %>%
  left_join(lang_family)
```

```{r deal_with_exclusions}
# remove exclusions and fix croatian to be mean of hr and sr (only in wiki)
EXCLUSIONS_PATH <- here("data/study1b/language_exclusions.csv") 
exclusions <- read_csv(EXCLUSIONS_PATH)

hr_new_wiki <- mean(c(filter(iat_lang_es, language_code == "hr") %>%  pull(lang_es_wiki),
         filter(iat_lang_es, language_code == "sr") %>%  pull(lang_es_wiki)))

all_es_tidy <- all_es %>%
  left_join(exclusions) %>%
  mutate(lang_es_wiki = case_when(exclude_wiki == TRUE ~ NA_real_,
                                  TRUE ~ lang_es_wiki),
         lang_es_sub = case_when(exclude_sub == TRUE ~ NA_real_,
                                  TRUE ~ lang_es_sub)) %>%
  select(-exclude_wiki, -exclude_sub) %>%
  mutate(lang_es_wiki = case_when(language_code == "hr" ~ hr_new_wiki,
                                  TRUE ~ lang_es_wiki),
         lang_es_sub = case_when(language_code == "hr" ~ NA_real_, # sr is missing from sub
                                  TRUE ~ lang_es_sub)) 

```

For each country represented in our analysis of the Project Implicit, we identified the most frequently spoken language in each country using Ethnologue [@simons2018]. This included a total of `r nrow(all_es)` unique languages. For each language, we then obtained translations from native speakers for the stimuli in the Project Implicit gender-career IAT behavioral task  [@nosek2002harvesting] with one slight modification. In the behavioral task, proper names were used to cue the male and female categories (e.g.\ "John," "Amy"), but because there are not direct translation equivalents of proper names, we instead used a set of generic gendered words which had been previously used for a different version of the gender IAT [e.g., "man," "woman;" @nosek2002harvesting]. Our linguistic stimuli were therefore a set of 8 female and 8 male Target Words (identical to Study 1a), a set of 8 Attribute Words associated with the concept "career" ("career," "executive,"    "management,"   "professional," "corporation,"  "salary," "office," "business") and 8 Attribute Words associated with the concept "family" ("family," "home,"      "parents,"   "children,"  "cousins,"   "marriage,"  "wedding," "relatives"). For one language, Tagalog, we were unable to obtain translations from a native speaker, and so translations were gathered from several translations sources. All analyses remain the same when this language is excluded [check this]. 

We used these translations to calculate a gender bias effect size from word embedding models trained on text in each language. Our effect size measure is a standardized difference score of the relative similarity of the target words to the target attributes (i.e.\ relative similarity of male to career vs.\ relative similarity of female to career). Our effect size measure is identical to that used by CBN with an exception for grammatically gendered languages (see SM for replication of CBN on our corpora). Namely, for languages with grammatically gendered Attribute Words (e.g., ninas for female children in Spanish), we calculated the relationship between target words and attribute words of the same gender (i.e.\ "hombre" (man) to "ni&ntilde;os" and "mujer" (woman) to "ni&ntilde;as"). In cases where there were multiple translations for a word, we averaged across words such that each of our target words was associated with a single vector in each language. In cases where the translation contained multiple words, we used the entry for the multiword phrase in the model, when present, and averaged across words otherwise. Like the behavioral effect size from the Project Implicit data, larger values indicate larger gender bias.

We calculated gender bias estimates from word-embedding models that had been trained on texts Wikipedia [@bojanowski2016enriching] and subtitles from movies and TV shows [@vanparidon;@lison, as in Study 1a] in each of our target languages.  We excluded languages from the analysis for which 20% or more of the target words were missing from the model or the model did not exist (see SM for more details). This lead us to exclude one language (Zulu) from the analysis of the Wikipedia corpus and six languages from the analysis of the Subtitle corpus (Chinese, Croatian, Hindi, Japanese, Tagalog, and Zulu).  Our final sample included `r all_es_tidy %>% filter(!is.na(lang_es_wiki)|  !is.na(lang_es_sub)) %>% nrow()` languages in total (_N_~Wikipedia~ = `r filter(all_es_tidy, !is.na(lang_es_wiki)) %>% nrow()`; _N_~Subtitle~ = `r filter(all_es_tidy, !is.na(lang_es_sub)) %>% nrow()`), representing `r  length(unique(all_es_tidy$family))` different language families. We then compared estimates of linguistic gender bias for each language from each model to  the  behavioral IAT gender bias estimated from Project Implicit, averaging across countries whose participants speak the same language.

As before, we included two country level variables in our analysis, percentage of women in STEM fields and median country age. To obtain language-level estimates of these variables, we took the mean across countries whose participants speak the same primary language.

### Results

```{r, overall_lang_bias_by_source}
corpus_effect_t_test <- t.test(all_es_tidy$lang_es_sub, 
                               all_es_tidy$lang_es_wiki, 
                               paired = T)
```


```{r, fig.pos = "!t!", fig.height = 4, fig.cap = "Residualized behavioral IAT gender bias as a function of linguistic gender bias, with each point corresponding to a language (Study 1b). Linguistic biases are estimated from models trained on text in each language from a subtitle corpus (left) and a Wikipedia corpus (right). Larger values indicate a larger bias to associate men with the concept of career and men with the concept of family. Error bands indicate standard error of the model estimate."}

# plot lang vs behavioral
all_es_tidy %>%
  select(language_code, lang_es_sub, lang_es_wiki,
         es_iat_sex_age_order_implicit_resid, n_participants) %>%
  gather("model", "lang_es", -language_code,
         -es_iat_sex_age_order_implicit_resid, -n_participants) %>%
  mutate(model = fct_recode(model, "Subtitle Embeddings" = "lang_es_sub",
                                   "Wikipedia Embeddings" = "lang_es_wiki")) %>%
  ggplot(aes(x = lang_es, y = es_iat_sex_age_order_implicit_resid)) +
  facet_wrap( . ~ model) +
  scale_x_continuous(breaks = c( -.5, -0, .5, 1), 
                     label = c("-.5\n(male-\nfamily)", "0", ".5","1\n(male-\ncareer)") , limits = c(-.6, 1.1)) +
  scale_y_continuous(breaks = c(-.075, -.05, -.025, 0, .025, .05), 
                     label = c("-.075\n(male-\nfamily)", "-.05", "-.025", "0", ".025", ".05\n(male-\ncareer)") , limits = c(-.08, .06) ) +
  geom_smooth(method = "lm", alpha = .1, size = .9) +
  ggrepel::geom_text_repel(aes(label = language_code), 
                           size = 3, box.padding = 0.1) + 
  geom_point(size = .7) + #aes(size  = log(n_participants))) + #
  theme_minimal() +
  ggtitle("Behavioral and Linguistic Gender Biases") +
  ylab("Behavioral IAT Gender Bias (residualized)\n") +
  xlab("\nLinguistic Gender Bias\n (effect size)") +
  theme_classic(base_size = 12) 
```


```{r get_corrs}
# corr of lanng, behavioral, etc.
all_corr_vars <- all_es_tidy %>%
  select(lang_es_sub, lang_es_wiki, es_iat_sex_age_order_explicit_resid, 
         es_iat_sex_age_order_implicit_resid, per_women_stem_2012_2017, median_country_age) %>%
  rename(`Residualized Behavioral IAT` = "es_iat_sex_age_order_implicit_resid",
          `Residualized Explicit Bias` = "es_iat_sex_age_order_explicit_resid",
          `Language IAT (Subtitles)` = "lang_es_sub",
          `Language IAT (Wikipedia)` = "lang_es_wiki",
          `Percent Women in STEM` = "per_women_stem_2012_2017",
          `Median Country Age` = "median_country_age") 

simple_corr <- psych::corr.test(all_corr_vars, adjust = "none")$r %>%
  as_tibble(rownames = "rowname") %>%
  gather("var2", "simple_r", -rowname)
  
simple_corr_p <- psych::corr.test(all_corr_vars, adjust = "none")$p %>%
  as_tibble(rownames = "rowname") %>%
  gather("var2", "simple_p", -rowname)
  
partial_psych_obj <- psych::partial.r(data = all_corr_vars, x = 1:5, y = 6) 
partial_corr <- psych::corr.p(partial_psych_obj, n = nrow(all_corr_vars) - 1, 
                              adjust = "none")$r %>%
  psych_to_mat() %>%
  as_tibble(rownames = "rowname") %>%
  gather("var2", "partial_r", -rowname)

partial_corr_p <- psych::corr.p(partial_psych_obj, n = nrow(all_corr_vars) - 1, 
                                adjust = "none")$p %>%
  psych_to_mat() %>%
  as_tibble(rownames = "rowname") %>%
  gather("var2", "partial_p", -rowname)

tidy_corrs <- simple_corr %>%
                left_join(simple_corr_p) %>%
                left_join(partial_corr) %>%
                left_join(partial_corr_p) 
```

```{r tidy_corrs_for_text}
text_tidy_corrs <- tidy_corrs %>%
  filter(rowname != var2) %>%
  mutate_at(vars(simple_r, partial_r, simple_p, partial_p), ~ round(.,2)) %>%
  rowwise() %>%
  mutate(r_equality_sign = case_when(simple_p < .01 ~ ", _p_ < .01", 
                                     TRUE ~ paste0(", _p_ = ", simple_p)),
          partial_equality_sign = case_when(partial_p < .01 ~  ", _p_ < .01", 
                                     TRUE ~ paste0(", _p_ = ", partial_p)),
          r_print_text = paste0("_r_ = ", simple_r , r_equality_sign),
          partial_print_text = paste0("_r_ = ", partial_r , partial_equality_sign)) %>%
    mutate_if(is.numeric, ~str_remove(as.character(.), "^0+|^-0+")) 

L_B_sub <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Subtitles)", var2 == "Residualized Behavioral IAT") %>%
  pull(r_print_text)

L_B_wiki <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Wikipedia)", var2 == "Residualized Behavioral IAT") %>%
  pull(r_print_text)

L_B_sub_partial <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Subtitles)", var2 == "Residualized Behavioral IAT") %>%
  pull(partial_print_text)

L_B_wiki_partial <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Wikipedia)", var2 == "Residualized Behavioral IAT") %>%
  pull(partial_print_text)

L_E_sub <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Subtitles)", var2 == "Residualized Explicit Bias") %>%
  pull(r_print_text)

L_E_wiki <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Wikipedia)", var2 == "Residualized Explicit Bias") %>%
  pull(r_print_text)

L_S_sub <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Subtitles)", var2 == "Percent Women in STEM") %>%
  pull(r_print_text)

L_S_wiki <- text_tidy_corrs %>%
  filter(rowname ==  "Language IAT (Wikipedia)", var2 == "Percent Women in STEM") %>%
  pull(r_print_text)
```

There was no overall difference in the estimates of gender bias between models trained on the subtitle corpora versus the Wikipedia corpora (_t_(`r pluck(corpus_effect_t_test$parameter, "df")`) =  `r pluck(corpus_effect_t_test$statistic, "t")`, _p_ =  `r corpus_effect_t_test$p.value`). We next asked about the relationship between  estimates of gender bias for each language and implicit gender bias of participants from countries where that language was dominant (and, we assume, was the native language of most of these individuals). Implicit gender bias were positively correlated with estimates of language bias from both Subtitles (`r L_B_sub`) and Wikipedia (`r L_B_wiki`; Fig. 2, Table 1 shows the language-level correlations between all variables). The relationship between behavioral bias and language bias remained reliable after partialling out the effect of median country age  (Subtitles: `r L_B_sub_partial`; Wikipedia: `r L_B_wiki_partial`). Linguistic gender bias was not correlated with explicit gender bias (Subtitles: `r L_E_sub`; Wikipedia: `r L_E_wiki`). Finally, estimates of language bias from the Subtitles corpus were correlated with the objective measure of gender equality, percentage of women in STEM fields (`r L_S_sub`), though this relationship was not reliable for the Wikipedia corpus (`r L_S_wiki`). <!-- do we want to weight by model size, see repo history for analyses in this file -->

```{r, include = F}
print_tidy_corrs <- tidy_corrs %>%
  filter(rowname != var2) %>%
  mutate_at(vars(simple_p, partial_p), ~ case_when(
    . < .01 ~ "**", . < .05 ~ "*",  . < .1 ~ "+", TRUE ~ "")) %>%
  mutate_at(vars(simple_r, partial_r), ~ round(.,2)) %>%
  mutate_if(is.numeric, ~str_remove(as.character(.), "^0+|^-0+")) %>%
  mutate(partial_print = case_when(
    !is.na(partial_r) ~ paste0(" (", partial_r, partial_p, ")"),TRUE ~ ""),
    r_print = paste0(simple_r, simple_p, partial_print)) %>%
  select(rowname, var2, r_print)

tidy_corrs_to_print <- print_tidy_corrs %>%
  spread(var2, r_print)  %>%
  mutate_all(funs(replace_na(., ""))) %>%
  select("rowname", contains("IAT"), 
         contains("Residualized"), 
         contains("STEM"), contains("Age")) %>%
  rename(" " = "rowname")

tidy_corrs_to_print_reordered <- tidy_corrs_to_print[c(1,2,5,6,4,3),]

kable(tidy_corrs_to_print_reordered, "latex", booktabs = T, escape = F,
      caption = "Correlation (Pearson's r) for all measures in Study 1 at the level of languages. Numbers in parentheses show partial correlations controlling for median country age. Single astericks indicate p < .05 and double astericks indicate p < .01. The + symbol indicates a marginally significant p-value, p < .1.",
      align = "lrrrrrr") %>%
  column_spec(2:7, width = "1.6cm") %>%
  kable_styling(bootstrap_options = "condensed", 
                full_width = F,  font_size = 7)

```

### Discussion 

Study 1 suggests that t
