```{r}
library(corrr)
```

# Study 1: Gender bias and semantics


Are participants' implicit and explicit gender biases predictable from biases found in the semantic structure of their native languages? For example, are the semantics of the words  “woman” and “family” more similar in Spanish than in English? Both the language-as-reflection and language-as-causal hypotheses predict a positive correlation between the measured biases and biases present in language.
 

 
As a model of word meanings, we use large-scale distributional semantics
models derived from auto-encoding neural networks trained on large
text corpora. The underlying assumption of these models is that the
meaning of a word can be described by the words it tends to co-occur
with---words occurring in similar contexts, tend to have similar meanings [@firth1957synopsis]. The word like "dog", for example is represented as more similar to "hound" than to "banana" because
"dog" co-occurs with words more in common with "hound"
than "banana."Recent developments in machine learning allow the idea of distributional semantics to be implemented in a way that takes into account many
features of language structure while remaining computationally
tractable. The best known of these word embedding models is
_word2vec_ [@mikolov2013efficient]. The model takes as input a corpus of text and outputs a vector for each word corresponding to its semantics. From these vectors, we can derive a measure of the semantic similarity between two words by taking the distance between their vectors (e.g., cosine distance). 

As it turns out, many of the biases previously reported using implicit association tests can be predicted from distributional semantics models like word2vec. Caliskan, Bryson, and Narayanan (2017; henceforth _CBN_) measured the distance in vector space between the words presented to participants in the IAT task. CBN found that these distance
measures were highly correlated with reaction times in the behavioral IAT
task. For example,  CBN find a bias to associate males with career and females with family in the career-gender IAT, suggesting that the biases measured by the IAT are also found in the lexical semantics of natural language.

CBN only measured semantic biases in English, however. In Study 1, we
use the method described by CBN to examine whether the gender bias of the participants in the Project Implicit dataset is correlated with the gender bias measured in the dominant languages spoken in the countries of these participants. We begin by validating word embedding measures of gender bias by comparing them to explicit human judgements of word genderness (Study 1a). We then apply this method to models trained on text in other languages (Study 1b). To foreshadow the results, we find that the implicit gender biases reported in Study 1 for individual countries are correlated with the biases found in the distributional semantics of the language spoken in the countries of the participants.


## Study 1a: Word embeddings as a measure of psychological gender bias

To validate word embeddings as a measure of psychological gender bias, we asked whether words that were closely associated with males in the word embedding models tended to be rated by human participants as being more male biased. We found human and word-embedding estimates of gender bias to be highly correlated.

### Methods

```{r}
GENDER_NORMS <- here("data/study1a/raw/GlasgowNorms.csv")
glasgow_norms <- read_csv(GENDER_NORMS) %>%
  select(word, GEND_M) %>%
  rename(maleness_norm = GEND_M)  %>% # take the mean across multiple sense of word 
  rowwise() %>%
  mutate(word =  str_split(word, " ", simplify = T)[1],
         word = tolower(word)) %>%
  distinct() %>%
  group_by(word)  %>%
  summarize(maleness_norm = mean(maleness_norm))

EMBEDDING_BIAS  <- here("data/study1a/processed/embedding_gender_bias_average_method.csv")
embedding_ratings <- read_csv(EMBEDDING_BIAS) %>%
  select(word, male_score) %>%
  rename(maleness_embedding = male_score) %>%
  filter(!is.na(maleness_embedding))

MALE_WORDS <- c("son", "his","him","he", "brother","boy", "man", "male")
FEMALE_WORDS <- c("daughter", "hers", "her", "she",  "sister", "girl", "woman", "female")

all_ratings <- embedding_ratings %>%
  left_join(glasgow_norms) %>%
  filter(!(word %in% c(MALE_WORDS, FEMALE_WORDS)))
```

We used an existing set of word norms in which participants were asked to rate "the gender associated with each word" on a Likert scale ranging from _very feminine_ (1) to _very masculine_  [7; @scott2018glasgow]. We compared these gender norms to estimates of gender bias from a word embedding model pre-trained on the corpus of English Wikipedia using the fastText algorithm [a variant of word2vec; @bojanowski2016enriching;@joulin2016bag]. Each word in the model is represented by a 300 dimensional vector. To calculate a gender score from the word embeddings, for each word we calculated the average cosine distance to a standard set of male "anchor" words: ("male", "man", "he", "boy", "his", "him", "son",  "brother") and the average cosine similarity to a set of female words ("female",  "woman", "she", "girl", "hers", "her", "daughter", "sister"). A gender score for each word was then obtained by taking the difference of the similarity estimates (mean male similarity - mean female similarity), such that larger values indicated a stronger association with males. There were `r nrow(all_ratings)` words in total that overlapped between the two data sources.


### Results and Discussion
```{r}
gender_score_corr <- cor.test(all_ratings$maleness_embedding,
                              all_ratings$maleness_norm) %>%
  tidy()

poly_model <- lm(maleness_embedding ~ poly(maleness_norm,3), all_ratings) %>%
  summary()
```

Estimates of gender bias from word embeddings (_M_ = `r round(mean(all_ratings$maleness_embedding),2)`; _SD_ =  `r round(sd(all_ratings$maleness_embedding),2)`) and human judgements (_M_ = `r round(mean(all_ratings$maleness_norm),2)`; _SD_ =  `r round(sd(all_ratings$maleness_norm),2)`) were highly correlated (_r_ = `r round(gender_score_corr$estimate, 2)`; _p_ <  .0001; Fig. 1). This suggests that the psychological gender bias of a word can be reasonably estimated from word embeddings. 

```{r, fig.pos = "!t", fig.height = 4, fig.width = 4.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Word embedding estimates of gender bias as a function of human judgments of gender bias (Study 1a).  Each point corresponds to a word. Larger numbers indicate stronger association with males. Blue line shows linear fit and the error band corresponds to a standard error (too small to be visible)."}
ggplot(all_ratings, aes(x = maleness_norm, y = maleness_embedding)) +
  geom_point(size = .2, alpha = .3) +
  ggtitle("Word Gender Association") +
  xlab("Human judgement of gender association") +
  ylab("Word embedding estimate of gender association") +
  scale_x_continuous(breaks = 1:7, 
                     label = c("1\n(female)", "2", "3", "4", "5", "6", "7\n(male)   ")) +
  scale_y_continuous(breaks = c(-.2, -.1, -0, .1), 
                     label = c("-0.2\n(female)", "-0.1", "0.0"," -0.1\n(male)")) +
 # geom_smooth(method = "lm", 
  #            formula=y ~ poly(x, 3, raw=TRUE)) + #third-order polynomial fit
  geom_smooth(method = "lm", color = "blue") +
  annotate("text", x = 5, y = -.15, label = paste0("r = ", round(gender_score_corr$estimate, 2)), 
           color = "red", size = 4) +
  theme_classic()
```

## Study 1b: Gender bias across languages
Having validated our method, we next turn toward examining the relationship between psychological and linguistic gender biases. In Study 1b, we estimate the magnitude of the gender-career bias in the dominant language spoken in the countries of the Project Implicit participants and compare it with estimates of behavioral gender bias from the Project Implicit data set. 


### Methods

```{r}
LANG_IAT_PATH <- here("data/study1b/iat_es_lang.csv")
iat_lang_es <- read_csv(LANG_IAT_PATH)

LANGKEY_PATH <- here("data/study0/processed/lang_name_to_wiki_iso.csv")
lang_key <- read_csv(LANGKEY_PATH)

BEHAVIORAL_IAT_PATH <- here("data/study0/processed/by_language_df.csv")
iat_behavioral_es <- read_csv(BEHAVIORAL_IAT_PATH) %>%
  rename(language_code = "wiki_language_code") %>%
  select(language_code, median_country_age, 
         prop_male,log_age, es_iat_sex_age_order_explicit_resid,
         es_iat_sex_age_order_implicit_resid, per_women_stem, n_participants)


# fix croatian
hr_new <- mean(c(filter(iat_lang_es, language_code == "hr") %>%  pull(lang_es_wiki),
       filter(iat_lang_es, language_code == "sr") %>%  pull(lang_es_wiki)))
all_es <- left_join(iat_behavioral_es, iat_lang_es, by = "language_code") %>%
  mutate(lang_es_wiki = case_when(language_code == "hr" ~ hr_new,
                                  TRUE ~ lang_es_wiki))
```

For each country represented in our analysis of the Project Implicit, we identified the most frequently spoken language in each country using Ethnologue [@simons2018]. This included a total of `r nrow(all_es)` unique languages. For two languages, Zulu and Tagalog, the corpora that the models were trained on (see below) were too small to be reliable, and so we excluded these languages from our analysis. Our final sample included `r nrow(all_es)-2` languages, representing XX different language families.

For each language, we then obtained translations from native speakers for the stimuli in the Project Implicit gender-career IAT behavioral task  [@nosek2002harvesting] with one slight modification. In the behavioral task, proper names were used to cue the male and female categories (e.g.\ "John," "Amy"), but because there are not direct translation equivalents of proper names, we instead used a set of generic gendered words which had been previously used for a different version of the gender IAT [e.g., "man," "woman;" @nosek2002harvesting]. Our linguistic stimuli were therefore a set of 8 female and 8 male Target Words (identical to Study 1a), a set of 8 Attribute Words associated with the concept "career" ("career," "executive,"    "management,"   "professional," "corporation,"  "salary," "office," "business") and 8 Attribute Words associated with the concept "family" ("family," "home,"      "parents,"   "children,"  "cousins,"   "marriage,"  "wedding," "relatives"). 

We used these translations to calculate a gender bias effect size from word embedding models trained on text in each language. Our effect size measure is a standardized difference score of the relative similarity of the target words to the target attributes (i.e.\ relative similarity of male to career vs.\ relative similarity of female to career). Our effect size measure is identical to that used by CBN with one exception (see SM for replication of CBN on our corpora). Namely, for languages with grammatically gendered Attribute Words (e.g., ninas for female children in Spanish), we calculated the relationship between target words and attribute words of the same gender (i.e. "hombre" (man) to "ninos" and "mujer" (woman) to "ninas"). In cases where there were multiple translations for a word, or the translation contained multiple words, we averaged across words such that each of our target words was associated with a single vector in each language. Our effect size measure is  analogous to the to the behavioral effect size measure obtained from the IAT task in Project Implicit, where larger values indicate larger gender bias.

We calculated gender bias estimates based on models trained on two different corpora: Wikipedia [@bojanowski2016enriching] and Subtitles [where are these from?]. These two models capture different types of XX. We then compared the effect size of the linguistic gender bias to  the  behavioral IAT gender bias estimated from Project Implicit, averaging across countries that speak whose participants speak the same language.

### Results

```{r, fig.pos = "!t!", fig.height = 4, fig.cap = "Residualized behavioral IAT gender bias as a function of linguistic gender bias, with each point corresponding to a language (Study 1b). Linguistic biases are estimated from models trained on each language using a subtitle corpus (left) and a sample of Wikipedia (right). Larger values indicate a larger bias to associate men with the concept of career and women with the concept of family. Error bands indicate standard error of the model estimate."}

# plot lang vs behavioral
all_es %>%
  select(language_code, lang_es_sub, lang_es_wiki,
         es_iat_sex_age_order_implicit_resid, n_participants) %>%
  gather("model", "lang_es", -language_code, -es_iat_sex_age_order_implicit_resid, -n_participants) %>%
  mutate(model = fct_recode(model, "Subtitle Embeddings" =  "lang_es_sub",
                                   "Wikipedia Embeddings" =  "lang_es_wiki")) %>%
  ggplot(aes(x = lang_es, y = es_iat_sex_age_order_implicit_resid)) +
  facet_wrap(.~ model) +
  geom_smooth(method = "lm", alpha = .1, size = .9) +
  ggrepel::geom_text_repel(aes(label = language_code), 
                           size = 3, box.padding = 0.1) + 
  geom_point(size = .7) + #aes(size  = log(n_participants))) + #
  theme_minimal() +
  ggtitle("Behavioral and Linguistic Gender Biases") +
  ylab("Residualized Behavioral IAT Gender Bias") +
  xlab("Linguistic Gender Bias\n (effect size)") +
  theme_classic() 
```


```{r}
implicit_corr <- cor.test(all_es$es_iat_sex_age_order_implicit_resid,
                              all_es$lang_es_wiki) %>%
  tidy()

implicit_controlled <- lm(es_iat_sex_age_order_implicit_resid ~ lang_es_wiki + median_country_age, data = all_es) %>%
  tidy()

lang_beta <- implicit_controlled %>%
  filter(term == "lang_es_wiki") %>%
  pull(estimate) %>%
  round(2)

lang_se <- implicit_controlled %>%
  filter(term == "lang_es_wiki") %>%
  pull(std.error) %>%
  round(2)

lang_p <- implicit_controlled %>%
  filter(term == "lang_es_wiki") %>%
  pull(p.value) %>%
  round(2)

explicit_corr <- cor.test(all_es$es_iat_sex_age_order_explicit_resid,
                              all_es$lang_es_wiki) %>%
  tidy()
```


The estimate of gender bias for each language was positively correlated the implicit gender bias of participants from countries where that language was dominant (and, we assume, was the native language of most of these individuals; _r_ = `r round(implicit_corr$estimate, 2)`; _p_ =  `r round(implicit_corr$p.value, 2)`; Fig. 2). This relationship held when controlling for median country age in a linear model ($\beta$= `r lang_beta`; _SE_ = `r lang_se`, _p_ = `r lang_p`). Linguistic gender bias was not correlated with explicit gender bias (_r_ = `r round(explicit_corr$estimate, 2)`; _p_ =  `r round(explicit_corr$p.value, 2)`). [Percentage women in stem and language are marginal p = .06]. Table 1 shows the language-level correlations between all variables.


```{r corrtable, results="asis", tab.env = "table", cache = F}
# corr of lanng, behavioral, etc.
all_vars <- all_es %>%
  select(lang_es_sub, lang_es_wiki, es_iat_sex_age_order_explicit_resid, 
         es_iat_sex_age_order_implicit_resid, per_women_stem) %>%
  rename(`Residualized Behavioral IAT` = "es_iat_sex_age_order_implicit_resid",
          `Residualized Explicit Bias` = "es_iat_sex_age_order_explicit_resid",
          `Language IAT (Subtitles)` = "lang_es_sub",
          `Language IAT (Wikipedia)` = "lang_es_wiki",
          `Percent Women in Stem` = "per_women_stem") 

tidy_table <- glrstab(all_vars)

kable(tidy_table, "latex", booktabs = T, escape = F,
      caption = "Correlation (Pearson's r) for all measures in Study 1 at the level of languages. Asterisks indicate significance at the .05 level.",
      col.names = c("", "Linguistic Gender Bias \n(Subtitles)",
              "Linguistic Gender Bias \n (Wikipedia)",
              "Explicit Gender Bias \n (residualized)",
              "Behavioral Gender Bias \n(residualized IAT score)"),
      align = "lrrrr") %>%
  column_spec(2:5, width = "2.5cm") %>%
  kable_styling(bootstrap_options = "condensed", 
                full_width = F,  font_size = 10)

```


```{r, eval = F}
# exploreing weighting by file size
tidy_language <- all_es %>%
  select(language_code, lang_es_sub, lang_es_wiki,
         es_iat_sex_age_order_implicit_resid,
         es_iat_sex_age_order_explicit_resid,
         median_country_age,
         per_women_stem) %>%
  gather("source", "lang_bias", c(-1, -4:-7)) %>%
  rowwise() %>%
  mutate(source = str_split(source, "es_")[[1]][2]) %>%
  filter(!is.na(lang_bias)) %>%
  rename(wiki_language_code = language_code)

SIZE_FILE <- here("data/study1b/language_weights.csv") 
sizes <- read_csv(SIZE_FILE)
all_df <- tidy_language %>%
  left_join(sizes) 


wiki = all_df %>%
  filter(source == "wiki")

wiki2 = all_df %>%
  filter(source == "wiki") %>%
  filter(!(wiki_language_code %in% c("hi", "tl", "zh" , "zu")))

sub = all_df %>%
  filter(source == "sub")


# sub
lm(es_iat_sex_age_order_implicit_resid ~ lang_bias, data = sub) %>%
  summary()

lm(es_iat_sex_age_order_implicit_resid ~ lang_bias, data = sub,
   weights = file_size) %>%
  summary()

lm(es_iat_sex_age_order_implicit_resid ~ lang_bias  + median_country_age, data = sub, weights = file_size) %>%
  summary()

# wiki
lm(es_iat_sex_age_order_implicit_resid ~ lang_bias, data = wiki) %>%
  summary()

lm(es_iat_sex_age_order_implicit_resid ~ lang_bias, data = wiki,
   weights = file_size) %>%
  summary()

lm(es_iat_sex_age_order_implicit_resid ~ lang_bias  + median_country_age, data = wiki, weights = file_size) %>%
  summary()

# per_women_stem

lm(per_women_stem ~ lang_bias, data = sub) %>%
  summary()

lm(per_women_stem ~ lang_bias, data = sub, weights = file_size) %>%
  summary()

lm(per_women_stem ~ lang_bias + median_country_age, data = sub, weights = file_size) %>%
  summary()

lm(per_women_stem ~ lang_bias, data = wiki) %>%
  summary()

lm(per_women_stem ~ lang_bias, data = wiki, weights = file_size) %>%
  summary()


# median_country_age
lm(median_country_age ~ lang_bias, data = sub, weights = file_size) %>%
  summary()

lm(median_country_age ~ lang_bias, data = wiki, weights = file_size) %>%
  summary()





```
### Discussion 
