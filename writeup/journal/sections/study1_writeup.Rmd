```{r}
library(corrr)
```

# Study 1: Gender bias and semantics

In Study 1, we ask whether participants' implicit and explicit gender biases are correlated with the biases in the semantic structure of their native languages. For example, are the semantics of the words “woman” and “family” more similar in Spanish than in English? Both the language-as-reflection and language-as-causal hypotheses predict  a  positive correlation between psychological and semantic gender biases. 

As a model of word meanings, we use large-scale distributional semantics models derived from auto-encoding neural networks trained on large corpora of text. The underlying assumption of these models is that the meaning of a word can be described by the words it tends to co-occur with---an approach known as distributional semantics 
[@firth1957synopsis]. Under this approach, a word like "dog" is represented as more similar to "hound" than to "banana" because "dog" co-occurs with words more in common with "hound" than "banana."

Recent developments in machine learning allow the idea of distributional
semantics to be implemented in a way that takes into account many
features of local language structure while remaining computationally
tractable. The best known of these word embedding models is
_word2vec_ [@mikolov2013efficient]. The model takes as input a corpus of text and outputs a vector for each word corresponding to its semantics. From these vectors, we can derive a measure of the semantic similarity between two words by taking the distance between their vectors (e.g., cosine distance). 

As it turns out, the biases previously reported using IAT tests can be
predicted from distributional semantics models like word2vec using materials identical to those used in the IAT experiments. Caliskan, Bryson, and Narayanan (2017; henceforth _CBN_) measured the distance in vector space between the words presented to participants in the IAT task. CBN found that these distance
measures were highly correlated with reaction times in the behavioral IAT
task. For example,  CBN find a bias to associate males with career and females with family in the career-gender IAT, suggesting that the biases measured by the IAT are also found in the lexical semantics of natural language.

CBN only measured semantic biases in English, however. In Study 1, we use the method described by CBN to measure gender bias in the range of first languages spoken by  participants in the Project Implicit dataset by using models trained on those languages.  In Study 1a, we first validate word embedding measures of gender bias by comparing them to explicit human judgements of gender bias. In Study1b, we apply this method to models trained on text in other languages. We find that the implicit gender biases reported in Study 1 for individual countries are correlated with the biases found in the semantics of the natural language spoken by those participants.

## Study 1a: Word embeddings as a measure of psychological gender bias

To validate word embeddings as a measure of psychological gender bias, we asked whether words that were closely associated with males in the word embedding models tended to be rated by human participants as being more male biased. We found human and word-embedding estimates of gender bias to be highly correlated.

### Methods

```{r}
GENDER_NORMS <- here("data/study1a/raw/GlasgowNorms.csv")
glasgow_norms <- read_csv(GENDER_NORMS) %>%
  select(word, GEND_M) %>%
  rename(maleness_norm = GEND_M)  %>% # take the mean across multiple sense of word 
  rowwise() %>%
  mutate(word =  str_split(word, " ", simplify = T)[1],
         word = tolower(word)) %>%
  distinct() %>%
  group_by(word)  %>%
  summarize(maleness_norm = mean(maleness_norm))

EMBEDDING_BIAS  <- here("data/study1a/processed/embedding_gender_bias_average_method.csv")
embedding_ratings <- read_csv(EMBEDDING_BIAS) %>%
  select(word, male_score) %>%
  rename(maleness_embedding = male_score) %>%
  filter(!is.na(maleness_embedding))

MALE_WORDS <- c("son", "his","him","he", "brother","boy", "man", "male")
FEMALE_WORDS <- c("daughter", "hers", "her", "she",  "sister", "girl", "woman", "female")

all_ratings <- embedding_ratings %>%
  left_join(glasgow_norms) %>%
  filter(!(word %in% c(MALE_WORDS, FEMALE_WORDS)))
```

We used an existing set of word norms in which participants were asked to rate "the gender associated with each word" on a Likert scale ranging from _very feminine_ (1) to _very masculine_  [7; @scott2018glasgow]. We compared these gender norms to estimates of gender bias from a word embedding model pre-trained on the corpus of English Wikipedia using the fastText algorithm [a variant of word2vec; @bojanowski2016enriching;@joulin2016bag]. The model contains 2,519,370 words with each word represented by a 300 dimensional vector. To calculate a gender scores from the word embeddings, for each word we calculated the average cosine distance to a set of male words ("male", "man", "he", "boy", "his", "him", "son",  "brother") and the average cosine similarity to a set of female words ("female",  "woman", "she", "girl", "hers", "her", "daughter", "sister"). A gender score for each word was then obtained by taking the difference of the similarity estimates (mean male similarity - mean female similarity), such that larger values indicated a stronger association with males. There were `r nrow(all_ratings)` words in total that overlapped between the two data sources.


### Results and Discussion
```{r}
gender_score_corr <- cor.test(all_ratings$maleness_embedding,
                              all_ratings$maleness_norm) %>%
  tidy()

poly_model <- lm(maleness_embedding ~ poly(maleness_norm,3), all_ratings) %>%
  summary()
```

Estimates of gender bias from word embeddings (_M_ = `r round(mean(all_ratings$maleness_embedding),2)`; _SD_ =  `r round(sd(all_ratings$maleness_embedding),2)`) and human judgements (_M_ = `r round(mean(all_ratings$maleness_norm),2)`; _SD_ =  `r round(sd(all_ratings$maleness_norm),2)`) were highly correlated (_r_ = `r round(gender_score_corr$estimate, 2)`; _p_ <  .0001; Fig. 2). 

```{r, fig.pos = "!t", fig.height = 4, fig.width = 4.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Word embedding estimates of gender bias as a function of human judgements of gender bias (Study 1a).  Each point corresponds to a word. Larger numbers indicate stronger association with males. Blue line shows linear fit and the error band corresponds to a standard error (too small to be visible)."}
ggplot(all_ratings, aes(x = maleness_norm, y = maleness_embedding)) +
  geom_point(size = .2, alpha = .3) +
  ggtitle("Word Gender Association") +
  xlab("Human judgement of gender association") +
  ylab("Semantic gender associaiton") +
  scale_x_continuous(breaks = 1:7, 
                     label = c("1\n(female)", "2", "3", "4", "5", "6", "7\n(male)   ")) +
  scale_y_continuous(breaks = c(-.2, -.1, -0, .1), 
                     label = c("-0.2\n(female)", "-0.1", "0.0"," -0.1\n(male)")) +
 # geom_smooth(method = "lm", 
  #            formula=y ~ poly(x, 3, raw=TRUE)) + #third-order polynomial fit
  geom_smooth(method = "lm", color = "blue") +
  annotate("text", x = 5, y = -.15, label = paste0("r = ", round(gender_score_corr$estimate, 2)), 
           color = "red", size = 4) +
  theme_classic()
```

## Study 1b: Cross-linguistic gender bias semantics
With our corpus validated, we next turn toward examining the relationship between psychological and linguistic gender biases. In Study 1b, we estimate the magnitude of the gender-career bias in each of the languages spoken in the countries described in the Project Implicit dataset and compare it with estimates of behavioral gender bias from the Project Implicit data set.  If language plays a causal role in shaping psychological gender biases, we predict that participants who speak a language with larger gender bias will tend to have a larger psychological gender bias. 

### Methods

```{r}
LANG_IAT_PATH <- here("data/study1b/iat_es_lang.csv")
iat_lang_es <- read_csv(LANG_IAT_PATH)

LANGKEY_PATH <- here("data/study0/processed/lang_name_to_wiki_iso.csv")
lang_key <- read_csv(LANGKEY_PATH)

BEHAVIORAL_IAT_PATH <- here("data/study0/processed/by_language_df.csv")
iat_behavioral_es <- read_csv(BEHAVIORAL_IAT_PATH) %>%
  rename(language_code = "wiki_language_code") %>%
  select(language_code, median_country_age, 
         prop_male,log_age, es_iat_sex_age_order_explicit_resid,
         es_iat_sex_age_order_implicit_resid, per_women_stem, n_participants)

all_es <- left_join(iat_behavioral_es, iat_lang_es, by = "language_code")

# missing: hi (hindi), hr (Croation), tl (Tagalog; small, can't find speaker?), zu (zulu: small?; can't find speaker)
```

```{r, eval = F}

# move this to get_top_lang_by_country
LANG_TO_COUNTRY_PATH <- here("data/study0/processed/top_lang_by_country.csv")

lang_to_country <- read_csv(LANG_TO_COUNTRY_PATH) %>%
  distinct(language_name) %>%
  filter(!(language_name %in% c("IsiZulu", "Tagalog"))) %>%
  mutate(language_name = fct_recode(language_name, 
                                    "Standard Arabic" = "Arabic",
                                    "Western Farsi" = "Persian",
                                    "Mandarin Chinese" = "Chinese",
                                    "Modern Greek" = "Greek",
                                    "Modern Hebrew"= "Hebrew",
                                    "Mandarin Chinese" = "Mandarin",
                                    "Standard Malay" = "Malaysian")) %>%
  pull(language_name)

lingtypology::aff.lang(lang_to_country) %>%
  str_split(", ") %>%
  map(~.[1]) %>%
  unlist()
```


For each country represented in our analysis of the Project Implicit, we identified the most frequently spoken language in each country using the CIA factbook (2017). This included a total of `r nrow(all_es)` unique languages. For two languages, Zulu and Tagalog, the corpora that the models were trained on (see below) was insufficiently large to be reliable, and so we excluded these languages from our analysis. Our final sample included `r nrow(all_es)-2` languages, representing XX different language families..

We obtained translations from native speakers into each these languages for a set of 8 female and 8 male target words (identical to Study 1a), a set of 8 attribute words associated with the concept "career" ("career," "executive,"    "management,"   "professional," "corporation,"  "salary," "office," "business") and 8 attribute words associated with the concept "family" ("family," "home,"      "parents,"   "children,"  "cousins,"   "marriage,"  "wedding," "relatives"). Our word lists are identical to the stimuli in the Project Implicit gender-career IAT behavioral task  [@nosek2002harvesting] with one slight modification. In the behavioral task, proper names were used to cue the male and female categories (e.g.\ "John," "Amy"), but because there are not direct translation equivalents of proper names, we instead used a set of generic gendered words which had been previously used for a different version of the gender IAT [e.g., "man," "woman;" @nosek2002harvesting].

We used these translations to calculate a gender bias effect size from word embedding models trained on text in each language. Our effect size measure is a standardized difference score of the relative similarity of the target words to the target attributes (i.e.\ relative similarity of male to career vs.\ relative similarity of female to career). Our effect size measure is identical to that used by CBN with one exception (see SM for replication of CBN on our corpora). Namely, for languages with grammatically gendered attribute words (e.g., ninas for female children in Spanish), we calculated the relationship between target words and attribute words of the same gender (i.e. "man" to "ninos" and "woman" to "ninas"). In cases where there were multiple translations for a word, or the translation contained multiple words, we averaged across words such that each of our target words was associated with a single vector in each language. Our effect size measure is  analogous to the to the behavioral effect size measure obtained from the IAT task in Project Implicit, where larger values indicate larger gender bias.

We calculated gender bias estimates based on models trained on two different corpora: Wikipedia [@bojanowski2016enriching] and Subtitles [where are these from?]. These two models capture different types of XX. We then compared the effect size of the linguistic gender bias to  the  behavioral IAT gender bias estimated from Project Implicit, averaging across countries that speak whose participants speak the same language.

### Results

```{r, fig.pos = "!t!", fig.height = 4, fig.cap = "Residualized behavioral IAT gender bias as a function of semantic IAT gender bias by language (Study 1b). Semantic biases are estimated from models trained on each language using a subtitle corpus (left) and a sample of Wikipedia (right). Larger values indicate a larger bias to associate men with the concept of career and women with the concept of family. Error bands correspond to standard errors. "}

# plot lang vs behavioral
all_es %>%
  select(language_code, lang_es_sub, lang_es_wiki,
         es_iat_sex_age_order_implicit_resid, n_participants) %>%
  gather("model", "lang_es", -language_code, -es_iat_sex_age_order_implicit_resid, -n_participants) %>%
  mutate(model = fct_recode(model, "Subtitle Embeddings" =  "lang_es_sub",
                                   "Wikipedia Embeddings" =  "lang_es_wiki")) %>%
  ggplot(aes(x = lang_es, y = es_iat_sex_age_order_implicit_resid)) +
  facet_wrap(.~ model) +
  geom_smooth(method = "lm", alpha = .1, size = .9) +
  ggrepel::geom_text_repel(aes(label = language_code), 
                           size = 3, box.padding = 0.1) + 
  geom_point(size = .7) + #aes(size  = log(n_participants))) + #
  theme_minimal() +
  ggtitle("Behavioral and Semantic Gender Bias") +
  ylab("Residualized Behavioral IAT Gender Bias") +
  xlab("Semantic IAT Gender Bias\n (effect size)") +
  theme_classic() 
```


```{r}
implicit_corr <- cor.test(all_es$es_iat_sex_age_order_implicit_resid,
                              all_es$lang_es_wiki) %>%
  tidy()

implicit_controlled <- lm(es_iat_sex_age_order_implicit_resid ~ lang_es_wiki + median_country_age, data = all_es) %>%
  tidy()

lang_beta <- implicit_controlled %>%
  filter(term == "lang_es_wiki") %>%
  pull(estimate) %>%
  round(2)

lang_se <- implicit_controlled %>%
  filter(term == "lang_es_wiki") %>%
  pull(std.error) %>%
  round(2)

lang_p <- implicit_controlled %>%
  filter(term == "lang_es_wiki") %>%
  pull(p.value) %>%
  round(2)

explicit_corr <- cor.test(all_es$es_iat_sex_age_order_explicit_resid,
                              all_es$lang_es_wiki) %>%
  tidy()
```

At the level of languages, our estimate of gender bias in the semantics of each language was positively correlated the implicit gender bias of participants who spoke that language (_r_ = `r round(implicit_corr$estimate, 2)`; _p_ =  `r round(implicit_corr$p.value, 2)`; Fig. 3). Importantly, this relationship held when controling for median country age in a linear model ($\beta$= `r lang_beta`; _SE_ = `r lang_se`, _p_ = `r lang_p`). Semantic IAT gender bias was not correlated with explicit gender bias (_r_ = `r round(explicit_corr$estimate, 2)`; _p_ =  `r round(explicit_corr$p.value, 2)`). [Percentage women in stem and language are marginal p = .06]. Table 1 shows the language-level correlations between all variables.

```{r corrtable, results="asis", tab.env = "table", cache = F}
# corr of lanng, behavioral, etc.
all_vars <- all_es %>%
  select(lang_es_sub, lang_es_wiki, es_iat_sex_age_order_explicit_resid, 
         es_iat_sex_age_order_implicit_resid, per_women_stem) %>%
  rename(`Residualized Behavioral IAT` = "es_iat_sex_age_order_implicit_resid",
          `Residualized Explicit Bias` = "es_iat_sex_age_order_explicit_resid",
          `Language IAT (Subtitles)` = "lang_es_sub",
          `Language IAT (Wikipedia)` = "lang_es_wiki",
          `Perecent Women in Stem` = "per_women_stem") 

tidy_table <- glrstab(all_vars)

kable(tidy_table, "latex", booktabs = T, escape = F,
      caption = "Correlation (Pearson's r) for all measures in Study 1 at the level of languages. Asterisks indicate significance at the .05 level.",
      col.names = c("", "Language IAT\n(Subtitles)",
              "Language IAT \n (Wikipedia)",
              "Residualized \nExplicit Bias",
              "Residualized \nBehavioral IAT"),
      align = "lrrrr") %>%
  column_spec(2:5, width = "2.5cm") %>%
  kable_styling(bootstrap_options = "condensed", 
                full_width = F,  font_size = 10)

```

### Discussion 
