\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
\usepackage{supertabular}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{upgreek}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{What are we learning from language? Associations between gender biases
and distributional semantics in 25 languages}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,b,1]{Molly Lewis}
\author[b,1,2]{Gary Lupyan} 

\affil[a]{University of Chicago}
\affil[b]{University of Wisconsin-Madison}

% Please give the surname of the lead author for the running footer
\leadauthor{Lewis} 

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the Significance Statement is to explain the relevance of the work in broad context to a broad readership. The Significance Statement appears in the paper itself and is required for all research papers.}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{Please declare any conflict of interest here.}
\equalauthors{\textsuperscript{1}A.O.(Author One) and A.T. (Author Two)...}
\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: mollyllewis@gmail.com}

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
\keywords{cultural stereotypes $|$ implicit association task (IAT) $|$ gender } 

\begin{abstract}
Cultural stereotypes such as the idea that men are more suited for paid
work while women for taking care of the home and family may contribute
to gender imbalances in STEM fields (e.g., Leslie, Cimpian, Meyer, \&
Freeland, 2015) and other undesirable gender disparities. Here, we test
the hypothesis that word co-occurrence statistics (e.g., the
co-occurrence of ``nurse'' with ``she'') play a causal role in the
formation of the men-career/women-family stereotype. We use word
embedding models to measure bias in the distributional statistics of 25
languages and find that languages with larger biases tend to have
speakers with larger implicit biases (\emph{N} = 657,335). These biases
are further related to the extent that languages mark gender in their
lexical forms (e.g., ``waiter''/``waitress'') hinting that linguistic
biases may be causally related to biases shown in people's implicit
judgments.

\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.
\dropcap{B}y the time children are two years old, they have begun to acquire the
gender stereotypes in their culture (Gelman et al., 2004). These
stereotypes can have undesirable effects. For example, in one study,
6-year-old girls were less likely than boys to choose activities that
were described as for children \enquote{who are very, very smart} and
also less likely to think of themselves as \enquote{brilliant} (Bian,
Leslie, \& Cimpian, 2017). Such beliefs may, over time, translate to the
observed lower rates of female participation in STEM fields (Ceci \&
Williams, 2011; Leslie, Cimpian, Meyer, \& Freeland, 2015; Miller,
Eagly, \& Linn, 2015; Stoet \& Geary, 2018). For this reason and others,
it is important to better understand how cultural stereotypes are
formed.

We can distinguish between two major sources of information on which
gender stereotypes may be based. The first is direct experience. For
example, one may observe that most nurses are women and most
philosophers are men and conclude that women are better suited for
nursing and men for philosophy. The second is language. Even without any
direct experience with nurses or philosophers, one may learn about their
stereotypical gender from language about nurses and philosophers.
Languages encode gender in multiple ways. These include gender-specific
titles (\enquote{Mr}. vs. \enquote{Miss}), proper names (\enquote{Sam}
vs. \enquote{Ashley}), pronouns (\enquote{he} vs. \enquote{she}),
certain job titles (\enquote{waiter} vs. \enquote{waitress}), and
higher-order linguistic associations (otherwise gender-neutral words can
become gendered by being associated with explicitly gendered contexts).
Another source of linguistic information comes from sex-based
grammatical gender systems found in approximately 30\% of languages
(Dryer \& Haspelmath, 2013). For example, in Spanish, the gender of a
nurse must be specified grammatically (\enquote{enfermer\emph{a}} vs.
\enquote{enfermer\emph{o}}).

To the extent that language is a source of information for forming
cultural stereotypes, two people with similar direct experiences, but
different linguistic experiences, may develop different stereotypes.
Some past work hints at people's surprising sensitivity to
stereotype-relevant information delivered through language. Young
children perform worse in a game if they are told that someone of the
opposite gender performed better than they did on a previous round
(Rhodes \& Brickman, 2008), or merely told that the game is associated
with a particular gender (Cimpian, Mu, \& Erickson, 2012). In some
cases, a subtle turn of phrase can influence children's gender-based
generalization (Cimpian \& Markman, 2011; Rhodes, Leslie, Yee, \&
Saunders, 2019). For example, Cimpian and Markman found that children
were more likely to infer that a novel skill is stereotypical of a
gender if the skill is introduced with a generic as opposed a
non-generic subject (``{[}Girls are/There is a girl who is{]} really
good at a game called \enquote{gorp''}). Such work shows that in certain
experimental settings, language can influence stereotype formation. We
were interested in whether it actually does, and by what means.

A widely used method for quantifying cultural stereotypes at an
individual level is the \emph{Implicit Association Test} (IAT;
Greenwald, McGhee, \& Schwartz, 1998). Here, we use previously
administered IATs designed to measure a particular type of gender
stereotype: A bias to associate men with careers and women with family
(\emph{N} = 657,335; Nosek, et al., 2002). These data span native
speakers of 25 languages allowing us to assess how performance varies
with properties of languages.

Discovering that linguistic bias predicts people's implicit biases can
be interpreted in at least two ways. The first is that some cultures
have stronger stereotypes and these are reflected in what people talk
about. Language, on this view, simply \emph{reflects} pre-existing
biases. We refer to this as the \emph{language as reflection}
hypothesis. However, language may not simply reflect pre-existing
biases, but may also provide a distinct source of information for
learning about these stereotypes. We refer to this second possibility
that language exerts a causal influence on people's biases as the
\emph{language as causal factor} hypothesis.

In Study 1, we examine whether language-derived gender biases predict
responses on the gender-career IAT. Our analysis focuses on the
\emph{distributional} structure of language rather than the specifics of
the communicated content. In Study 2, we examine how the psychological
biases measured by the IAT and the linguistic biases we measure relate
to more structural aspects of language: sex-based grammatical gender and
the prevalence of gender-specific occupation terms (e.g.,
\enquote{waiter}/\enquote{waitress} but
\enquote{teacher}/\enquote{teacher}). The results of Study 2 suggest
that language not only reflects existing gender biases, but may play a
causal role in shaping them.

\section*{Description of Cross-Cultural Dataset of Psychological Gender
Bias}\label{description-of-cross-cultural-dataset-of-psychological-gender-bias}

\subsection*{Materials and Methods}\label{materials-and-methods}

To quantify cross-cultural gender bias, we used data from a large-scale
administration of an Implicit Association Task (IAT; Greenwald et al.,
1998) by Project Implicit (\url{https://implicit.harvard.edu/implicit/};
Nosek, Banaji, \& Greenwald, 2002). The IAT measures the strength of
respondents' implicit associations between two pairs of concepts (e.g.,
male-career/female-family vs.~male-family/female-career) accessed via
words (e.g., \enquote{man,} \enquote{business}). The underlying
assumption of the IAT is that words denoting more similar meanings
should be easier to pair together compared to more dissimilar pairs.

Meanings are paired in the task by assigning them to the same response
keys in a two-alternative forced-choice categorization task. In the
critical blocks of the task, meanings are assigned to keys in a way that
is either bias-congruent (i.e.~Key A = male/career; Key B =
female/family) or bias-incongruent (i.e.~Key A = male/family; Key B =
female/career). Participants are then presented with a word related to
one of the four concepts and asked to classify it as quickly as possible
(see Study 1b Methods for list of target words). Slower reaction times
in the bias-incongruent blocks relative to the bias-congruent blocks are
interpreted as indicating an implicit association between the
corresponding concepts (i.e.~a bias to associate male with career and
female with family).

We analyzed gender-career IAT scores collected by Project Implicit
between 2005 and 2016, restricting our sample based on participants'
reaction times and error rates using the same criteria described in
Nosek, Banjai, and Greenwald (2002, pg.~104). We only analyzed data for
countries that had complete demographic information and complete data
from the IAT for least 400 participants (2\% of these respondents did
not give responses to the explicit bias question). This cutoff was
arbitrary, but the pattern of findings reported here holds for a range
of minimum participant values (see
SM\footnote{SM available here: \url{https://mollylewis.shinyapps.io/iatlang_SI/}; All data and code available here: \url{https://github.com/mllewis/IATLANG}}).
Our final sample included 657,335 participants from 39 countries, with a
median of 1,145 participants per country. Importantly, although the
respondents were from largely non-English speaking countries, the IAT
was conducted in English. We do not have language background data from
the participants, but we assume that a large fraction of the respondents
from non-English speaking countries were native speakers of the dominant
language of the country and second language speakers of English. The
fact that the test was administered in English lowers the prior
likelihood of finding language-specific predictors of the kind we report
here.

To quantify participants' performance on the IAT we adopt the widely
used \emph{D-score}, which measures the difference between critical
blocks for each participant while controlling for individual differences
in response time (Greenwald, Nosek, \& Banaji, 2003). After completing
the IAT, participants were asked \enquote{How strongly do you associate
the following with males and females?} for both the words
\enquote{career} and \enquote{family.} Participants indicated their
response on a Likert scale ranging from \emph{female} (1) to \emph{male}
(7). We calculated an explicit gender-career bias score for each
participant as the Career response minus the Family response, such that
greater values indicate a greater bias to associate males with career.

\subsection*{Results}\label{results}

At the participant level, implicit bias scores were positively
correlated with participant age (\emph{r} = 0.06, \emph{p} \textless{}
.0001). Male participants (\emph{M} = 0.32, \emph{SD} = 0.37) had a
significantly smaller implicit gender bias than female participants
(\emph{M} = 0.41, \emph{SD} = 0.35; \emph{t} = 96.82, \emph{p}
\textless{} .0001), a pattern consistent with previous findings (Nosek
et al., 2002). Implicit bias scores were larger for participants that
received the block of trials with bias-incongruent mappings first
relative to the opposite order (\emph{t} = -104.03, \emph{p} \textless{}
.0001).

Because we did not have language information at the participant level,
in the remaining analyses we examine gender bias and its predictors at
the country level. To account for the above-mentioned influences on
implicit bias, we calculated a residual implicit bias score for each
participant, controlling for participant age, participant sex, and block
order. We also calculated a residual explicit bias score controlling for
the same set of variables. We then averaged across participants to
estimate the country-level gender bias (Implicit: \emph{M} = -0.01;
\emph{SD} = 0.03; Explicit: \emph{M} = 0.00; \emph{SD} = 0.18). Implicit
gender biases were moderately correlated with explicit gender biases at
the level of participants (\emph{r} = 0.16, \emph{p} \textless{} .0001)
but not countries (\emph{r} = 0.26, \emph{p} = 0.12).

Do the implicit and explicit biases measured by the Project Implicit
dataset predict any real world outcomes? We compared our residual
country-level implicit and explicit gender biases to a gender equality
metric reported by the United Nations Educational, Scientific and
Cultural Organization (UNESCO) for each country: the percentage of women
among STEM graduates in tertiary education from 2012 to 2017 (Miller et
al., 2015; Stoet \& Geary, 2018; available here:
\url{http://data.uis.unesco.org/}). These data were available for 33 out
of 39 of the countries in our sample. Consistent with previous research
(Miller et al., 2015), we found that implicit gender bias was negatively
correlated with percentage of women in STEM fields: Countries with
smaller gender biases tended to have more women in STEM fields (\emph{r}
= -0.54, \emph{p} \textless{} .001). In contrast, there was no
relationship between the percentage of women in STEM fields and the
explicit gender bias measure used by Project Implicit (\emph{r} = 0.14,
\emph{p} = 0.43). In addition, we found a strong correlation between the
median age of each country's population (as reported by the CIA
factbook, 2017) and the residual implicit bias (in which participant age
was held constant): Countries with older populations tended to have
larger gender biases (\emph{r} = 0.64, \emph{p} \textless{} .0001).

In sum, we replicate previously-reported patterns of gender bias in the
gender-career IAT literature, with roughly comparable effect sizes
(c.f.~Nosek, et al., 2002). The weak correlation between implicit and
explicit measures is consistent with claims that these two measures tap
into different cognitive constructs (Forscher et al., 2016). In
addition, we find that an objective measure of gender equality---female
enrollment in STEM fields---is associated with implicit gender bias. The
finding that older participants show stronger biases may stem from a
cohort effect, but it is not obvious why there is a strong positive
association between the median age of a country's population and a
larger implicit bias when adjusting for the age of individual
participants.

\section*{Study 1: Relating gender biases in distributional semantics and
human
behavior}\label{study-1-relating-gender-biases-in-distributional-semantics-and-human-behavior}

Are participants' gender biases predictable from the language they
speak? Both the language-as-reflection and language-as-causal-factor
hypotheses predict a positive correlation between the two, but showing
that such a relationship exists is the first step to investigating a
possible causal link. We begin by validating word embedding measures of
gender bias by comparing them to explicit human judgements of word
genderness (Study 1a). We then apply this method to models trained on
text in other languages (Study 1b). We find that the implicit gender
bias of participants in a country is correlated with the gender bias in
the language spoken in that country.

\subsection*{Study 1a: Word embeddings as a measure of psychological
gender
bias}\label{study-1a-word-embeddings-as-a-measure-of-psychological-gender-bias}

\subsubsection*{Methods}\label{methods}

To model word meanings, we use semantic embeddings derived from a model
that learns meanings by trying to predict a word from surrounding words,
given a large corpus. The core assumption of these models is that the
meaning of a word can be described by the words it tends to co-occur
with---words occurring in similar contexts, tend to have similar
meanings (Firth, 1957). A word like \enquote{dog,} for example is
represented as more similar to \enquote{cat} and \enquote{hound} than to
\enquote{banana} because \enquote{dog} co-occurs with words more in
common with \enquote{cat} and \enquote{hound} than with \enquote{banana}
(Landauer \& Dumais, 1997; Lund \& Burgess, 1996). Recent developments
in machine learning allow the idea of distributional semantics to be
implemented in a way that takes into account many features of language
structure while remaining computationally tractable. The best known of
these word embedding models is \emph{word2vec} (Mikolov, Chen, Corrado,
\& Dean, 2013). By attempting to predict the words that surround another
word, the model is able to learn a vector-based representation for each
word that represents its similarity to other words, i.e., a semantic
embedding. We can then compute the similarity between two words by
taking the distance between their vectors (e.g., cosine of angle).

In order to validate word embeddings as a measure of psychological
gender bias, we used an existing set of word norms in which participants
were asked to rate \enquote{the gender associated with each word} on a
Likert scale ranging from \emph{very feminine} (1) to \emph{very
masculine} (7; Scott, Keitel, Becirspahic, Yao, \& Sereno, 2018). We
compared these norms to estimates of gender bias obtained from embedding
models pre-trained on two different corpora of English text: Wikipedia
(Bojanowski, Grave, Joulin, \& Mikolov, 2016) and subtitles from movies
and TV shows (Lison \& Tiedemann, 2016; Van Paridon \& Thompson, in
prep.). The Wikipedia corpus is a large, naturalistic corpus of written
language; the Subtitle corpus is a smaller corpus of spoken language.
Both models were trained using the fastText algorithm (a variant of
word2vec; Joulin, Grave, Bojanowski, \& Mikolov, 2016). There were 4,671
words in total that overlapped between the word-embedding models and
human ratings.

Using the word embeddings, we calculated an estimate of gender bias for
each word by measuring the average cosine distance to a standard set of
male \enquote{anchor} words (\enquote{male,} \enquote{man,}
\enquote{he,} \enquote{boy,} \enquote{his,} \enquote{him,}
\enquote{son,} and \enquote{brother}; Nosek, Banaji, \& Greenwald, 2002)
and the average cosine similarity to a set of female words
(\enquote{female,} \enquote{woman,} \enquote{she,} \enquote{girl,}
\enquote{hers,} \enquote{her,} \enquote{daughter,} and
\enquote{sister}). A gender score for each word was then obtained by
taking the difference of the similarity estimates (mean male similarity
- mean female similarity), such that larger values indicated a stronger
association with males.

\subsubsection*{Results}\label{results-1}

\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{iat_lang_files/figure-latex/unnamed-chunk-10-1.pdf}
\caption{\label{fig:unnamed-chunk-10}Human judgements of word gender bias as
a function of gender bias from the Subtitle-trained embedding model
(Study 1a). Each point corresponds to a word. Larger numbers indicate
stronger association with females (note that this differs from the
design of the rating task, but is changed here for consistency with
other plots). Blue line shows linear fit and the error band indicates
standard error (too small to be visible).}
\end{figure}

Estimates of gender bias from the Subtitle corpus (\emph{M} = 0.01;
\emph{SD}= 0.03) and the Wikipedia corpus (\emph{M} = 0; \emph{SD} =
0.03) were highly correlated with each other (\emph{r} = 0.71; \emph{p}
\textless{} .0001). Critically, bias estimates from both word embedding
models were also highly correlated with human judgements (\emph{M} =
4.10; \emph{SD} = 0.92; \emph{r}\textsubscript{Subtitle} = 0.63;
\emph{p} \textless{} .0001; \emph{r}\textsubscript{Wikipedia} = 0.59;
\emph{p} \textless{} .0001; Fig. 1). This suggests that the
psychological gender bias of a word can be reasonably estimated from
word embeddings.

\subsection*{Study 1b: Gender bias across
languages}\label{study-1b-gender-bias-across-languages}

Having validated our method, we now use it to examine the relationship
between psychological and linguistic gender biases. In Study 1b, we
estimate the magnitude of the linguistic bias in the dominant language
spoken in each country represented in the Project Implicit dataset, and
compare this estimate to estimates of psychological gender bias from the
Project Implicit participants.

\subsubsection*{Methods}\label{methods-1}

Previous work has shown biases studied using IATs can be predicted from
the distributional statistics of language (word co-occurrences). Using
these statistics, Caliskan, Bryson, and Narayanan (2017; henceforth,
CBN) measured the distance between the words presented to participants
in the IAT task. CBN found that these distances were highly correlated
with the biases computed by a variety of IATs (e.g., valence and
Caucasian vs.~African-American names; gender and math vs.~arts;
permanence and mental vs.~physical diseases). CBN only measured semantic
biases in English. Here, we extend CBN's method to 25 languages
examining whether languages with a stronger gender bias as expressed in
distributional semantics predict stronger implicit and explicit gender
biases on a large dataset of previously administered gender-career IATs.

We identified the most frequently spoken language in each country in our
analysis using Ethnologue (Simons \& Charles, 2018). After exclusions
(see below), our final sample included 25
languages.\footnote{Note that while Hindi is identified as the most frequently spoken language in India, India is highly multilingual and so Hindi embeddings may be a poor representation of  the linguistic statistics for speakers in India as a group.}
For each language, we obtained translations from native speakers for the
stimuli in the Project Implicit gender-career IAT behavioral task (Nosek
et al., 2002) with one slight modification. In the behavioral task,
proper names were used to cue the male and female categories
(e.g.~\enquote{John,} \enquote{Amy}), but because there are not direct
translation equivalents of proper names, we instead used a set of
generic gendered words which had been previously used for a different
version of the gender IAT (e.g., ``man,'' ``woman;'' Nosek et al.,
2002). Our linguistic stimuli were therefore a set of 8 female and 8
male Target Words (identical to Study 1a), and the set of 8 Attribute
Words words used in the Project Implicit gender-career IAT: 8 related to
careers (\enquote{career,} \enquote{executive,} \enquote{management,}
\enquote{professional,} \enquote{corporation,} \enquote{salary,}
\enquote{office,} \enquote{business}) and 8 related to families
(\enquote{family,} \enquote{home,} \enquote{parents,}
\enquote{children,} \enquote{cousins,} \enquote{marriage,}
\enquote{wedding,} \enquote{relatives}). For one language, Filipino, we
were unable to obtain translations from a native speaker, and so
Filipino translations were compiled from dictionaries.

We used these translations to calculate a gender bias effect size from
word embedding models trained on text in each language. Our effect size
measure is a standardized difference score of the relative similarity of
the target words to the target attributes (i.e.~relative similarity of
male to career vs.~relative similarity of female to career). Our effect
size measure is identical to that used by CBN with an exception for
grammatically gendered languages (see SM for replication of CBN on our
corpora). Namely, for languages with grammatically gendered Attribute
Words (e.g., niñas for female children in Spanish), we calculated the
relationship between Target Words and Attribute Words of the same gender
(i.e.~\enquote{hombre} (man) to \enquote{niños} and \enquote{mujer}
(woman) to \enquote{niñas}). In cases where there were multiple
translations for a word, we averaged across words such that each of our
target words was associated with a single vector in each language. In
cases where the translation contained multiple words, we used the entry
for the multiword phrase in the model when present, and averaged across
words otherwise. Like the psychological measures of bias from the
Project Implicit data, larger values indicate larger gender bias.

We calculated gender bias estimates using the same word embedding models
as in Study 1a (Subtitle and Wikipedia corpora). We excluded languages
from the analysis for which 20\% or more of the target words were
missing from the model or the model did not exist. This led us to
exclude one language (Zulu) from the analysis of the Wikipedia corpus
and six languages from the analysis of the Subtitle corpus (Chinese,
Croatian, Hindi, Japanese, Filipino, and Zulu). Our final sample
included 25 languages in total (\emph{N}\textsubscript{Wikipedia} = 25;
\emph{N}\textsubscript{Subtitle} = 20), representing 8 language
families. Finally, we calculated language-level measures for four
additional measures by averaging across countries whose participants
speak the same language: implicit and explicit psychological gender bias
(estimated from the Project Implicit dataset), percentage of women in
STEM fields, and median country age.

\subsubsection*{Results}\label{results-2}

\begin{figure*}[t!]
\centering
\includegraphics{iat_lang_files/figure-latex/unnamed-chunk-11-1.pdf}
\caption{\label{fig:unnamed-chunk-11}Implicit gender bias (adjusted for age,
sex, and block order) as a function of the linguistic gender bias
derived from word-embeddings (Study 1b). Each point corresponds to a
language, with the size of the point corresponding to the number of
participants speaking that langauge. Linguistic biases are estimated
from models trained on text in each language from Subtitle (left) and
Wikipedia (right) corpora. Larger values indicate a larger bias to
associate men with the concept of career and women with the concept of
family. Error bands indicate standard error of the linear model
estimate.}
\end{figure*}

Despite the differences in the specific content conveyed by the
Subtitles and Wikipedia corpus, the estimated gender bias for each
language was similar across the two corpora (\emph{t}(19) = -0.06,
\emph{p} = 0.95). We next examined the relationship between these
estimates of gender bias for each language and the mean IAT bias score
for participants from countries where that language was dominant (and,
we assume, was the native language of most of these individuals).
Implicit gender bias was positively correlated with estimates of
language bias from both the Subtitle (\emph{r} = 0.5, \emph{p} = 0.02)
and Wikipedia trained models (\emph{r} = 0.48, \emph{p} = 0.01; Fig. 2;
Table 1 shows the language-level correlations between all variables in
Studies 1b and 2). The relationship between implicit gender bias and
language bias remained reliable after partialling out the effect of
median country age (Subtitle: \emph{r} = 0.42, \emph{p} = 0.04;
Wikipedia: \emph{r} = 0.43, \emph{p} = 0.04). Linguistic gender bias was
not correlated with explicit gender bias (Subtitle: \emph{r} = -0.08,
\emph{p} = 0.74; Wikipedia: \emph{r} = 0.34, \emph{p} = 0.09). Estimates
of language bias from the Subtitle corpus were correlated with the
objective measure of gender equality, percentage of women in STEM fields
(\emph{r} = -0.55, \emph{p} = 0.02); this relationship was not reliable
for the Wikipedia corpus (\emph{r} = -0.19, \emph{p} = 0.4).

\begingroup\fontsize{6}{8}\selectfont

\begin{table}{llllllllll}
%\caption{\label{tab:bigtable}Correlation (Pearson's r) for all measures in Study 1 and 2 at the level of languages. Top panel shows simple correlations; bottom panel shows partial correlations controlling for median country age. Single asterisks indicate p < .05 and double asterisks indicate p < .01. The + symbol indicates a marginally significant p-value, p < .1.}\\
\toprule
\rotatebox{90}{ } & \rotatebox{90}{Residualized Explicit Bias} & \rotatebox{90}{Residualized Implicit Bias (IAT)} & \rotatebox{90}{Percent Women in STEM} & \rotatebox{90}{Language IAT (Subtitle)} & \rotatebox{90}{Language IAT (Wikipedia)} & \rotatebox{90}{Prop. Gendered Occupation Labels} & \rotatebox{90}{Occupation Bias (Subtitle)} & \rotatebox{90}{Occupation Bias (Wikipedia)} & \rotatebox{90}{Median Country Age}\\
\midrule
\addlinespace[0.3em]
\multicolumn{10}{l}{\textbf{Simple Correlations}}\\
\hspace{1em}Residualized Explicit Bias &  & \ .18 & \ .18 & -.08 & \ .34+ & \ .11 & \ .28 & \ .29 & -.07\\
\hspace{1em}Residualized Implicit Bias (IAT) & \ .18 &  & -.53* & \ .50* & \ .48* & \ .57** & \ .64** & \ .59** & \ .61**\\
\hspace{1em}Percent Women in STEM & \ .18 & -.53* &  & -.55* & -.19 & -.35 & -.39 & -.32 & -.42+\\
\hspace{1em}Language IAT (Subtitle) & -.08 & \ .50* & -.55* &  & \ .51* & \ .28 & \ .42+ & \ .40+ & \ .31\\
\hspace{1em}Language IAT (Wikipedia) & \ .34+ & \ .48* & -.19 & \ .51* &  & \ .18 & \ .28 & \ .44* & \ .25\\
\hspace{1em}Prop. Gendered Occupation Labels & \ .11 & \ .57** & -.35 & \ .28 & \ .18 &  & \ .75** & \ .70** & \ .35+\\
\hspace{1em}Occupation Bias (Subtitle) & \ .28 & \ .64** & -.39 & \ .42+ & \ .28 & \ .75** &  & \ .80** & \ .36\\
\hspace{1em}Occupation Bias (Wikipedia) & \ .29 & \ .59** & -.32 & \ .40+ & \ .44* & \ .70** & \ .80** &  & \ .34+\\
\hspace{1em}Median Country Age & -.07 & \ .61** & -.42+ & \ .31 & \ .25 & \ .35+ & \ .36 & \ .34+ & \\
\addlinespace[0.3em]
\multicolumn{10}{l}{\textbf{Partial Correlations}}\\
\hspace{1em}Residualized Explicit Bias &  & \ .28 & \ .16 & -.06 & \ .38+ & \ .14 & \ .33 & \ .34 & \\
\hspace{1em}Residualized Implicit Bias (IAT) & \ .28 &  & -.38+ & \ .42* & \ .43* & \ .48* & \ .57** & \ .52** & \\
\hspace{1em}Percent Women in STEM & \ .16 & -.38+ &  & -.49* & -.09 & -.23 & -.28 & -.20 & \\
\hspace{1em}Language IAT (Subtitle) & -.06 & \ .42* & -.49* &  & \ .47* & \ .20 & \ .35+ & \ .33 & \\
\hspace{1em}Language IAT (Wikipedia) & \ .38+ & \ .43* & -.09 & \ .47* &  & \ .11 & \ .21 & \ .39+ & \\
\hspace{1em}Prop. Gendered Occupation Labels & \ .14 & \ .48* & -.23 & \ .20 & \ .11 &  & \ .71** & \ .66** & \\
\hspace{1em}Occupation Bias (Subtitle) & \ .33 & \ .57** & -.28 & \ .35+ & \ .21 & \ .71** &  & \ .77** & \\
\hspace{1em}Occupation Bias (Wikipedia) & \ .34 & \ .52** & -.20 & \ .33 & \ .39+ & \ .66** & \ .77** &  & \\
\bottomrule
\end{supertabular}

\endgroup{}

\subsection{Study 1c: A pre-registered study of British versus American
English
biases}\label{study-1c-a-pre-registered-study-of-british-versus-american-english-biases}

In Study 1c, we conducted a confirmatory, pre-registered analysis of our
hypothesis that biases present in language statistics are reflected in
the psychological biases of speakers of those languages. To do this, we
leveraged an exisiting dataset from the Attitiudes, Identities, and
Individual Differences Study (AIID; Hussey et al., 2019) containing
measures of IAT performance from over 200,000 participants for a wide
range of IAT types (e.g.~career - family, team - individual, etc.).
Because most participants in this sample were English speakers, we
compared biases between participants who spoke two different dialects of
English: British and American. For each of the 31 IAT types in the
sample, we predicted that the degree to which that bias was present in a
speaker's English dialect (British or American) would predict the
magnitude of their psychological bias, as measured by the IAT.

\subsection*{Method}\label{method}

The AIID datset was partioned into two samples: exploratory (15\%) and
confirmatory (85\%). Based on the exploratory sample, we pre-registered
our analysis plan for the confirmatory sample
(\url{https://osf.io/3f9ed}). We note where are analysis diverges from
the preregistration plan below.

Of the 95 IAT types present in the dataset, we identified 31 types based
on the following criteria: (1) stimuli were words rather than pictures,
and (2) 75\% of the target words for each IAT test were present in both
the BNC and COCA models. To measure the bias in language, we trained
word embedding models on comparably-sized corpora of British (British
National Corpus; Burnard, 1995) and American English (Corpus of
Contemporary American English; see SM for details; Davies, 2008). We
then calculated a language bias effect size for each IAT in each English
dialect, using the same method as in Study 1b.

Within the confirmatory AIID dataset, there were 187,969 administrations
of the IAT from participants in the United States (USA) or United
Kingdom (UK). Our exclusion procedure for the behavioral data was
similiar to Study 1a above (see SM for details). Our final sample
included data from 135,240 administrations of the IAT across the 31 IAT
types (USA: \emph{N} = 127,630; UK: \emph{N} = 7,610). Each participant
completed an average of 6.13 IAT types (\emph{SD} = 3.99). For each
administrations of the IAT, we calculated a residual IAT score,
controlling for sex, age, education, task order (relative ordering of
implicit versus explicit measures), and block order (relative ordering
of congruent versus incongruent mappings on IAT task).

We fit a linear mixed effect model predicting the magnitude of the bias
for each participant with language dialect (British or American
English), magnitude of the bias difference between the two dialects, and
an interaction betwen these two terms as fixed effect. We included
participant and IAT type as random intercepts. This model differs from
the pre-registered analysis, which is also consistent with results of
the presented analysis, but does not account for participant-level
variance (see SM for results of pre-registered model).

\subsection*{Results}\label{results-3}

As predicted, there was a reliable interaction between language dialect
and the magnitude of the bias difference between the two dialects
(\(\beta\) = -.05, \emph{SE} = .02, \emph{t} = -2.88), suggesting that
language bias was a reliable predictor of implicit bias. Figure 3 shows
the difference in bias (British - American) in language and behavior for
each of the 31 IATs in our dataset.

\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{iat_lang_files/figure-latex/1cplot-1.pdf}
\caption{\label{fig:1cplot}Difference (British - American) in implicit bias
(y-axis) and linguistic bias (x-axis) for each of the 31 IAT types in
our dataset.}
\end{figure}

\subsubsection*{Discussion}\label{discussion}

In Study 1, we found that a previously-reported psychological gender
bias -- the bias to associate men with career and women with family --
was correlated with the magnitude of that same bias as measured in the
language statistics of 25 languages. Participants completing the IAT in
countries where the dominant language had stronger associations between
men and career words, and women and family words, showed stronger biases
on the gender-career IAT. In a pre-registered, confirmatory analysis, we
also find that this pattern extends to other biases: In a comparision of
31 different IAT types, the magnitude of the bias in speaker's language
predicted their behavioral bias, as measured by the IAT. These results
are consistent with both the \emph{language-as-reflection} and
\emph{language-as-causal-factor} hypotheses. In Study 2, we try to
better distinguish between these hypotheses by investigating whether the
gender-career bias is associated with two structural features of
language: grammatical gender and the presence of gendered occupation
terms (e.g., waiter/waitress).

\section*{Study 2: Gender bias and lexicalized
gender}\label{study-2-gender-bias-and-lexicalized-gender}

In Study 1 we examined cross-linguistic differences in gender bias
without any reference to structural differences that exist in the
languages in our sample. One such structural difference concerns the
grammaticalization of gender. Some languages such as Spanish mark gender
distinctions in a grammatically obligatory way, e.g.,
\enquote{enfermero} (nurse-\textsc{masc}) versus \enquote{enfermera}
(nurse-\textsc{fem}). Grammatical gender systems frequently demand
gender-based agreement, e.g., \enquote{el enfermero alto} (the tall
nurse-\textsc{masc}) versus \enquote{la enfermera alta} (the tall
nurse-\textsc{fem}), which while informationally redundant, may act to
amplify gender biases in the language. Another way languages convey
gender is through gender-specific terms such as \enquote{waiter} vs.
\enquote{waitress.} Languages with grammatical gender do tend to use
more such terms, but the two are distinct. French has grammatical
gender, but many occupation terms are gender-neutral (e.g., auteur,
athlète, juge).

In Study 2, we examined whether grammatical gender and use of
gender-specific occupation terms are associated with a greater
psychological gender bias and whether this relationship is further
mediated by language statistics. Finding such associations would lend
support to the language-as-causal-factor hypothesis because grammatical
gender and (to a lesser degree) lexical gender encoding are relatively
stable features of language. Although both can change over time, these
changes are somewhat independent of the propositional content conveyed
by language. For example, a Finnish document about nursing being
unsuitable for men would still use a gender-neutral form of
\enquote{nurse} while a Spanish document promoting nursing careers to
men would be committed to using gender-marked forms.

\subsubsection*{Methods}\label{methods-2}

We identified 20 occupation names that were likely to have corresponding
terms in all 25 of our languages, and that were balanced in terms of
their perceived gender bias in the workforce (Misersky et al., 2014). We
then translated these words into each of the 25 languages in our sample,
distinguishing between male and female variants (e.g., \enquote{waiter}
vs. \enquote{waitress}) where present. The words were translated by
consulting native speakers and dictionaries, as necessary.

We coded each language for the presence or absence of a sex-based
grammatical gender system using WALS (Dryer \& Haspelmath, 2013) and
other sources, as necessary. To estimate the extent to which a language
lexically encoded gender, we calculated the proportion of occupations
within each language for which the male and female forms differed.
Larger values indicate a preponderance for more gender-specific forms in
a language. Finally, we also estimated the extent to which each
occupation term was gender biased in its language statistics using word
embedding models trained in each language on the Subtitle and Wikipedia
corpora. For each occupation form, we estimated its bias in language
statistics using the same pairwise similarity metric as in Study 1a, and
then averaged across occupations within a language to get a
language-level estimate of gender bias. Larger values indicate greater
gender bias in language statistics. We then compared each of the three
language measures (grammatical gender, proportion specific gender forms,
and bias in language statistics for occupation words) to the
psychological gender measures described in Study 1b (implicit and
explicit bias, adjusted for age, sex and block order).

\subsubsection*{Results}\label{results-4}

\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{iat_lang_files/figure-latex/unnamed-chunk-14-1.pdf}
\caption{\label{fig:unnamed-chunk-14}Implicit gender bias (adjusted for age,
sex, and block order) as a function of the proportion of gender-specific
labels for set of words referring to occupations. Each point corresponds
to a language, with the size of the point corresponding the number of
participants speaking that language. Error band indicates standard error
of the linear model estimate.}
\end{figure}

In additive linear models controlling for median country age, there was
no difference in implicit or explicit psychological gender bias for
speakers of languages with a grammatical gender system (\emph{N} = 12),
compared to those without (\emph{N} = 13; Implicit: \(\beta\) = 0;
\emph{SE} = 0.01; \emph{t} = -0.43; Explicit: \(\beta\) = -0.09;
\emph{SE} = 0.07; \emph{t} = -1.23). Languages with grammatical gender
systems were more likely to have gender-specific terms for occupations
(\emph{t}(14.89) = 4.85, \emph{p} \textless{} .001). Implicit gender
bias was reliably correlated with degree of gender-specific marking on
occupation words: Languages with more gender-specific forms tended to
have speakers with greater psychological gender bias (\emph{r} = 0.57,
\emph{p} \textless{} .01), even after partialling out the effect of
median country age (\emph{r} = 0.48, \emph{p} = 0.02; Table 1). There
was no relationship between explicit psychological gender bias and
lexical marking of occupation words after partialling out the effect of
median country age (\emph{r} = 0.14, \emph{p} = 0.51).

\begin{figure*}[t!]
\centering
\includegraphics{iat_lang_files/figure-latex/unnamed-chunk-15-1.pdf}
\caption{\label{fig:unnamed-chunk-15}Implicit gender bias (adjusted for age,
sex, and block order) as a function of mean gender bias of words
referring to occupations, with each point corresponding to a language
(Study 2). The size of the point corresponds the number of participants
speaking that language. Occupation gender bias is estimated for each
language from word embedding models trained on Subtitle (left) and
Wikipedia (right) corpora. Error bands indicate standard error of the
linear model estimate.}
\end{figure*}

We next examined whether having gender-specific forms for a particular
occupation was associated with greater gender bias in the language
statistics for that form. We fit a mixed effect model predicting degree
of gender bias in language statistics (estimated from word embedding
models) as a function of degree of distinctiveness between male and
female forms for that word, with random intercepts and slopes by
language. Degree of form distinctiveness was a strong predictor of
language statistics for models trained on both the Subtitle corpus
(\(\beta\) = 0.59; \emph{SE} = 0.07; \emph{t} = 8.72) and Wikipedia
corpus (\(\beta\) = 0.81; \emph{SE} = 0.09; \emph{t} = 9.48), with words
with shared male and female forms tending to have less gender bias. This
relationship also held at the level of languages: Languages with more
distinct forms had a greater gender-career bias in language statistics
(Subtitle: \emph{r} = 0.75, \emph{p} \textless{} .01; Wikipedia:
\emph{r} = 0.7, \emph{p} \textless{} .01; Fig. 4).

Finally, we examined the relationship between gender bias in language
statistics and psychological gender biases at the level of languages.
Unlike in Study 1, all the target words in the present study referred to
people (occupations) and thus potentially could be marked for the gender
of the referenced person. Consequently, if explicit gender marking
drives language statistics, we should expect to see a strong positive
relationship at the level of languages between bias in language
statistics \emph{for occupation words} and psychological biases for
speakers of that language. Consistent with this prediction, gender bias
in language statistics for occupation words was positively correlated
with implicit gender bias (Subtitle: \emph{r} = 0.64, \emph{p}
\textless{} .01; Wikipedia: \emph{r} = 0.59, \emph{p} \textless{} .01),
and remained reliable after partialling out the effect of median country
age (Subtitle: \emph{r} = 0.57, \emph{p} \textless{} .01; Wikipedia:
\emph{r} = 0.52, \emph{p} = 0.01; Fig. 5). There was no relationship
between language statistics for occupation words and explicit
psychological gender bias, even after partialling out the effect of
median country age (Subtitle: \emph{r} = 0.33, \emph{p} = 0.12;
Wikipedia: \emph{r} = 0.34, \emph{p} = 0.11).

To understand the relative predictive power of language statistics and
distinct forms, we fit an additive linear model predicting implicit bias
from language statistics and proportion distinct forms, controlling for
median country age. Because language statistics for occupation terms and
proportion distinct forms were highly colinear (Wikipedia: \emph{r} =
0.70, \emph{p} \textless{} .001; Subtitle: \emph{r} = 0.75, \emph{p}
\textless{} .001), we used the estimate of bias in language statistics
for each language based on the set IAT words described in Study 1b. Both
gender bias in language statistics (based on IAT words) and the
proportion of gender-specific occupation titles were independent
predictors of implicit bias. The two predictors accounted for 49\% of
variance in implicit bias when using the Subtitle corpus and 60\% of
variance for the Wikipedia corpus. Full model results are reported in
the SM.

The high degree of collinearity between language statistics for
occupation terms and proportion gender-specific occupations forms is
consistent with a causal model in which language statistics mediate the
effect of gender-specific forms on implicit bias: The presence of
distinct forms referring to people of different genders \emph{leads to}
biased language statistics, which in turn leads to gender bias in
behavior. Consistent with this model, a bootstrap test of mediation
revealed a marginal effect for the Subtitle model (path-ab = 0.28,
\emph{p} = 0.10; Alfons, 2018), and significant mediation effect for the
Wikipedia model (path-ab = 0.35, \emph{p} =
0.04).\footnote{Though our power to detect this effect is relatively low (approximately, .4; Schoemann, Boulton, \& Short, 2017).}

\section*{Discussion}\label{discussion-1}

In Study 2, we asked whether structural features of language -- the
presence of a grammatical gender systems and the propensity to
lexicalize gender distinctions -- correlated with implicit bias.
Grammatical gender was not reliably correlated with implicit bias.
Languages that use more gender-specific occupation terms, however did
predict a greater implicit bias. There is some evidence that the effect
of lexical gender distinctions on implicit bias may be mediated by the
influence this terminology introduces on the ways that gender is
statistically encoded in different language. What does this finding mean
for our two hypotheses? The fact that, e.g., German explicitly marks the
gender of professors while English does not, has cognitive consequences
for German speakers; it is not simply a matter of current cultural
differences being reflected in language. Language does not merely
reflect our biases, it seems to contribute to them.

\section*{General Discussion}\label{general-discussion}

Where do we get our gender stereotypes? Non-linguistic experiences
surely play a role, but might we also be learning our biases from the
statistics of language to which we are exposed? We used a large-scale
dataset of Implicit Association Tests (IATs) measuring the bias to
associate men with career and women with family and related people's
measured implicit bias to the statistics of the dominant language spoken
in the country of the participants. In Study 1, we found that languages
with a greater gender bias in their distributional structure, tend to
have speakers that have stronger implicit biases. In Study 2, we found a
positive relationship between a structural language feature -- the
prevalence of gender-marked occupation terms -- and implicit bias. There
is suggestive evidence that this greater implicit bias is mediated by
the greater gender bias encoded in the distributional patterns of
gender-marked terms.

Our work is the first to characterize the relationship between broad
structural patterns in language and cultural stereotypes. The positive
correlation that we find between gender bias in language and gender bias
in speakers is consistent both with language playing a causal role in
the emergence of cultural stereotypes and the idea that language merely
reflects existing stereotypes of its speakers. The positive correlation
we find in Study 2 between prevalence of gender-specific terms and
implicit bias is most parsimoniously explained by the
language-as-causal-factor hypothesis because it is unlikely that
language forms change on a timescale that could directly reflect
behavior. The two causal forces are not mutually exclusive, and in fact
may amplify the effects of each other. Future work could use
experimental methods to manipulate language statistics in order to more
directly examine these causal influences.

A central contribution of the current work is that it sheds light onto
the potential origins of psychological biases that exist at the level of
the individual. Given observed large-scale stuctural patterns of gender
inequality, such as differences in STEM participation, researchers from
a range of fields have sought to understand the individual-level causal
processes that led to the emergence of structural inequality. But,
critically, these previous efforts have taken properties of the
indidividual -- such as feelings of self-efficacy in science (Stoet \&
Geary, 2018) and general preferences (Falk \& Hermle, 2018) -- as
largely exogeneous. Here, we provide a potential explanation of the
origins of these psychological biases by arguing that exposure to biased
language statistics could play a causal role in the emergence of these
biases at the level of the invididual. Consistent with this account,
biases in language statistics are correlated with previous
individual-level predictors of STEM inequality, such as self-efficacy in
science and general preferences (see SM for details).

One limitation of our work is the reliance on the IAT, which has been
criticized for both its low reliability (Lane, Banaji, Nosek, \&
Greenwald, 2007) and limited external validity (Fazio \& Olson, 2003).
Issues of reliability are less relevant here because we use the IAT to
measure group-level differences rather than as an individual-difference
measure. However, concerns about validity are important particularly
because we find that language measures and explicit psychological
measures of gender bias are uncorrelated, though explicit bias was
measured in a fairly coarse way. Understanding the full import of
linguistic biases on cultural stereotypes would therefore require
obtaining measures more closely related to real-world behavior.

Cultural stereotypes are acquired through experience. Here, we show that
group-level differences in implicit bias are strongly correlated with
the strength of gender bias encoded in the statistics of different
languages. This pattern suggests that the statistics of language use are
an important source of cultural experience: The mere process of
listening to and producing language exposes one to statistics that may
lead to the formation of cultural stereotypes. Many cultural
associations present in the statistics of language may be innocuous --
indeed, these statistics may be an important mechanism through which
cultural information is transmitted (Lupyan \& Lewis, 2017). But, in
other cases, like the kind of gender stereotypes investigated here,
language may play a powerful role in their formation, and ultimately
contribute to undesirable real-world consequences like gender inequality
in STEM. Understanding the causal role that language plays in the
formation of these stereotypes is therefore an important first step to
changing these consequences.

\matmethods{Please describe your materials and methods here. This can be more than one paragraph, and may contain subsections and equations as required. Authors should include a statement in the methods section describing how readers will be able to access the data in the paper. 

\subsection*{Subsection for Method}
Example text for subsection.
}

\showmatmethods{} % Display the Materials and Methods section

\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}

\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{pnas-sample}

\end{document}