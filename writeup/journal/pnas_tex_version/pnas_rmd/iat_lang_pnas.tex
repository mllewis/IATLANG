\documentclass[man,floatsintext]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={What are we learning from language? Associations between gender biases and distributional semantics in 25 languages},
            pdfauthor={Molly Lewis~\& Gary Lupyan},
            pdfkeywords={cultural stereotypes, implicit association task (IAT), gender},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}


  \title{What are we learning from language? Associations between gender biases and distributional semantics in 25 languages}
    \author{Molly Lewis\textsuperscript{1,2}~\& Gary Lupyan\textsuperscript{1}}
    \date{}
  
\shorttitle{What are we learning from language?}
\affiliation{
\vspace{0.5cm}
\textsuperscript{1} University of Wisconsin-Madison\\\textsuperscript{2} University of Chicago}
\keywords{cultural stereotypes, implicit association task (IAT), gender\newline\indent Word count: 1998 (excluding methods/results)}
\usepackage{csquotes}
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[flushleft]{threeparttable}
\usepackage{threeparttablex}

\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}
\usepackage{float}
\floatplacement{figure}{t!}

\authornote{Portions of this manuscript appeared in Lewis \& Lupyan, 2018, Cog. Sci. Proceedings.

Correspondence concerning this article should be addressed to Molly Lewis, . E-mail: \href{mailto:mollyllewis@gmail.com}{\nolinkurl{mollyllewis@gmail.com}}}

\abstract{
Cultural stereotypes such as the idea that men are more suited for paid work while women for taking care of the home and family may contribute to gender imbalances in STEM fields (e.g., Leslie, Cimpian, Meyer, \& Freeland, 2015) and other undesirable gender disparities. Here, we test the hypothesis that word co-occurrence statistics (e.g., the co-occurrence of ``nurse'' with ``she'') play a causal role in the formation of the men-career/women-family stereotype. We use word embedding models to measure bias in the distributional statistics of 25 languages and find that languages with larger biases tend to have speakers with larger implicit biases (\emph{N} = 657,335). These biases are further related to the extent that languages mark gender in their lexical forms (e.g., ``waiter''/``waitress'') hinting that linguistic biases may be causally related to biases shown in people's implicit judgments.


}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

By the time children are two years old, they have begun to acquire the gender stereotypes in their culture (Gelman et al., 2004). These stereotypes can have undesirable effects. For example, in one study, 6-year-old girls were less likely than boys to choose activities that were described as for children \enquote{who are very, very smart} and also less likely to think of themselves as \enquote{brilliant} (Bian, Leslie, \& Cimpian, 2017). Such beliefs may, over time, translate to the observed lower rates of female participation in STEM fields (Ceci \& Williams, 2011; Leslie, Cimpian, Meyer, \& Freeland, 2015; Miller, Eagly, \& Linn, 2015; Stoet \& Geary, 2018). For this reason and others, it is important to better understand how cultural stereotypes are formed.

We can distinguish between two major sources of information on which gender stereotypes may be based. The first is direct experience. For example, one may observe that most nurses are women and most philosophers are men and conclude that women are better suited for nursing and men for philosophy. The second is language. Even without any direct experience with nurses or philosophers, one may learn about their stereotypical gender from language about nurses and philosophers. Languages encode gender in multiple ways. These include gender-specific titles (\enquote{Mr}. vs.~\enquote{Miss}), proper names (\enquote{Sam} vs.~\enquote{Ashley}), pronouns (\enquote{he} vs.~\enquote{she}), certain job titles (\enquote{waiter} vs.~\enquote{waitress}), and higher-order linguistic associations (otherwise gender-neutral words can become gendered by being associated with explicitly gendered contexts). Another source of linguistic information comes from sex-based grammatical gender systems found in approximately 30\% of languages (Dryer \& Haspelmath, 2013). For example, in Spanish, the gender of a nurse must be specified grammatically (\enquote{enfermer\emph{a}} vs.~\enquote{enfermer\emph{o}}).

To the extent that language is a source of information for forming cultural stereotypes, two people with similar direct experiences, but different linguistic experiences, may develop different stereotypes. Some past work hints at people's surprising sensitivity to stereotype-relevant information delivered through language. Young children perform worse in a game if they are told that someone of the opposite gender performed better than they did on a previous round (Rhodes \& Brickman, 2008), or merely told that the game is associated with a particular gender (Cimpian, Mu, \& Erickson, 2012). In some cases, a subtle turn of phrase can influence children's gender-based generalization (Cimpian \& Markman, 2011; Rhodes, Leslie, Yee, \& Saunders, 2019). For example, Cimpian and Markman found that children were more likely to infer that a novel skill is stereotypical of a gender if the skill is introduced with a generic as opposed a non-generic subject (``{[}Girls are/There is a girl who is{]} really good at a game called \enquote{gorp''}). Such work shows that in certain experimental settings, language can influence stereotype formation. We were interested in whether it actually does, and by what means.

A widely used method for quantifying cultural stereotypes at an individual level is the \emph{Implicit Association Test} (IAT; Greenwald, McGhee, \& Schwartz, 1998). Here, we use previously administered IATs designed to measure a particular type of gender stereotype: A bias to associate men with careers and women with family (\emph{N} = 657,335; Nosek, et al., 2002). These data span native speakers of 25 languages allowing us to assess how performance varies with properties of languages.

Discovering that linguistic bias predicts people's implicit biases can be interpreted in at least two ways. The first is that some cultures have stronger stereotypes and these are reflected in what people talk about. Language, on this view, simply \emph{reflects} pre-existing biases. We refer to this as the \emph{language as reflection} hypothesis. However, language may not simply reflect pre-existing biases, but may also provide a distinct source of information for learning about these stereotypes. We refer to this second possibility that language exerts a causal influence on people's biases as the \emph{language as causal factor} hypothesis.

In Study 1, we examine whether language-derived gender biases predict responses on the gender-career IAT. Our analysis focuses on the \emph{distributional} structure of language rather than the specifics of the communicated content. In Study 2, we examine how the psychological biases measured by the IAT and the linguistic biases we measure relate to more structural aspects of language: sex-based grammatical gender and the prevalence of gender-specific occupation terms (e.g., \enquote{waiter}/\enquote{waitress} but \enquote{teacher}/\enquote{teacher}). The results of Study 2 suggest that language not only reflects existing gender biases, but may play a causal role in shaping them.

\hypertarget{description-of-cross-cultural-dataset-of-psychological-gender-bias}{%
\section{Description of Cross-Cultural Dataset of Psychological Gender Bias}\label{description-of-cross-cultural-dataset-of-psychological-gender-bias}}

\hypertarget{materials-and-methods}{%
\subsection{Materials and Methods}\label{materials-and-methods}}

To quantify cross-cultural gender bias, we used data from a large-scale administration of an Implicit Association Task (IAT; Greenwald et al., 1998) by Project Implicit (\url{https://implicit.harvard.edu/implicit/}; Nosek, Banaji, \& Greenwald, 2002). The IAT measures the strength of respondents' implicit associations between two pairs of concepts (e.g., male-career/female-family vs.~male-family/female-career) accessed via words (e.g., \enquote{man,} \enquote{business}). The underlying assumption of the IAT is that words denoting more similar meanings should be easier to pair together compared to more dissimilar pairs.

Meanings are paired in the task by assigning them to the same response keys in a two-alternative forced-choice categorization task. In the critical blocks of the task, meanings are assigned to keys in a way that is either bias-congruent (i.e.~Key A = male/career; Key B = female/family) or bias-incongruent (i.e.~Key A = male/family; Key B = female/career). Participants are then presented with a word related to one of the four concepts and asked to classify it as quickly as possible (see Study 1b Methods for list of target words). Slower reaction times in the bias-incongruent blocks relative to the bias-congruent blocks are interpreted as indicating an implicit association between the corresponding concepts (i.e.~a bias to associate male with career and female with family).

We analyzed gender-career IAT scores collected by Project Implicit between 2005 and 2016, restricting our sample based on participants' reaction times and error rates using the same criteria described in Nosek, Banjai, and Greenwald (2002, pg.~104). We only analyzed data for countries that had complete demographic information and complete data from the IAT for least 400 participants (2\% of these respondents did not give responses to the explicit bias question). This cutoff was arbitrary, but the pattern of findings reported here holds for a range of minimum participant values (see SM\footnote{SM available here: \url{https://mollylewis.shinyapps.io/iatlang_SI/}; All data and code available here: \url{https://github.com/mllewis/IATLANG}}). Our final sample included 657,335 participants from 39 countries, with a median of 1,145 participants per country. Importantly, although the respondents were from largely non-English speaking countries, the IAT was conducted in English. We do not have language background data from the participants, but we assume that a large fraction of the respondents from non-English speaking countries were native speakers of the dominant language of the country and second language speakers of English. The fact that the test was administered in English lowers the prior likelihood of finding language-specific predictors of the kind we report here.

To quantify participants' performance on the IAT we adopt the widely used \emph{D-score}, which measures the difference between critical blocks for each participant while controlling for individual differences in response time (Greenwald, Nosek, \& Banaji, 2003). After completing the IAT, participants were asked \enquote{How strongly do you associate the following with males and females?} for both the words \enquote{career} and \enquote{family.} Participants indicated their response on a Likert scale ranging from \emph{female} (1) to \emph{male} (7). We calculated an explicit gender-career bias score for each participant as the Career response minus the Family response, such that greater values indicate a greater bias to associate males with career.

\hypertarget{results}{%
\subsection{Results}\label{results}}

At the participant level, implicit bias scores were positively correlated with
participant age (\emph{r} = 0.06, \emph{p} \textless{} .0001). Male participants (\emph{M} = 0.32, \emph{SD} = 0.37) had a significantly smaller implicit gender bias than female participants (\emph{M} = 0.41, \emph{SD} = 0.35; \emph{t} = 96.82, \emph{p} \textless{} .0001), a pattern consistent with previous findings (Nosek et al., 2002). Implicit bias scores were larger for participants that received the block of trials with bias-incongruent mappings first relative to the opposite order (\emph{t} = -104.03, \emph{p} \textless{} .0001).

Because we did not have language information at the participant level, in the remaining analyses we examine gender bias and its predictors at the country level. To account for the above-mentioned influences on implicit bias, we calculated a residual implicit bias score for each participant, controlling for participant age, participant sex, and block order. We also calculated a residual explicit bias score controlling for the same set of variables. We then averaged across participants to estimate the country-level gender bias (Implicit: \emph{M} = -0.01; \emph{SD} = 0.03; Explicit: \emph{M} = 0.00; \emph{SD} = 0.18). Implicit gender biases were moderately correlated with explicit gender biases at the level of participants (\emph{r} = 0.16, \emph{p} \textless{} .0001) but not countries (\emph{r} = 0.26, \emph{p} = 0.12).

Do the implicit and explicit biases measured by the Project Implicit dataset predict any real world outcomes? We compared our residual country-level implicit and explicit gender biases to a gender equality metric reported by the United Nations Educational, Scientific and Cultural Organization (UNESCO) for each country: the percentage of women among STEM graduates in tertiary education from 2012 to 2017 (Miller et al., 2015; Stoet \& Geary, 2018; available here: \url{http://data.uis.unesco.org/}). These data were available for 33 out of 39 of the countries in our sample. Consistent with previous research (Miller et al., 2015), we found that implicit gender bias was negatively correlated with percentage of women in STEM fields: Countries with smaller gender biases tended to have more women in STEM fields (\emph{r} = -0.54, \emph{p} \textless{} .001). In contrast, there was no relationship between the percentage of women in STEM fields and the explicit gender bias measure used by Project Implicit (\emph{r} = 0.14, \emph{p} = 0.43). In addition, we found a strong correlation between the median age of each country's population (as reported by the CIA factbook, 2017) and the residual implicit bias (in which participant age was held constant): Countries with older populations tended to have larger gender biases (\emph{r} = 0.64, \emph{p} \textless{} .0001).

In sum, we replicate previously-reported patterns of gender bias in the gender-career IAT literature, with roughly comparable effect sizes (c.f.~Nosek, et al., 2002). The weak correlation between implicit and explicit measures is consistent with claims that these two measures tap into different cognitive constructs (Forscher et al., 2016). In addition, we find that an objective measure of gender equality---female enrollment in STEM fields---is associated with implicit gender bias. The finding that older participants show stronger biases may stem from a cohort effect, but it is not obvious why there is a strong positive association between the median age of a country's population and a larger implicit bias when adjusting for the age of individual participants.

\hypertarget{study-1-relating-gender-biases-in-distributional-semantics-and-human-behavior}{%
\section{Study 1: Relating gender biases in distributional semantics and human behavior}\label{study-1-relating-gender-biases-in-distributional-semantics-and-human-behavior}}

Are participants' gender biases predictable from the language they speak? Both the language-as-reflection and language-as-causal-factor hypotheses predict a positive correlation between the two, but showing that such a relationship exists is the first step to investigating a possible causal link. We begin by validating word embedding measures of gender bias by comparing them to explicit human judgements of word genderness (Study 1a). We then apply this method to models trained on text in other languages (Study 1b). We find that the implicit gender bias of participants in a country is correlated with the gender bias in the language spoken in that country.

\hypertarget{study-1a-word-embeddings-as-a-measure-of-psychological-gender-bias}{%
\subsection{Study 1a: Word embeddings as a measure of psychological gender bias}\label{study-1a-word-embeddings-as-a-measure-of-psychological-gender-bias}}

\hypertarget{methods}{%
\subsubsection{Methods}\label{methods}}

To model word meanings, we use semantic embeddings derived from a model that learns meanings by trying to predict a word from surrounding words, given a large corpus. The core assumption of these models is that the meaning of a word can be described by the words it tends to co-occur with---words occurring in similar contexts, tend to have similar meanings (Firth, 1957). A word like \enquote{dog,} for example is represented as more similar to \enquote{cat} and \enquote{hound} than to \enquote{banana} because \enquote{dog} co-occurs with words more in common with \enquote{cat} and \enquote{hound} than with \enquote{banana} (Landauer \& Dumais, 1997; Lund \& Burgess, 1996). Recent developments in machine learning allow the idea of distributional semantics to be implemented in a way that takes into account many features of language structure while remaining computationally tractable. The best known of these word embedding models is \emph{word2vec} (Mikolov, Chen, Corrado, \& Dean, 2013). By attempting to predict the words that surround another word, the model is able to learn a vector-based representation for each word that represents its similarity to other words, i.e., a semantic embedding. We can then compute the similarity between two words by taking the distance between their vectors (e.g., cosine of angle).

In order to validate word embeddings as a measure of psychological gender bias, we used an existing set of word norms in which participants were asked to rate \enquote{the gender associated with each word} on a Likert scale ranging from \emph{very feminine} (1) to \emph{very masculine} (7; Scott, Keitel, Becirspahic, Yao, \& Sereno, 2018). We compared these norms to estimates of gender bias obtained from embedding models pre-trained on two different corpora of English text: Wikipedia (Bojanowski, Grave, Joulin, \& Mikolov, 2016) and subtitles from movies and TV shows (Lison \& Tiedemann, 2016; Van Paridon \& Thompson, n.d.). The Wikipedia corpus is a large, naturalistic corpus of written language; the Subtitle corpus is a smaller corpus of spoken language. Both models were trained using the fastText algorithm (a variant of word2vec; Joulin, Grave, Bojanowski, \& Mikolov, 2016). There were 4,671 words in total that overlapped between the word-embedding models and human ratings.

Using the word embeddings, we calculated an estimate of gender bias for each word by measuring the average cosine distance to a standard set of male \enquote{anchor} words (\enquote{male,} \enquote{man,} \enquote{he,} \enquote{boy,} \enquote{his,} \enquote{him,} \enquote{son,} and \enquote{brother}; Nosek, Banaji, \& Greenwald, 2002) and the average cosine similarity to a set of female words (\enquote{female,} \enquote{woman,} \enquote{she,} \enquote{girl,} \enquote{hers,} \enquote{her,} \enquote{daughter,} and \enquote{sister}). A gender score for each word was then obtained by taking the difference of the similarity estimates (mean male similarity - mean female similarity), such that larger values indicated a stronger association with males.

\hypertarget{results-1}{%
\subsubsection{Results}\label{results-1}}

\begin{figure}
\centering
\includegraphics{iat_lang_pnas_files/figure-latex/unnamed-chunk-10-1.pdf}
\caption{\label{fig:unnamed-chunk-10}Human judgements of word gender bias as a function of gender bias from the Subtitle-trained embedding model (Study 1a). Each point corresponds to a word. Larger numbers indicate stronger association with females (note that this differs from the design of the rating task, but is changed here for consistency with other plots). Blue line shows linear fit and the error band indicates standard error (too small to be visible).}
\end{figure}

Estimates of gender bias from the Subtitle corpus (\emph{M} = 0.01; \emph{SD}= 0.03) and the Wikipedia corpus (\emph{M} = 0; \emph{SD} = 0.03) were highly correlated with each other (\emph{r} = 0.71; \emph{p} \textless{} .0001). Critically, bias estimates from both word embedding models were also highly correlated with human judgements (\emph{M} = 4.10; \emph{SD} = 0.92; \emph{r}\textsubscript{Subtitle} = 0.63; \emph{p} \textless{} .0001; \emph{r}\textsubscript{Wikipedia} = 0.59; \emph{p} \textless{} .0001; Fig. 1). This suggests that the psychological gender bias of a word can be reasonably estimated from word embeddings.

\hypertarget{study-1b-gender-bias-across-languages}{%
\subsection{Study 1b: Gender bias across languages}\label{study-1b-gender-bias-across-languages}}

Having validated our method, we now use it to examine the relationship between psychological and linguistic gender biases. In Study 1b, we estimate the magnitude of the linguistic bias in the dominant language spoken in each country represented in the Project Implicit dataset, and compare this estimate to estimates of psychological gender bias from the Project Implicit participants.

\hypertarget{methods-1}{%
\subsubsection{Methods}\label{methods-1}}

Previous work has shown biases studied using IATs can be predicted from the distributional statistics of language (word co-occurrences). Using these statistics, Caliskan, Bryson, and Narayanan (2017; henceforth, CBN) measured the distance between the words presented to participants in the IAT task. CBN found that these distances were highly correlated with the biases computed by a variety of IATs (e.g., valence and Caucasian vs.~African-American names; gender and math vs.~arts; permanence and mental vs.~physical diseases). CBN only measured semantic biases in English. Here, we extend CBN's method to 25 languages examining whether languages with a stronger gender bias as expressed in distributional semantics predict stronger implicit and explicit gender biases on a large dataset of previously administered gender-career IATs.

We identified the most frequently spoken language in each country in our analysis using Ethnologue (Simons \& Charles, 2018). After exclusions (see below), our final sample included 25 languages.\footnote{Note that while Hindi is identified as the most frequently spoken language in India, India is highly multilingual and so Hindi embeddings may be a poor representation of  the linguistic statistics for speakers in India as a group.} For each language, we obtained translations from native speakers for the stimuli in the Project Implicit gender-career IAT behavioral task (Nosek et al., 2002) with one slight modification. In the behavioral task, proper names were used to cue the male and female categories (e.g.~\enquote{John,} \enquote{Amy}), but because there are not direct translation equivalents of proper names, we instead used a set of generic gendered words which had been previously used for a different version of the gender IAT (e.g., ``man,'' ``woman;'' Nosek et al., 2002). Our linguistic stimuli were therefore a set of 8 female and 8 male Target Words (identical to Study 1a), and the set of 8 Attribute Words words used in the Project Implicit gender-career IAT: 8 related to careers (\enquote{career,} \enquote{executive,} \enquote{management,} \enquote{professional,} \enquote{corporation,} \enquote{salary,} \enquote{office,} \enquote{business}) and 8 related to families (\enquote{family,} \enquote{home,} \enquote{parents,} \enquote{children,} \enquote{cousins,} \enquote{marriage,} \enquote{wedding,} \enquote{relatives}). For one language, Filipino, we were unable to obtain translations from a native speaker, and so Filipino translations were compiled from dictionaries.

We used these translations to calculate a gender bias effect size from word embedding models trained on text in each language. Our effect size measure is a standardized difference score of the relative similarity of the target words to the target attributes (i.e.~relative similarity of male to career vs.~relative similarity of female to career). Our effect size measure is identical to that used by CBN with an exception for grammatically gendered languages (see SM for replication of CBN on our corpora). Namely, for languages with grammatically gendered Attribute Words (e.g., niñas for female children in Spanish), we calculated the relationship between Target Words and Attribute Words of the same gender (i.e.~\enquote{hombre} (man) to \enquote{niños} and \enquote{mujer} (woman) to \enquote{niñas}). In cases where there were multiple translations for a word, we averaged across words such that each of our target words was associated with a single vector in each language. In cases where the translation contained multiple words, we used the entry for the multiword phrase in the model when present, and averaged across words otherwise. Like the psychological measures of bias from the Project Implicit data, larger values indicate larger gender bias.

We calculated gender bias estimates using the same word embedding models as in Study 1a (Subtitle and Wikipedia corpora). We excluded languages from the analysis for which 20\% or more of the target words were missing from the model or the model did not exist. This led us to exclude one language (Zulu) from the analysis of the Wikipedia corpus and six languages from the analysis of the Subtitle corpus (Chinese, Croatian, Hindi, Japanese, Filipino, and Zulu). Our final sample included 25 languages in total (\emph{N}\textsubscript{Wikipedia} = 25; \emph{N}\textsubscript{Subtitle} = 20), representing 8 language families. Finally, we calculated language-level measures for four additional measures by averaging across countries whose participants speak the same language: implicit and explicit psychological gender bias (estimated from the Project Implicit dataset), percentage of women in STEM fields, and median country age.

\hypertarget{results-2}{%
\subsubsection{Results}\label{results-2}}

\begin{figure}
\centering
\includegraphics{iat_lang_pnas_files/figure-latex/unnamed-chunk-11-1.pdf}
\caption{\label{fig:unnamed-chunk-11}Implicit gender bias (adjusted for age, sex, and block order) as a function of the linguistic gender bias derived from word-embeddings (Study 1b). Each point corresponds to a language, with the size of the point corresponding to the number of participants speaking that langauge. Linguistic biases are estimated from models trained on text in each language from Subtitle (left) and Wikipedia (right) corpora. Larger values indicate a larger bias to associate men with the concept of career and women with the concept of family. Error bands indicate standard error of the linear model estimate.}
\end{figure}

Despite the differences in the specific content conveyed by the Subtitles and Wikipedia corpus, the estimated gender bias for each language was similar across the two corpora (\emph{t}(19) = -0.06, \emph{p} = 0.95). We next examined the relationship between these estimates of gender bias for each language and the mean IAT bias score for participants from countries where that language was dominant (and, we assume, was the native language of most of these individuals). Implicit gender bias was positively correlated with estimates of language bias from both the Subtitle (\emph{r} = 0.5, \emph{p} = 0.02) and Wikipedia trained models (\emph{r} = 0.48, \emph{p} = 0.01; Fig. 2; Table 1 shows the language-level correlations between all variables in Studies 1b and 2). The relationship between implicit gender bias and language bias remained reliable after partialling out the effect of median country age (Subtitle: \emph{r} = 0.42, \emph{p} = 0.04; Wikipedia: \emph{r} = 0.43, \emph{p} = 0.04). Linguistic gender bias was not correlated with explicit gender bias (Subtitle: \emph{r} = -0.08, \emph{p} = 0.74; Wikipedia: \emph{r} = 0.34, \emph{p} = 0.09). Estimates of language bias from the Subtitle corpus were correlated with the objective measure of gender equality, percentage of women in STEM fields (\emph{r} = -0.55, \emph{p} = 0.02); this relationship was not reliable for the Wikipedia corpus (\emph{r} = -0.19, \emph{p} = 0.4).

\begingroup\fontsize{6}{8}\selectfont

\begin{longtable}{llllllllll}
\caption{\label{tab:bigtable}Correlation (Pearson's r) for all measures in Study 1 and 2 at the level of languages. Top panel shows simple correlations; bottom panel shows partial correlations controlling for median country age. Single asterisks indicate p < .05 and double asterisks indicate p < .01. The + symbol indicates a marginally significant p-value, p < .1.}\\
\toprule
\rotatebox{90}{ } & \rotatebox{90}{Residualized Explicit Bias} & \rotatebox{90}{Residualized Implicit Bias (IAT)} & \rotatebox{90}{Percent Women in STEM} & \rotatebox{90}{Language IAT (Subtitle)} & \rotatebox{90}{Language IAT (Wikipedia)} & \rotatebox{90}{Prop. Gendered Occupation Labels} & \rotatebox{90}{Occupation Bias (Subtitle)} & \rotatebox{90}{Occupation Bias (Wikipedia)} & \rotatebox{90}{Median Country Age}\\
\midrule
\addlinespace[0.3em]
\multicolumn{10}{l}{\textbf{Simple Correlations}}\\
\hspace{1em}Residualized Explicit Bias &  & \ .18 & \ .18 & -.08 & \ .34+ & \ .11 & \ .28 & \ .29 & -.07\\
\hspace{1em}Residualized Implicit Bias (IAT) & \ .18 &  & -.53* & \ .50* & \ .48* & \ .57** & \ .64** & \ .59** & \ .61**\\
\hspace{1em}Percent Women in STEM & \ .18 & -.53* &  & -.55* & -.19 & -.35 & -.39 & -.32 & -.42+\\
\hspace{1em}Language IAT (Subtitle) & -.08 & \ .50* & -.55* &  & \ .51* & \ .28 & \ .42+ & \ .40+ & \ .31\\
\hspace{1em}Language IAT (Wikipedia) & \ .34+ & \ .48* & -.19 & \ .51* &  & \ .18 & \ .28 & \ .44* & \ .25\\
\hspace{1em}Prop. Gendered Occupation Labels & \ .11 & \ .57** & -.35 & \ .28 & \ .18 &  & \ .75** & \ .70** & \ .35+\\
\hspace{1em}Occupation Bias (Subtitle) & \ .28 & \ .64** & -.39 & \ .42+ & \ .28 & \ .75** &  & \ .80** & \ .36\\
\hspace{1em}Occupation Bias (Wikipedia) & \ .29 & \ .59** & -.32 & \ .40+ & \ .44* & \ .70** & \ .80** &  & \ .34+\\
\hspace{1em}Median Country Age & -.07 & \ .61** & -.42+ & \ .31 & \ .25 & \ .35+ & \ .36 & \ .34+ & \\
\addlinespace[0.3em]
\multicolumn{10}{l}{\textbf{Partial Correlations}}\\
\hspace{1em}Residualized Explicit Bias &  & \ .28 & \ .16 & -.06 & \ .38+ & \ .14 & \ .33 & \ .34 & \\
\hspace{1em}Residualized Implicit Bias (IAT) & \ .28 &  & -.38+ & \ .42* & \ .43* & \ .48* & \ .57** & \ .52** & \\
\hspace{1em}Percent Women in STEM & \ .16 & -.38+ &  & -.49* & -.09 & -.23 & -.28 & -.20 & \\
\hspace{1em}Language IAT (Subtitle) & -.06 & \ .42* & -.49* &  & \ .47* & \ .20 & \ .35+ & \ .33 & \\
\hspace{1em}Language IAT (Wikipedia) & \ .38+ & \ .43* & -.09 & \ .47* &  & \ .11 & \ .21 & \ .39+ & \\
\hspace{1em}Prop. Gendered Occupation Labels & \ .14 & \ .48* & -.23 & \ .20 & \ .11 &  & \ .71** & \ .66** & \\
\hspace{1em}Occupation Bias (Subtitle) & \ .33 & \ .57** & -.28 & \ .35+ & \ .21 & \ .71** &  & \ .77** & \\
\hspace{1em}Occupation Bias (Wikipedia) & \ .34 & \ .52** & -.20 & \ .33 & \ .39+ & \ .66** & \ .77** &  & \\
\bottomrule
\end{longtable}\endgroup{}

\hypertarget{study-1c-a-pre-registered-study-of-british-versus-american-english-biases}{%
\subsection{Study 1c: A pre-registered study of British versus American English biases}\label{study-1c-a-pre-registered-study-of-british-versus-american-english-biases}}

In Study 1c, we conducted a confirmatory, pre-registered analysis of our hypothesis that biases present in language statistics are reflected in the psychological biases of speakers of those languages. To do this, we leveraged an exisiting dataset from the Attitiudes, Identities, and Individual Differences Study (AIID; Hussey et al., 2019) containing measures of IAT performance from over 200,000 participants for a wide range of IAT types (e.g.~career - family, team - individual, etc.). Because most participants in this sample were English speakers, we compared biases between participants who spoke two different dialects of English: British and American. For each of the 31 IAT types in the sample, we predicted that the degree to which that bias was present in a speaker's English dialect (British or American) would predict the magnitude of their psychological bias, as measured by the IAT.

\hypertarget{method}{%
\subsection{Method}\label{method}}

The AIID datset was partioned into two samples: exploratory (15\%) and confirmatory (85\%). Based on the exploratory sample, we pre-registered our analysis plan for the confirmatory sample (\url{https://osf.io/3f9ed}). We note where are analysis diverges from the preregistration plan below.

Of the 95 IAT types present in the dataset, we identified 31 types based on the following criteria: (1) stimuli were words rather than pictures, and (2) 75\% of the target words for each IAT test were present in both the BNC and COCA models. To measure the bias in language, we trained word embedding models on comparably-sized corpora of British (British National Corpus; Burnard, 1995) and American English (Corpus of Contemporary American English; see SM for details; Davies, 2008). We then calculated a language bias effect size for each IAT in each English dialect, using the same method as in Study 1b.

Within the confirmatory AIID dataset, there were 187,969 administrations of the IAT from participants in the United States (USA) or United Kingdom (UK). Our exclusion procedure for the behavioral data was similiar to Study 1a above (see SM for details). Our final sample included data from 135,240 administrations of the IAT across the 31 IAT types (USA: \emph{N} = 127,630; UK: \emph{N} = 7,610). Each participant completed an average of 6.13 IAT types (\emph{SD} = 3.99). For each administrations of the IAT, we calculated a residual IAT score, controlling for sex, age, education, task order (relative ordering of implicit versus explicit measures), and block order (relative ordering of congruent versus incongruent mappings on IAT task).

We fit a linear mixed effect model predicting the magnitude of the bias for each participant with language dialect (British or American English), magnitude of the bias difference between the two dialects, and an interaction betwen these two terms as fixed effect. We included participant and IAT type as random intercepts. This model differs from the pre-registered analysis, which is also consistent with results of the presented analysis, but does not account for participant-level variance (see SM for results of pre-registered model).

\hypertarget{results-3}{%
\subsection{Results}\label{results-3}}

As predicted, there was a reliable interaction between language dialect and the magnitude of the bias difference between the two dialects (\(\beta\) = .05, \emph{SE} = .02, \emph{t} = 2.88; see SM for full model results), suggesting that language bias was a reliable predictor of implicit bias. Figure 3 shows the difference in bias (British - American) in language and behavior for each of the 31 IATs in our dataset.

\begin{figure}
\centering
\includegraphics{iat_lang_pnas_files/figure-latex/1cplot-1.pdf}
\caption{\label{fig:1cplot}Difference (British - American) in implicit bias versus linguistic bias for each of 31 IAT types.}
\end{figure}

\hypertarget{discussion}{%
\subsubsection{Discussion}\label{discussion}}

In Study 1, we found that a previously-reported psychological gender bias -- the bias to associate men with career and women with family -- was correlated with the magnitude of that same bias as measured in the language statistics of 25 languages. Participants completing the IAT in countries where the dominant language had stronger associations between men and career words, and women and family words, showed stronger biases on the gender-career IAT. In a pre-registered, confirmatory analysis, we also find that this pattern extends to other biases: In a comparision of 31 different IAT types, the magnitude of the bias in speaker's language predicted their behavioral bias, as measured by the IAT. These results are consistent with both the \emph{language-as-reflection} and \emph{language-as-causal-factor} hypotheses. In Study 2, we try to better distinguish between these hypotheses by investigating whether the gender-career bias is associated with two structural features of language: grammatical gender and the presence of gendered occupation terms (e.g., waiter/waitress).

\hypertarget{study-2-gender-bias-and-lexicalized-gender}{%
\section{Study 2: Gender bias and lexicalized gender}\label{study-2-gender-bias-and-lexicalized-gender}}

In Study 1 we examined cross-linguistic differences in gender bias without any reference to structural differences that exist in the languages in our sample. One such structural difference concerns the grammaticalization of gender. Some languages such as Spanish mark gender distinctions in a grammatically obligatory way, e.g., \enquote{enfermero} (nurse-\textsc{masc}) versus \enquote{enfermera} (nurse-\textsc{fem}). Grammatical gender systems frequently demand gender-based agreement, e.g., \enquote{el enfermero alto} (the tall nurse-\textsc{masc}) versus \enquote{la enfermera alta} (the tall nurse-\textsc{fem}), which while informationally redundant, may act to amplify gender biases in the language. Another way languages convey gender is through gender-specific terms such as \enquote{waiter} vs.~\enquote{waitress.} Languages with grammatical gender do tend to use more such terms, but the two are distinct. French has grammatical gender, but many occupation terms are gender-neutral (e.g., auteur, athlète, juge).

In Study 2, we examined whether grammatical gender and use of gender-specific occupation terms are associated with a greater psychological gender bias and whether this relationship is further mediated by language statistics. Finding such associations would lend support to the language-as-causal-factor hypothesis because grammatical gender and (to a lesser degree) lexical gender encoding are relatively stable features of language. Although both can change over time, these changes are somewhat independent of the propositional content conveyed by language. For example, a Finnish document about nursing being unsuitable for men would still use a gender-neutral form of \enquote{nurse} while a Spanish document promoting nursing careers to men would be committed to using gender-marked forms.

\hypertarget{methods-2}{%
\subsubsection{Methods}\label{methods-2}}

We identified 20 occupation names that were likely to have corresponding terms in all 25 of our languages,
and that were balanced in terms of their perceived gender bias in the workforce (Misersky et al., 2014). We then translated these words into each of the 25 languages in our sample, distinguishing between male and female variants (e.g., \enquote{waiter} vs.~\enquote{waitress}) where present. The words were translated by consulting native speakers and dictionaries, as necessary.

We coded each language for the presence or absence of a sex-based grammatical gender system using WALS (Dryer \& Haspelmath, 2013) and other sources, as necessary. To estimate the extent to which a language lexically encoded gender, we calculated the proportion of occupations within each language for which the male and female forms differed. Larger values indicate a preponderance for more gender-specific forms in a language. Finally, we also estimated the extent to which each occupation term was gender biased in its language statistics using word embedding models trained in each language on the Subtitle and Wikipedia corpora. For each occupation form, we estimated its bias in language statistics using the same pairwise similarity metric as in Study 1a, and then averaged across occupations within a language to get a language-level estimate of gender bias. Larger values indicate greater gender bias in language statistics. We then compared each of the three language measures (grammatical gender, proportion specific gender forms, and bias in language statistics for occupation words) to the psychological gender measures described in Study 1b (implicit and explicit bias, adjusted for age, sex and block order).

\hypertarget{results-4}{%
\subsubsection{Results}\label{results-4}}

\begin{figure}
\centering
\includegraphics{iat_lang_pnas_files/figure-latex/unnamed-chunk-14-1.pdf}
\caption{\label{fig:unnamed-chunk-14}Implicit gender bias (adjusted for age, sex, and block order) as a function of the proportion of gender-specific labels for set of words referring to occupations. Each point corresponds to a language, with the size of the point corresponding the number of participants speaking that language. Error band indicates standard error of the linear model estimate.}
\end{figure}

In additive linear models controlling for median country age, there was no difference in implicit or explicit psychological gender bias for speakers of languages with a grammatical gender system (\emph{N} = 12), compared to those without (\emph{N} = 13; Implicit: \(\beta\) = 0; \emph{SE} = 0.01; \emph{t} = -0.43; Explicit: \(\beta\) = -0.09; \emph{SE} = 0.07; \emph{t} = -1.23). Languages with grammatical gender systems were more likely to have gender-specific terms for occupations (\emph{t}(14.89) = 4.85, \emph{p} \textless{} .001). Implicit gender bias was reliably correlated with degree of gender-specific marking on occupation words: Languages with more gender-specific forms tended to have speakers with greater psychological gender bias (\emph{r} = 0.57, \emph{p} \textless{} .01), even after partialling out the effect of median country age (\emph{r} = 0.48, \emph{p} = 0.02; Table 1). There was no relationship between explicit psychological gender bias and lexical marking of occupation words after partialling out the effect of median country age (\emph{r} = 0.14, \emph{p} = 0.51).

\begin{figure}
\centering
\includegraphics{iat_lang_pnas_files/figure-latex/unnamed-chunk-15-1.pdf}
\caption{\label{fig:unnamed-chunk-15}Implicit gender bias (adjusted for age, sex, and block order) as a function of mean gender bias of words referring to occupations, with each point corresponding to a language (Study 2). The size of the point corresponds the number of participants speaking that language. Occupation gender bias is estimated for each language from word embedding models trained on Subtitle (left) and Wikipedia (right) corpora. Error bands indicate standard error of the linear model estimate.}
\end{figure}

We next examined whether having gender-specific forms for a particular occupation was associated with greater gender bias in the language statistics for that form. We fit a mixed effect model predicting degree of gender bias in language statistics (estimated from word embedding models) as a function of degree of distinctiveness between male and female forms for that word, with random intercepts and slopes by language. Degree of form distinctiveness was a strong predictor of language statistics for models trained on both the Subtitle corpus (\(\beta\) = 0.59; \emph{SE} = 0.07; \emph{t} = 8.72) and Wikipedia corpus (\(\beta\) = 0.81; \emph{SE} = 0.09; \emph{t} = 9.48), with words with shared male and female forms tending to have less gender bias. This relationship also held at the level of languages: Languages with more distinct forms had a greater gender-career bias in language statistics (Subtitle: \emph{r} = 0.75, \emph{p} \textless{} .01; Wikipedia: \emph{r} = 0.7, \emph{p} \textless{} .01; Fig. 4).

Finally, we examined the relationship between gender bias in language statistics and psychological gender biases at the level of languages. Unlike in Study 1, all the target words in the present study referred to people (occupations) and thus potentially could be marked for the gender of the referenced person. Consequently, if explicit gender marking drives language statistics, we should expect to see a strong positive relationship at the level of languages between bias in language statistics \emph{for occupation words} and psychological biases for speakers of that language. Consistent with this prediction, gender bias in language statistics for occupation words was positively correlated with implicit gender bias (Subtitle: \emph{r} = 0.64, \emph{p} \textless{} .01; Wikipedia: \emph{r} = 0.59, \emph{p} \textless{} .01), and remained reliable after partialling out the effect of median country age (Subtitle: \emph{r} = 0.57, \emph{p} \textless{} .01; Wikipedia: \emph{r} = 0.52, \emph{p} = 0.01; Fig. 5). There was no relationship between language statistics for occupation words and explicit psychological gender bias, even after partialling out the effect of median country age (Subtitle: \emph{r} = 0.33, \emph{p} = 0.12; Wikipedia: \emph{r} = 0.34, \emph{p} = 0.11).

To understand the relative predictive power of language statistics and distinct forms, we fit an additive linear model predicting implicit bias from language statistics and proportion distinct forms, controlling for median country age. Because language statistics for occupation terms and proportion distinct forms were highly colinear (Wikipedia: \emph{r} = 0.70, \emph{p} \textless{} .001; Subtitle: \emph{r} = 0.75, \emph{p} \textless{} .001), we used the estimate of bias in language statistics for each language based on the set IAT words described in Study 1b. Both gender bias in language statistics (based on IAT words) and the proportion of gender-specific occupation titles were independent predictors of implicit bias. The two predictors accounted for 49\% of variance in implicit bias when using the Subtitle corpus and 60\% of variance for the Wikipedia corpus. Full model results are reported in the SM.

The high degree of collinearity between language statistics for occupation terms and proportion gender-specific occupations forms is consistent with a causal model in which language statistics mediate the effect of gender-specific forms on implicit bias: The presence of distinct forms referring to people of different genders \emph{leads to} biased language statistics, which in turn leads to gender bias in behavior. Consistent with this model, a bootstrap test of mediation revealed a marginal effect for the Subtitle model (path-ab = 0.28, \emph{p} = 0.10; Alfons, 2018), and significant mediation effect for the Wikipedia model (path-ab = 0.35, \emph{p} = 0.04).\footnote{Though our power to detect this effect is relatively low (approximately, .4; Schoemann, Boulton, \& Short, 2017).}

\hypertarget{discussion-1}{%
\section{Discussion}\label{discussion-1}}

In Study 2, we asked whether structural features of language -- the presence of a grammatical gender systems and the propensity to lexicalize gender distinctions -- correlated with implicit bias. Grammatical gender was not reliably correlated with implicit bias. Languages that use more gender-specific occupation terms, however did predict a greater implicit bias. There is some evidence that the effect of lexical gender distinctions on implicit bias may be mediated by the influence this terminology introduces on the ways that gender is statistically encoded in different language. What does this finding mean for our two hypotheses? The fact that, e.g., German explicitly marks the gender of professors while English does not, has cognitive consequences for German speakers; it is not simply a matter of current cultural differences being reflected in language. Language does not merely reflect our biases, it seems to contribute to them.

\hypertarget{general-discussion}{%
\section{General Discussion}\label{general-discussion}}

Where do we get our gender stereotypes? Non-linguistic experiences surely play a role, but might we also be learning our biases from the statistics of language to which we are exposed? We used a large-scale dataset of Implicit Association Tests (IATs) measuring the bias to associate men with career and women with family and related people's measured implicit bias to the statistics of the dominant language spoken in the country of the participants. In Study 1, we found that languages with a greater gender bias in their distributional structure, tend to have speakers that have stronger implicit biases. In Study 2, we found a positive relationship between a structural language feature -- the prevalence of gender-marked occupation terms -- and implicit bias. There is suggestive evidence that this greater implicit bias is mediated by the greater gender bias encoded in the distributional patterns of gender-marked terms.

Our work is the first to characterize the relationship between broad structural patterns in language and cultural stereotypes. The positive correlation that we find between gender bias in language and gender bias in speakers is consistent both with language playing a causal role in the emergence of cultural stereotypes and the idea that language merely reflects existing stereotypes of its speakers. The positive correlation we find in Study 2 between prevalence of gender-specific terms and implicit bias is most parsimoniously explained by the language-as-causal-factor hypothesis because it is unlikely that language forms change on a timescale that could directly reflect behavior. The two causal forces are not mutually exclusive, and in fact may amplify the effects of each other. Future work could use experimental methods to manipulate language statistics in order to more directly examine these causal influences.

A central contribution of the current work is that it sheds light onto the potential origins of psychological biases that exist at the level of the individual. Given observed large-scale stuctural patterns of gender inequality, such as differences in STEM participation, researchers from a range of fields have sought to understand the individual-level causal processes that led to the emergence of structural inequality. But, critically, these previous efforts have taken properties of the indidividual -- such as feelings of self-efficacy in science (Stoet \& Geary, 2018) and general preferences (Falk \& Hermle, 2018) -- as largely exogeneous. Here, we provide a potential explanation of the origins of these psychological biases by arguing that exposure to biased language statistics could play a causal role in the emergence of these biases at the level of the invididual. Consistent with this account, biases in language statistics are correlated with previous individual-level predictors of STEM inequality, such as self-efficacy in science and general preferences (see SM for details).

One limitation of our work is the reliance on the IAT, which has been criticized for both its low reliability (Lane, Banaji, Nosek, \& Greenwald, 2007) and limited external validity (Fazio \& Olson, 2003). Issues of reliability are less relevant here because we use the IAT to measure group-level differences rather than as an individual-difference measure. However, concerns about validity are important particularly because we find that language measures and explicit psychological measures of gender bias are uncorrelated, though explicit bias was measured in a fairly coarse way. Understanding the full import of linguistic biases on cultural stereotypes would therefore require obtaining measures more closely related to real-world behavior.

Cultural stereotypes are acquired through experience. Here, we show that group-level differences in implicit bias are strongly correlated with the strength of gender bias encoded in the statistics of different languages. This pattern suggests that the statistics of language use are an important source of cultural experience: The mere process of listening to and producing language exposes one to statistics that may lead to the formation of cultural stereotypes. Many cultural associations present in the statistics of language may be innocuous -- indeed, these statistics may be an important mechanism through which cultural information is transmitted (Lupyan \& Lewis, 2017). But, in other cases, like the kind of gender stereotypes investigated here, language may play a powerful role in their formation, and ultimately contribute to undesirable real-world consequences like gender inequality in STEM. Understanding the causal role that language plays in the formation of these stereotypes is therefore an important first step to changing these consequences.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-robmed}{}%
Alfons, A. (2018). \emph{Robmed: (Robust) mediation analysis}. Retrieved from \url{https://CRAN.R-project.org/package=robmed}

\leavevmode\hypertarget{ref-bian2017gender}{}%
Bian, L., Leslie, S.-J., \& Cimpian, A. (2017). Gender stereotypes about intellectual ability emerge early and influence children's interests. \emph{Science}, \emph{355}(6323), 389--391.

\leavevmode\hypertarget{ref-bojanowski2016enriching}{}%
Bojanowski, P., Grave, E., Joulin, A., \& Mikolov, T. (2016). Enriching word vectors with subword information.

\leavevmode\hypertarget{ref-burnard1995users}{}%
Burnard, L. (1995). \emph{Users reference guide for the british national corpus}. Oxford University Computing Services.

\leavevmode\hypertarget{ref-caliskan2017semantics}{}%
Caliskan, A., Bryson, J. J., \& Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. \emph{Science}, \emph{356}(6334), 183--186.

\leavevmode\hypertarget{ref-ceci2011understanding}{}%
Ceci, S. J., \& Williams, W. M. (2011). Understanding current causes of women's underrepresentation in science. \emph{Proceedings of the National Academy of Sciences}, 3157--3162.

\leavevmode\hypertarget{ref-ciafactbook}{}%
CIA. (2017). The World Factbook. Retrieved from \url{https://www.cia.gov/library/publications/the-world-factbook/index.html}

\leavevmode\hypertarget{ref-cimpian2011generic}{}%
Cimpian, A., \& Markman, E. M. (2011). The generic/nongeneric distinction influences how children interpret new information about social others. \emph{Child Development}, \emph{82}(2), 471--492.

\leavevmode\hypertarget{ref-cimpian2012good}{}%
Cimpian, A., Mu, Y., \& Erickson, L. C. (2012). Who is good at this game? Linking an activity to a social category undermines children's achievement. \emph{Psychological Science}, \emph{23}(5), 533--541.

\leavevmode\hypertarget{ref-davies2008corpus}{}%
Davies, M. (2008). The corpus of contemporary american english (coca), \emph{https://corpus.byu.edu/coca/.}(https://corpus.byu.edu/coca/.), https://corpus.byu.edu/coca/.

\leavevmode\hypertarget{ref-wals}{}%
Dryer, M. S., \& Haspelmath, M. (Eds.). (2013). \emph{WALS online}. Leipzig: Max Planck Institute for Evolutionary Anthropology. Retrieved from \url{http://wals.info/}

\leavevmode\hypertarget{ref-falk2018relationship}{}%
Falk, A., \& Hermle, J. (2018). Relationship of gender differences in preferences to economic development and gender equality. \emph{Science}, \emph{362}(6412), eaas9899.

\leavevmode\hypertarget{ref-fazio2003implicit}{}%
Fazio, R. H., \& Olson, M. A. (2003). Implicit measures in social cognition research: Their meaning and use. \emph{Annual Review of Psychology}, \emph{54}(1), 297--327.

\leavevmode\hypertarget{ref-firth1957synopsis}{}%
Firth, J. (1957). A synopsis of linguistic theory 1930-1955 in studies in linguistic analysis, Philological Society. Oxford.

\leavevmode\hypertarget{ref-forscher2016meta}{}%
Forscher, P. S., Lai, C., Axt, J., Ebersole, C. R., Herman, M., Devine, P. G., \& Nosek, B. A. (2016). A meta-analysis of change in implicit bias.

\leavevmode\hypertarget{ref-gelman2004mother}{}%
Gelman, S. A., Taylor, M. G., Nguyen, S. P., Leaper, C., \& Bigler, R. S. (2004). Mother-child conversations about gender: Understanding the acquisition of essentialist beliefs. \emph{Monographs of the Society for Research in Child Development}, i--142.

\leavevmode\hypertarget{ref-greenwald1998measuring}{}%
Greenwald, A. G., McGhee, D. E., \& Schwartz, J. L. (1998). Measuring individual differences in implicit cognition: The Implicit Association Test. \emph{Journal of Personality and Social Psychology}, \emph{74}(6), 1464.

\leavevmode\hypertarget{ref-greenwald2003understanding}{}%
Greenwald, A. G., Nosek, B. A., \& Banaji, M. R. (2003). Understanding and using the Implicit Association Test: An improved scoring algorithm. \emph{Journal of Personality and Social Psychology}, \emph{85}(2), 197.

\leavevmode\hypertarget{ref-aiid}{}%
Hussey, I., Hughes, S., Lai, C., Ebersole, C. S., Axt, J., \& Nosek, B. A. (2019). Attitudes, Identities, and Individual differences (AIID) Study.

\leavevmode\hypertarget{ref-joulin2016bag}{}%
Joulin, A., Grave, E., Bojanowski, P., \& Mikolov, T. (2016). Bag of tricks for efficient text classification. \emph{arXiv Preprint arXiv:1607.01759}.

\leavevmode\hypertarget{ref-landauer1997solution}{}%
Landauer, T. K., \& Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. \emph{Psychological Review}, \emph{104}(2), 211.

\leavevmode\hypertarget{ref-lane2007understanding}{}%
Lane, K. A., Banaji, M. R., Nosek, B. A., \& Greenwald, A. G. (2007). Understanding and using the Implicit Association Test: IV. \emph{Implicit Measures of Attitudes}, 59--102.

\leavevmode\hypertarget{ref-leslie2015expectations}{}%
Leslie, S.-J., Cimpian, A., Meyer, M., \& Freeland, E. (2015). Expectations of brilliance underlie gender distributions across academic disciplines. \emph{Science}, \emph{347}(6219), 262--265.

\leavevmode\hypertarget{ref-lison}{}%
Lison, P., \& Tiedemann, J. (2016). OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles. In \emph{Proceedings of the 10th International Conference on Language Resources and Evaluation}.

\leavevmode\hypertarget{ref-lund1996producing}{}%
Lund, K., \& Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. \emph{Behavior Research Methods, Instruments, \& Computers}, \emph{28}(2), 203--208.

\leavevmode\hypertarget{ref-lupyan2017wordsascues}{}%
Lupyan, G., \& Lewis, M. (2017). From words-as-mappings to words-as-cues: The role of language in semantic knowledge. \emph{Language, Cognition and Neuroscience}, 1--19. doi:\href{https://doi.org/10.1080/23273798.2017.1404114}{10.1080/23273798.2017.1404114}

\leavevmode\hypertarget{ref-mikolov2013efficient}{}%
Mikolov, T., Chen, K., Corrado, G., \& Dean, J. (2013). Efficient estimation of word representations in vector space.

\leavevmode\hypertarget{ref-miller2015women}{}%
Miller, D. I., Eagly, A. H., \& Linn, M. C. (2015). Women's representation in science predicts national gender-science stereotypes: Evidence from 66 nations. \emph{Journal of Educational Psychology}, \emph{107}(3), 631.

\leavevmode\hypertarget{ref-misersky2014norms}{}%
Misersky, J., Gygax, P. M., Canal, P., Gabriel, U., Garnham, A., Braun, F., \ldots{} others. (2014). Norms on the gender perception of role nouns in Czech, English, French, German, Italian, Norwegian, and Slovak. \emph{Behavior Research Methods}, \emph{46}(3), 841--871.

\leavevmode\hypertarget{ref-nosek2002harvesting}{}%
Nosek, B. A., Banaji, M. R., \& Greenwald, A. G. (2002). Harvesting implicit group attitudes and beliefs from a demonstration web site. \emph{Group Dynamics: Theory, Research, and Practice}, \emph{6}(1), 101.

\leavevmode\hypertarget{ref-rhodes2008preschoolers}{}%
Rhodes, M., \& Brickman, D. (2008). Preschoolers' responses to social comparisons involving relative failure. \emph{Psychological Science}, \emph{19}(10), 968--972.

\leavevmode\hypertarget{ref-rhodes2018subtle}{}%
Rhodes, M., Leslie, S.-J., Yee, K. M., \& Saunders, K. (2019). Subtle linguistic cues increase girls' engagement in science. \emph{Psychological Science}.

\leavevmode\hypertarget{ref-schoemann2017determining}{}%
Schoemann, A. M., Boulton, A. J., \& Short, S. D. (2017). Determining power and sample size for simple and complex mediation models. \emph{Social Psychological and Personality Science}, \emph{8}(4), 379--386.

\leavevmode\hypertarget{ref-scott2018glasgow}{}%
Scott, G. G., Keitel, A., Becirspahic, M., Yao, B., \& Sereno, S. C. (2018). The Glasgow Norms: Ratings of 5,500 words on nine scales. \emph{Behavior Research Methods}, 1--13.

\leavevmode\hypertarget{ref-simons2018}{}%
Simons, G. F., \& Charles, D. F. (Eds.). (2018). Ethnologue: Languages of the world. Dallas, Texas: Online version: http://www.ethnologue.com. SIL International.

\leavevmode\hypertarget{ref-stoet2018gender}{}%
Stoet, G., \& Geary, D. C. (2018). The gender-equality paradox in science, technology, engineering, and mathematics education. \emph{Psychological Science}, \emph{29}(4), 581--593.

\leavevmode\hypertarget{ref-vanparidon}{}%
Van Paridon, J., \& Thompson, B. (n.d.). Sub2Vec: Word embeddings from OpenSubtitles in 62 languages.

\endgroup


\end{document}
