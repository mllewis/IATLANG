---
title: "Language use shapes cultural biases: Large scale evidence from gender"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Molly Lewis} \\ \texttt{mollyllewis@gmail.com} \\ Department of Psychology  \\ University of Wisconsin-Madison
    \And {\large \bf Gary Lupyan} \\ \texttt{lupyan@wisc.edu} \\ Department of Psychology  \\ University of Wisconsin-Madison}

abstract: 
    "The abstract."
    
keywords:
    "IAT, cultural biases, gender, linguistic relativity."
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(feather)
library(broom)
library(forcats)
library(png)
library(grid)
library(xtable)
library(ggrepel)
library(langcog)
library(compute.es)

TEXT_SIZE <- 10
```

# Introduction
The language we use to communicate a message shapes how our listener interprets that message [@loftus1996eyewitness;@tversky1981framing;@fausey2010subtle]. A listener, for example, is more likely to infer that a person is at fault if the event is described actively (e.g., "she ignited the napkin"), as opposed to passively (e.g., "the napkin ignited"). The formative power of language is perhaps most potent in shaping meanings that necessarily must be learned from others: cultural meanings. In the present paper, we consider one type of cultural meaning---gender---and examine the extent to which differences in languge use may lead to cross-cultural differences in understandings of gender. 

The gender domain is a useful case study of the relationship between language and thought for a number of reasons. First, there is reason to think that abstract domains, like gender,  may be more subject to the influence of language relative to more perceptually grounded domains [@boroditsky2001does]. Second, many langauges encode gender explicitly in their grammar. Third, there is a large body of evidence suggesting that language plays a key role in transmitting social knowledge to children [e.g., @master2012thinking].  And, foruth, gender norms are highly variable across cultures and have clear and important social implications.

For our purposes, we define the hypothesis space of possible relationships between language and gender biases with two broad extremes: (1) language reflects a pre-existing gender bias in its speakers (_language-as-reflection hypothesis_); (2) language causally influences gender biases (_language-as-causal hypothesis_). We assume that the language-as-reflection hypothesis is true to some extent; the way we talk about gender almost certainly reflects how we think about gender.  If, for example, you think nurses are most commonly women, you are likely to use a female pronoun to refer to a generic nurse. Our goal here is to understand the extent to which language may also exert a causal influence on conceptualizations of gender.

In particular, we explore two possible mechanisms by which the way we speak may influence notions of gender.\footnote{These mechanisms are what Whorf (1945) refers to as phenotypes (overt) and cryptotypes (covert).}  The first is through the overt grammatical marking of gender on nouns, which are obligatory in roughly one quarter of languages [e.g., in Spanish, “nina” (girl) and “enfermera” (nurse) both take the gender marker _-a_ to indicate grammatical femininity; @corbett1991]. Because grammatical gender has a natural link to the real world, speakers may assume that grammatical markers are meaningful even when applied to inanimate objects that do not have a biological sex.  In addition, the mere presence of obligatory marking of grammatical gender may  promote bias  by making the  dimension of gender more salient to speakers.

A second route by which language may shape psychological gender is through the distribution of words in use: Words that tend to occur in similar contexts in language may lead speakers to assume---either implicitly or explicitly---that they have similar meanings. For example, statistically, the word “nurse” occurs in many of the same contexts as the pronoun “her,” providing an implicit link between these two concepts that may lead to a bias to assume that nurses are female. This second route may be particularly influential because the bias is encoded in language in a way that is more implicit than grammatical markers of gender.

There is an existing body of experimental work that points to a link between language and psychological gender bias in both adults [e.g.,@phillips2003can] and children [e.g., @sera1994grammatical]. For example, Phillips and Boroditsky (2003) asked Spanish-English and German-English adult bilinguals to make similarity judgements between pairs of pictures depicting an object with a natural gender (e.g., a bride) and one without (e.g., a toaster). They found that participants rated pairs as more similar when the pictures matched in grammatical gender in their native language. While these types of studies provide suggestive evidence for a causal link between language and psychological gender bias, they are limited by the fact that they typically only compare speakers of  2-3 different languages and measure bias in a way that is subject to demand characteristics.

In what follows, we ask whether the way gender is encoded linguistically across 31 different languages predicts cross-cultural variability in the bias to associate men with careers and women with family. We begin in Study 1 by describing cross-cultural variability in psychological gender biases using an implicit measure. In Study 2, we use machine learning methods to describe lexical semantics, and ask whether variability in lexical semantics predicts variability in psychological gender bias. In Study 3, we ask whether the presence of grammatical gender in a language predicts gender bias. Together, our data suggest that language likely plays a causal role in shaping culturally-specific notions of gender.

# Study 1: Cross-cultural variability in gender bias
In Study 1, we describe cross-cultural variability in psychological gender bias. To quantify this bias, we used data from the Implicit Association Task [IAT; @greenwald1998measuring]. The IAT measures the strength of respondents' implicit associations between two pairs of concepts (e.g., male-career/female-family vs.\ male-family/female-career). The underlying assumption of the measure is that concepts that are represented as more similar to each other should be easier to pair together in a behavioral task, compared to two concepts that are relatively dissimilar. Concepts are paired in the task by assigning them to the same response keys in a 2AFC categorization task. In the critical blocks of the task, concepts are assigned to keys in a way that is either bias-congruent (i.e. Key A = male/career; Key B = female/family) or bias-incongruent (i.e. Key A = male/family; Key B = female/career). Participants are then presented with a word related to one of the four concepts and asked to classify it as quickly as possible by responding with one of the two keys. Slower reaction times in the bias-incongruent blocks relative to the bias-congruent blocks are interpreted as indicating an implicit association between the corresponding concepts (i.e.\ a bias to associate male with career, and female with family). 

## Method

```{r}
raw_iat_behavioral <- read_feather("analysis/study1/data/Gender-Career IAT.public.2005-2016.feather") %>%
  select(D_biep.Male_Career_all,sex, countryres, PCT_error_3467, 
         Mn_RT_all_3467, Mn_RT_all_3, Mn_RT_all_4, Mn_RT_all_6, 
         Mn_RT_all_7, assocareer, assofamily, N_ERROR_3, N_ERROR_4,
         N_ERROR_6, N_ERROR_7, N_3, N_4, N_6, N_7) %>%
    rename(overall_iat_D_score = D_biep.Male_Career_all) 

# this file is "Gender-Career/Gender-Career IAT.public.2005-2016.sav" in feather form (taken from: https://osf.io/gmewy/) 
```

```{r, get_complete_data}
MIN_PARTICIPANTS_PER_COUNTRY <- 400

raw_iat_behavioral_complete <- raw_iat_behavioral %>%
                                filter(sex %in% c("f", "m"),
                                       !is.na(countryres), 
                                       countryres != ".",
                                       countryres != "nu",  # not clear what this refers to (it's lower case and not in codebook- Niue seems unlikely)
                                       !is.na(overall_iat_D_score))

country_ns <- raw_iat_behavioral_complete %>%
  count(countryres)  %>%
  filter(n >= MIN_PARTICIPANTS_PER_COUNTRY) %>%
  arrange(-n)

raw_iat_behavioral_complete_dense_country <- raw_iat_behavioral_complete %>%
  filter(countryres %in% country_ns$countryres)
```

```{r, behavioral_exclusions}
# same exclusions as Nosek, Banjali, & Greenwald (2002), pg. 104. 
iat_behavioral <- raw_iat_behavioral_complete_dense_country %>%
  filter(Mn_RT_all_3467 <= 1500, # RTs
         Mn_RT_all_3 <= 1800,
         Mn_RT_all_4 <= 1800,
         Mn_RT_all_6 <= 1800,
         Mn_RT_all_7 <= 1800) %>%
  filter(N_ERROR_3/N_3 <=.25, # errors
         N_ERROR_4/N_4 <=.25,
         N_ERROR_6/N_6 <=.25,
         N_ERROR_7/N_7 <=.25)
```

```{r, tidy_names}
project_implicit_countries <- read_csv("analysis/study1/data/project_implicit_country_codes.csv") 
# these are from Gender-Career_IAT_public_2005-2016_codebook.xlsx; country_name are relabeled from original

iat_behavioral_tidy <- iat_behavioral %>% # join in country names
                          left_join(project_implicit_countries) 

country_ns_final <- iat_behavioral_tidy %>%
                           count(countryres)  
```

We analyzed an existing IAT dataset collected online by Project Implicit [https://implicit.harvard.edu/implicit/; @nosek2002harvesting]\footnote{All analysis code can be found in an online repository: https://github.com/mllewis/IATLANG}. Our analysis included all gender-career IAT scores collected from respondents between 2005 and 2016 who had complete data and  were located in countries with more than `r MIN_PARTICIPANTS_PER_COUNTRY` total respondents (_N_ = `r format(nrow(raw_iat_behavioral_complete_dense_country),big.mark=",")`). We further restricted our sample based on participants' reaction times and errors using the same criteria described in Nosek, Banjai, and Greenwald (2002, pg.\ 104). Our final sample included `r format(nrow(iat_behavioral), big.mark=",")` participants from `r nrow(country_ns_final)` countries, with a median of `r format(round(median(country_ns_final$n)), big.mark = ",")` participants per Country. Note that although the respondents were from largely non-English speaking countries, the IAT was conducted in English. We do not have language background data from the participants, but we assume that most respondents from non-English speaking countries were native speakers of the dominant language of the country and L2 speakers of English. 

<!-- exclusion rate = nosek's (15%) --> 
Several measures have been used in the literature to quantify the difference in reaction time between congruent and incongruent blocks.  Here, we used the most robust measure, D-score, which measures the difference between critical blocks for each participant while controlling for individual differences in response time [@greenwald2003understanding]. For each country, we calculated an effect size as the mean D-score divided by its standard deviation (Cohen's _d_); larger values indicate greater bias.

In addition to the implicit measure, we also analyzed an explicit measure of gender bias. After completing the IAT, participants were asked, "How strongly do you associate the following with males and females?" for both the words "career" and "family." Participants indicated their response on a Likert scale ranging from female (1) to male (7).  We calculated an explicit gender bias score for each participant as the Career response minus the Family response, such that greater values indicate a greater bias to associate males with family.

We expected that countries with greater gender equality would have participants with lower implicit and explicit gender biases. As a measure of gender equality, we used
the Women's Peace and Security Index (WPS, 2017), which measures inclusion, justice, and security of women by country, with larger values indicating higher gender equality. 

## Results

```{r map, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.8, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "IAT gender bias effect size for 48 countries with available data. All countries show a gender bias, with red indicating above average and blue indicating below average bias."}
# Overall effect size
es_behavioral_iat_by_participant <- mean(iat_behavioral_tidy$overall_iat_D_score)/
  sd(iat_behavioral_tidy$overall_iat_D_score)

#mes(mean(iat_behavioral_tidy$overall_iat_D_score), 0, sd(iat_behavioral_tidy$overall_iat_D_score), sd(iat_behavioral_tidy$overall_iat_D_score), 663709, 663709) # tiny CI

# By country effect size
country_means_career_implicit <- iat_behavioral_tidy %>%
  group_by(country_name) %>%
  summarise(mean_iat = mean(overall_iat_D_score), 
            es_behavioral_iat = mean(overall_iat_D_score)/sd(overall_iat_D_score)) 

map_world <- map_data(map = "world") %>%
  mutate(country_name = region) %>%
  mutate(country_name = fct_recode(country_name,
    `United States of America` = "USA", 
    `Russian Federation` = "Russia",
    `Republic of Korea` = "South Korea")) 

map_data <- country_means_career_implicit %>%
  full_join(map_world) %>%
  filter(lat > -57, 
         long > -165) 

SCALE_MIDPOINT <- mean(country_means_career_implicit$es_behavioral_iat, na.rm = T)

# Europe inset (note that Hong Kong is missing from this map)
ggplot(map_data, aes(x = long, y = lat, 
                            group = group, 
                            fill = es_behavioral_iat)) + 
  scale_fill_gradient2("", breaks =  c(.9, 1, 1.1, 1.2),
                       limits=c(SCALE_MIDPOINT - .2, SCALE_MIDPOINT + .2), 
                       midpoint = SCALE_MIDPOINT, 
                       low = "blue", high = "red") +
  geom_polygon(color = "black", size = .1) +
  ggtitle("IAT Gender Bias") +
  theme(text = element_text(size = TEXT_SIZE),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white"),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = c(.1, .35),
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(0.2, "in"),
        legend.text = element_text(size = 8))
```

```{r explicit_implicit_all}
# by-subject correlations (overall_iat_D_score)
subj_means_career_explicit <- iat_behavioral_tidy %>%
    mutate(explicit_dif = assocareer - assofamily) 

imp_exp_subjs <- cor.test(subj_means_career_explicit$overall_iat_D_score, 
         subj_means_career_explicit$explicit_dif)

# by country correlations (effect sizes)
country_means_career_explicit_es <- subj_means_career_explicit %>%
  group_by(country_name) %>%
  summarise(mean_diff = mean(explicit_dif, na.rm = T),
            es_behavioral_iat = mean(overall_iat_D_score)/sd(overall_iat_D_score))

imp_exp_country_es <- cor.test(country_means_career_explicit_es$es_behavioral_iat,
                            country_means_career_explicit_es$mean_diff) 
```

```{r, participant_gender_bias}
men <- filter(iat_behavioral_tidy, sex == "m")
women <- filter(iat_behavioral_tidy, sex == "f")
t_test_gender <- t.test(women$overall_iat_D_score, men$overall_iat_D_score, paired = F)

n1 <- 470215 #nrow(women)# weirdly this doesn't work as variables
n2 <- 193494 #nrow(men) 
gender_participant_d <- tes(t_test_gender$statistic[[1]], n1, n2, verbose = F)
```

We replicate two key findings in the literature on the gender-career IAT [@nosek2002harvesting]. First, participants overall showed a bias to associate men with career and females with family (_d_ = `r round(es_behavioral_iat_by_participant,2)`). Figure 1a shows the mean effect size for each of the 48 countries in our sample, with participants from all countries showing a gender bias (_M_ = `r round(mean(country_means_career_implicit$es_behavioral_iat),2)`; _SD_ = `r round(sd(country_means_career_implicit$es_behavioral_iat), 2)`). Second, implicit and explicit bias measures were moderately correlated both at the level of individual participants (_r_ = `r round(as.numeric(imp_exp_subjs$estimate), 2)`; _p_ < .00001) and at the level of countries (_r_ = `r round(as.numeric(imp_exp_country_es$estimate), 2)`; _p_ = `r round(as.numeric(imp_exp_country_es$p.value), 2)`).

```{r WPS_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.5, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Magnitude of the implicit gender bias (measured by the IAT) predicted by an independent measure of gender equality, Women's Peace and Security Index (WPS).  Each point corresponds to a country with notable points labeled. Contra our prediction, we find that countries with greater gender equality have larger gender implicit bias.[I'LL FIX THESE LABELS LATER]"}

wps <- read_csv("analysis/study1/data/WPS_index.csv") %>%
    mutate(country_name = fct_recode(country_name, 
                                   "Republic of Korea" = "Korea, Republic of",
                                   "UK" = "United Kingdom"))

country_means_career_implicit_with_index <- country_means_career_implicit %>%
  left_join(wps) %>%
  mutate(country_name2 = ifelse(country_name %in% c("Afghanistan", "India", "Netherlands", 
                                                    "Portugal", "China", "United States of America",
                                                    "Sweden", "Hungary", "Philippines", "Colombia", 
                                                    "UK"), country_name, ""),
       country_name2 = fct_recode(country_name2, 
                                   "USA" = "United States of America"),
       country_name2 = fct_relevel(country_name2, "Croatia"))

country_means_career_implicit_index <- cor.test(country_means_career_implicit_with_index$es_behavioral_iat, 
         country_means_career_implicit_with_index$wps_index)

ggplot(country_means_career_implicit_with_index,
       aes(x = wps_index, y = es_behavioral_iat)) +
      geom_point(aes(fill=es_behavioral_iat),pch=21, size = 2) +

      geom_text_repel(aes(label = country_name2), size = 2.3) +
      geom_smooth(method = "lm", alpha = .2, color = "black") +
      scale_fill_gradient2("", breaks =  c(.9, 1, 1.1, 1.2),
                       limits = c(SCALE_MIDPOINT - .2, SCALE_MIDPOINT + .2), 
                       midpoint = SCALE_MIDPOINT, 
                       low = "blue", high = "red") +
      ylab("Psychological Gender Bias \n(effect size)") +
      xlab("Women's Peace and Security Equality Index") +
      #geom_label(aes(label = country_name), size = .9)
      annotate("text", x = .8, y = .87, 
           label = paste0("r = ", round(unlist(country_means_career_implicit_index$estimate, 
                                               use.names = F),2)),
           color = "red",
           size = 5) +
      theme_minimal() +
      theme(text = element_text(size = TEXT_SIZE), 
            legend.position = "none")

country_means_career_explicit_with_index <- country_means_career_explicit_es %>%
  left_join(wps) 

country_means_career_explicit_index <- cor.test(country_means_career_explicit_with_index$mean_diff,
                                                country_means_career_explicit_with_index$wps_index)
```

Our independent measure of gender equality---the Women's Peace and Security Index---was uncorrelated with explicit bias (_r_ = `r unlist(round(country_means_career_explicit_index$estimate, 2), use.names = F)`; _p_ = `r unlist(round(country_means_career_explicit_index$p.value, 2), use.names = F)`). Counter to our expectations, we found that countries such as the Netherlands, with allegedly greatest gender equality, have participants with the highest implicit gender bias according to the IAT (_r_ = `r unlist(round(country_means_career_implicit_index$estimate, 2), use.names = F)`; _p_ <.01; Fig.\ 1b).

## Discussion
In Study 1, we replicate previously reported patterns of gender bias in the gender-career IAT literature, with roughly comparable effect sizes (c.f.\ Nosek, et al., 2002: overall effect: _d_ = .72; explicit-implicit correlation: _r_ = .17; participant gender effect: _d_  =  .1). The weak correlation between explicit and implicit measures is consistent with claims that these two measures tap into different cognitive constructs [@oswald2013predicting].

The novel finding from Study 1 is the direction of the correlation between objective gender bias of a country (as measured by the WPS) and implicit gender bias---participants in countries with greater gender equality have _greater_ implicit gender bias. This robust correlation is particularly surprising given that the English was likely the second language for most of the participants in our sample, which introduces additional noise into the measurement. In the General Discussion, we speculate about possible reasons for this positive correlation. 

# Study 2: Gender bias and semantics
In Study 2, we ask whether participants' psychological gender biases---implicit and explicit---are correlated with the semantic structure of their native languages. For example, are the semantics of the words “woman” and “family” more similar in Hungarian than in English? Both the language-as-reflection and language-as-causal hypotheses predict  a  positive correlation between psychological and semantic gender biases. Importantly, we expect psychological and semantic gender biases to be correlated regardless of the direction of the relationship between psychological and objective gender bias (WPS) found in Study 1. 

To model semantics, we turn to a machine-learning methods for deriving lexical semantics from large corpora of text: auto-encoding neural network models. The underlying assumption of these models is that the meaning of a word can be described by the words it tends to co-occur with---an approach known as distributional semantics [@firth1957synopsis]. Under this approach, a word like "dog" is represented as more similar to "hound" than "banana" because it occurs with words more in common with "hound" than "banana" where co-occurrences are effectively defined at multiple hierarchical levels.

Recent developments in machine learning allow the idea of distributional
semantics to be implemented in a way that both takes into account many
features of local language structure while remaining computationally
tractable. The best known of these word embedding models is
_word2vec_ [@mikolov2013efficient]. The model takes as input a corpus of text and outputs a vector for each word corresponding to its semantics. From these vectors, we can derive a measure of the semantic similarity between two words by taking the distance between their vectors (e.g., cosine distance). Similarity measures estimated from these models have been shown to be highly correlated with human judgements of word similarity [e.g., @hill2015simlex], though more for some forms of similarity than others [@chen2017evaluating].

It turns out that various human biases as measured by the IAT can be predicted from distributional semantics models like word2vec. Caliskan, Bryson,  and Narayanan (2017; henceforth _CBN_) measured the distance in vector space between the same sets of
words that are presented to participants in the IAT task. CBN
found that these distance measures are highly correlated with
reaction times in the behavioral IAT task. For example, in the career-gender IAT, CBN find a bias in the semantics of English to associate males with career  and females with family,  suggesting that the biases measured by the IAT are also found in the lexical semantics of natural
language.

In Study 2, we use the same method described by CBN to measure the biases in the semantics of the natural languages spoken in the countries of participants in Study 1. While CBN only analyzed biases for models trained on English, we extend their method to compare biases across a wide number of languages. To do this, we take advantage of a set of models that have been pre-trained on a corpus of Wikipedia text in a large number of languages [@bojanowski2016enriching]. In Study 2a, we replicate the CBN findings with the Wikipedia corpus; In Study 2b, we show that the implicit gender biases reported in Study 1 for individual countries are correlated with the biases found in the semantics of the natural language spoken by those participants.

## Study 2a: Replication of Caliskan, et al.\ (2017)
### Method
We use a word embedding model that has been pre-trained model on the corpus of English Wikipedia using the fastText algorithm [@bojanowski2016enriching]\footnote{Available here: https://github.com/facebookresearch/fastText/}. The model contains 2,519,370 words with each word represented by a 300 dimensional vector. 

Using the Wikipedia-trained model, we calculate an effect size for each of the 10 biases reported in CBN which correspond to behavioral IAT results existing in the literature:   flowers/insects--pleasant/unpleasant, instruments/weapons--pleasant/unpleasant, European-American/Afro-American--pleasant/unpleasant,\footnote{CBN test three versions of this bias.}  males/females--career/family, math/arts--male/female, science/arts--male/female, mental-disease/physical-disease--permanent/temporary, and young/old--pleasant/unpleasant (labeled as WEAT 1-10 in CBN). We calculate the bias using the same effect size metric described in CBN, a  standardized difference score of the relative similarity of the target words to the target attributes (i.e.\ relative similarity of male to career vs.\ relative similarity of female to career). This measure is  analogous the behavioral effect size measure in Study 1 and, like for the behavioral effect size, larger values indicate larger gender bias.

### Results
```{r WEAT_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.6, fig.height=3.6, set.cap.width=T, num.cols.cap=1, fig.cap = "Effect sizes for the 10 IAT biases types (WEAT 1-10) reported in Caliskan et al.\ (2017; CBN). The effect sizes reported in CBN are plotted against  effect sizes from the Wikipedia corpus.  Point color corresponds to  bias type, and point shape corresponds to the two CBN models trained on different corpora and with different algorithms."}

wiki_es <- read_csv("analysis/study2a/data/caliskan_wiki_es.csv",  
                    col_names = c("test", "study_name",
                                  "wiki", "study_name2",
                                  "WEAT_name"))
# from caliskan_2017_replication.R

caliskan_es  <- read_csv("analysis/study2a/data/caliskan_es.csv")

all_es <- wiki_es %>%
  left_join(caliskan_es) %>%
  gather("es_source", "d", 6:8)  %>%
  filter(es_source != "original") %>%
  mutate(study_name2 = fct_relevel(study_name2, 
                                   "flowers-insects",
                                   "instruments-weapons",
                                   "race", 
                                   "gender-career",
                                   "gender-math", 
                                   "gender-science",
                                   "mental-physical",
                                   "age"),
         point_color = study_name2,
         study_name2 = paste0(as.character(study_name2),
                              "\n", WEAT_name),
         study_name2 = ifelse(es_source == "AC_w2v" & 
                                test %in% c("WEAT_1", 
                                            "WEAT_2",
                                            "WEAT_3",
                                            "WEAT_7", 
                                            "WEAT_8", 
                                            "WEAT_9", 
                                            "WEAT_10", 
                                            "WEAT_6"),
                              study_name2, ""), 
         es_source = fct_recode(es_source,
                                `Common Crawl (GloVe)` = "AC_glove",
                                `Google News (word2vec)` = "AC_w2v"))
ggplot(all_es, aes(x = wiki, 
                   y = d, 
                   shape = es_source, 
                   color = point_color)) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = "darkgrey") +
  geom_point(size = 2.5) +
  ylim(-.6, 2.2) +
  xlim(-.6, 2.2) +
  ggtitle("IAT language effect sizes") +
  xlab("Effect size (Wikipedia corpus)") +
  ylab("Effect size (Caliskan, et al., 2017)") +
  geom_text_repel(aes(label = study_name2), 
                             force = 5, 
                             color = "black", 
                             size = 2.3, 
                             fontface = 'bold', 
                             point.padding = 1) +
  theme_minimal() +
  scale_shape_discrete(name = "Caliskan et al. model") +
  scale_color_discrete(guide = FALSE) +
  theme(text = element_text(size = TEXT_SIZE),
        legend.position = c(.75, 0.15),
        legend.background = element_rect(fill = "white"),
        legend.text = element_text(size = TEXT_SIZE - 3),
        legend.title = element_text(size = TEXT_SIZE - 2),
        legend.margin=margin(3, 3, 0, 3, unit='pt'))
```

```{r english_language_career_es}
gender_bias_es_wiki <- filter(all_es, study_name == "gender-bias-career-family")$wiki[1]

gender_bias_es_cc <- filter(all_es, study_name == "gender-bias-career-family" & es_source == "Common Crawl (GloVe)" )$d[1]

gender_bias_es_gn <- filter(all_es, study_name == "gender-bias-career-family" & es_source == "Google News (word2vec)" )$d[1]
```

Figure 2 shows the effect size measures derived from the Wikipedia corpus plotted against effect size estimates reported by CBN from two different models (trained on the Common Crawl and  Google News corpora). With the exception of biases related to race and age, effect sizes from the Wikipedia corpus are comparable to those reported by CBN. In particular, for the gender-career IAT -- the bias relevant to our current purposes -- we estimate the effect size to be  `r round(gender_bias_es_wiki,2)`, while CBN estimates it as `r gender_bias_es_cc` (Common Crawl) and `r gender_bias_es_gn` (Google News).

## Study 2b: Cross-linguistic gender semantics
With our corpus validated, we next turn toward examining the relationship between psychological and linguistic gender biases. In Study 2b, we estimate the magnitude of the gender-career bias in each of the languages spoken in the countries described in Study 1 and compare it with estimates of behavioral gender bias from Study 1. We predict these two measures should be positively correlated.

### Method

```{r read_in_language_bias_data}
# behavioral means by langauge
countries_to_langs <- read_csv("analysis/study2b/data/languages_with_percent.csv") %>% # this comes from get_language_percentage.R
  mutate(country_name = fct_recode(country_name,
                               "United States of America"= "United States", 
                                GB = "United Kingdom",
                                "Russian Federation" = "Russia",
                                "Republic of Korea" = "South Korea"),
         wiki_language_code = fct_recode(wiki_language_code,
                               "zh"= "zh_yue")) # Cantonese isn't in gtranslate

# get one language per country
unique_langs_per_country <- countries_to_langs %>%
  group_by(country_name) %>%
  arrange(-prop_language)  %>%
  slice(1) %>%
  filter(!language_name %in% c("Niuean", "Cantonese"))
max_prop_es_translations <- unique_langs_per_country %>%
  select(country_name, country_code, language_name, wiki_language_code) 

# average across countries weighting by number of participatns
implicit_behavioral_means_by_language <- country_means_career_implicit %>%
  left_join(max_prop_es_translations) %>%
  left_join(country_ns_final, by = c("country_code"= "countryres")) %>%
  group_by(language_name) %>%
  mutate(normalized_n = n/sum(n)) %>%
  select(wiki_language_code, language_name, country_name, 
         es_behavioral_iat, normalized_n) 

# get language means
language_means_career_implicit_hand <- read.csv("analysis/study2b/data/career_effect_sizes_hand_translations.csv", 
col.names = c("wiki_language_code", "test_id", "test_name", "es_hand_translation"), 
header = F,
                                                fill = TRUE)  %>%
  select(-test_id, -test_name)

all_hand_es <- implicit_behavioral_means_by_language %>%
  group_by(wiki_language_code, language_name) %>%
  summarise(es_behavioral_iat_weighted = weighted.mean(es_behavioral_iat, 
                                                       normalized_n, na.rm = T),
            es_behavioral_iat = mean(es_behavioral_iat)) %>%
  left_join(language_means_career_implicit_hand, by = "wiki_language_code")  %>%
  filter(language_name != "Cantonese" & !is.na(es_behavioral_iat_weighted))  %>%
  ungroup()
```

```{r, read_in_language_bias_data_google, eval = F}
language_means_career_implicit_g <- read.csv("analysis/study2b/data/career_effect_sizes_google_translations_expanded.csv", 
col.names = c("wiki_language_code", "test_id", "test_name", "es_google_translation_ex"), 
header = F,
                                                fill = TRUE)  %>%
  select(-test_id, -test_name)
  
  google_expanded_es <- implicit_behavioral_means_by_language %>%
    group_by(wiki_language_code, language_name) %>%
    summarise(es_behavioral_iat_weighted = weighted.mean(es_behavioral_iat, 
                                                         normalized_n, na.rm = T),
              es_behavioral_iat = mean(es_behavioral_iat)) %>%
    left_join(language_means_career_implicit_g, by = "wiki_language_code")  %>%
    filter(language_name != "Cantonese" & !is.na(es_behavioral_iat_weighted))  %>% ungroup()

behavior_lang_cor_imp_google <- cor.test(google_expanded_es$es_google_translation_ex, google_expanded_es$es_behavioral_iat_weighted)

ggplot(google_expanded_es, aes(x = es_google_translation_ex, 
                      y = es_behavioral_iat_weighted)) +
  geom_smooth(method = "lm", alpha = .2, color = "black") +
  #geom_text_repel(aes(label = language_name), size = 2.3) + 
  geom_point() +
  annotate("text", x = .75, y = .9, 
           label = paste0("r = ", round(unlist(behavior_lang_cor_imp_google$estimate, use.names = F),2)),
           color = "red",
           size = 5) +
  theme_minimal() +
  ggtitle("Gender IAT Behavioral and \nLinguistic Effect Sizes") +
  ylab("Implicit Behavior Gender Bias\n (effect size)") +
  xlab("Language Embedding Gender Bias\n (effect size)") +
  theme(text = element_text(size = TEXT_SIZE))
```

```{r behavior_vs_language_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.5, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Gender bias effect size for each language from the behavioral IAT task (averaging across countries speaking the same primary language; Study 1) versus gender bias effect size estimated from embedding models trained on each language."}
#behavior_lang_cor_imp <- cor.test(all_hand_es$es_hand_translation, all_hand_es$es_behavioral_iat)
behavior_lang_cor_imp <- cor.test(all_hand_es$es_hand_translation, all_hand_es$es_behavioral_iat_weighted)

ggplot(all_hand_es, aes(x = es_hand_translation, y = es_behavioral_iat_weighted)) +
  geom_smooth(method = "lm", alpha = .2, color = "black") +
  geom_text_repel(aes(label = language_name), size = 2.3) + 
  geom_point() +
  annotate("text", x = .75, y = .9, 
           label = paste0("r = ", round(unlist(behavior_lang_cor_imp$estimate, use.names = F),2)),
           color = "red",
           size = 5) +
  theme_minimal() +
  ggtitle("Gender IAT Behavioral and \nLinguistic Effect Sizes") +
  ylab("Implicit Behavior Gender Bias\n (effect size)") +
  xlab("Language Embedding Gender Bias\n (effect size)") +
  theme(text = element_text(size = TEXT_SIZE))
```

For each country included in Study 1, we identified the most frequently spoken language in those countries using the CIA factbook [@ciafactbook]. This included a total of `r length(unique(unique_langs_per_country$language_name))` unique languages. For a sample of 20 of these languages (see Fig.\ 3), we had native speakers translate the set of 32 words from the gender-career IAT, with a slight modification.\footnote{The language sample was determined by accessibility to native speakers, but included languages from a variety of language families.} The original gender-career IAT task  [@nosek2002harvesting] used proper names to cue the male and female categories (e.g. "John," "Amy"). Because there are not direct translation equivalents of proper names across languages, we instead used a set of generic gendered words which had been previously used for a different version of the gender IAT [e.g., "male," "man," "female," "woman;" @nosek2002harvesting].

We used these translations to calculate an effect size from the models trained on Wikipedia in each language, using the same method as in Study 2a. We then compared the effect size of the linguistic gender bias to  the  behavioral gender bias, averaging across countries that spoke the same language and weighting by sample size.  

### Results
```{r explicit_corrs_with_language}
country_means_career_explicit_es_with_weights <- country_means_career_explicit_es %>%
  select(-es_behavioral_iat) %>%
  left_join(max_prop_es_translations, by = "country_name") %>%
  left_join(country_ns_final, by = c("country_code"= "countryres")) %>%
  group_by(language_name) %>%
  mutate(normalized_n = n/sum(n))

language_means_career_explicit_language <- country_means_career_explicit_es_with_weights %>%
  group_by(wiki_language_code, language_name) %>%
  summarise(mean_diff_weighted = 
              weighted.mean(mean_diff,normalized_n, na.rm = T)) %>%
  left_join(language_means_career_implicit_hand, by = "wiki_language_code")

behavior_lang_cor_exp <- cor.test(language_means_career_explicit_language$es_hand_translation, language_means_career_explicit_language$mean_diff_weighted)
```

Implicit IAT gender bias effect sizes were positively correlated with effect sizes of gender bias estimated from the native language embedding model (_r_ = `r unlist(round(behavior_lang_cor_imp$estimate, 2), use.names = F)`; _p_ = `r unlist(round(behavior_lang_cor_imp$p.value, 2), use.names = F)`; Fig.\ 3), suggesting that countries that have more gender bias encoded in their language also have a larger psychological gender bias. Explicit gender bias was not reliably correlated with language gender bias (_r_ = `r unlist(round(behavior_lang_cor_exp$estimate, 2), use.names = F)`; _p_ = `r unlist(round(behavior_lang_cor_exp$p.value, 2), use.names = F)`). 

<!--## Discussion-->

# Study 3: Gender bias and grammar
Study 2 suggests that psychological gender bias and linguistic gender bias are correlated, consistent with both the language-as-causal and language-as-reflection hypotheses. In Study 3, we test the language-as-causal hypothesis more directly by examining whether there is a relationship between psychological gender bias and language along a linguistic dimension that is unlikely to be a subject of rapid change -- namely, grammatical gender. While of course grammars do change, they are less malleable than the semantics of words, and thus less likely to be affected by psychological biases. We predict, therefore, that if language causally influences psychological gender biases, languages that encode gender grammatically will tend to have larger psychological gender biases.

## Method
```{r}
gender_data <- read_csv("analysis/study3/data/gender_grammar.csv") %>%
  select(language_code, language_name, wikipedia_grammar_type) %>%
  mutate(wikipedia_grammar_type2 = ifelse(wikipedia_grammar_type == "none",
                                          "No Grammatical Gender", 
                                          "Grammatical Gender"))
```
For each of the `r length(unique(unique_langs_per_country$language_name))` languages represented in our sample of participants (Study 1), we coded whether gender was encoded grammatically. We used a coarse binary coding scheme, categorizing a language as encoding grammatical gender if it made any gender distinction on noun classes (male, female, common or neuter), and as not encoding gender grammatically otherwise. We coded this distinction on the basis of the WALS typological database [Feature 32a; @wals] where available, and consulted additional resources as necessary. Our sample included `r nrow(filter(gender_data, wikipedia_grammar_type2 == "Grammatical Gender"))` languages that encoded grammatical gender and `r nrow(filter(gender_data, wikipedia_grammar_type2 == "No Grammatical Gender"))` that did not. 

## Results

```{r grammatical_gender_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.5, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Behavioral IAT effect size as a function of whether participants' (assumed) native language encodes gender grammatically. Each point corresponds to a language (N = 31) with outliers shown as triangles (jittered along the x-axis for visibility)."}

## es behavior
iat_means_with_grammar <- all_hand_es %>%
  full_join(gender_data, by = c("wiki_language_code" = "language_code")) %>%
  select(-contains("test")) 

iat_means_with_grammar %>%
  ungroup() %>%
  mutate(outlier_status = ifelse(language_name.x %in% c("Hungarian", "Hindi"), "o", "i")) %>%
  ggplot(aes(x = wikipedia_grammar_type2, 
             y = es_behavioral_iat_weighted,
             color = wikipedia_grammar_type2)) +
  geom_boxplot(outlier.alpha = 0) +
  scale_color_manual(values = c("red", "blue")) +
  geom_point(color = "black", alpha = .4,
             aes(x = jitter(as.numeric(as.factor(wikipedia_grammar_type2)), .5), 
                 shape = outlier_status), 
             size = 2) +
  theme_minimal() +
  ggtitle("IAT Effect Size and Grammar Type") +
  ylab("Behavioral IAT Effect Size\n (Cohen's IAT)") +
  xlab("Grammar Type") +
  theme(legend.position = "none",
        text = element_text(size = TEXT_SIZE))
```

```{r behavior_grammar_with_hindi_stats}
mf <- filter(iat_means_with_grammar, 
             es_behavioral_iat_weighted, 
             wikipedia_grammar_type2 == "Grammatical Gender")
none <- filter(iat_means_with_grammar,
               es_behavioral_iat_weighted, 
               wikipedia_grammar_type2 == "No Grammatical Gender")

with_hindi_t <- t.test(mf$es_behavioral_iat_weighted, 
                       none$es_behavioral_iat_weighted, 
                       paired = F)
gender_grammar_d <- tes(with_hindi_t$statistic[[1]], 
                        nrow(mf), 
                        nrow(none), 
                        verbose = F)
```

```{r behavior_grammar_no_hindi_stats}
# exclude outlier greater than 2 standard deviations past the mean of each group
upper_bound_mf <- mean(mf$es_behavioral_iat_weighted) + 2 * sd(mf$es_behavioral_iat_weighted)
lower_bound_mf <- mean(mf$es_behavioral_iat_weighted) - 2 * sd(mf$es_behavioral_iat_weighted)

upper_bound_none <- mean(none$es_behavioral_iat_weighted) + 2 * sd(none$es_behavioral_iat_weighted)
lower_bound_none <- mean(none$es_behavioral_iat_weighted) - 2 * sd(none$es_behavioral_iat_weighted)

iat_means_with_grammar_no_hindi <- iat_means_with_grammar %>%
  filter((wikipedia_grammar_type2 == "Grammatical Gender" &
            es_behavioral_iat_weighted > lower_bound_mf & 
            es_behavioral_iat_weighted < upper_bound_mf) |
           (wikipedia_grammar_type2 == "No Grammatical Gender" &
              es_behavioral_iat_weighted > lower_bound_none &
              es_behavioral_iat_weighted < upper_bound_none)) 

mf_no_hindi <-  iat_means_with_grammar_no_hindi %>%
                      filter(es_behavioral_iat_weighted, 
                            wikipedia_grammar_type2 == "Grammatical Gender")
none_no_hindi <- iat_means_with_grammar_no_hindi %>%
                      filter(es_behavioral_iat_weighted, 
                            wikipedia_grammar_type2 == "No Grammatical Gender")

no_hindi_t <- t.test(mf_no_hindi$es_behavioral_iat_weighted, 
       none_no_hindi$es_behavioral_iat_weighted, 
       paired = F)

gender_grammar_d_no_hindi <- tes(no_hindi_t$statistic[[1]], 
                                 nrow(mf_no_hindi), 
                                 nrow(none_no_hindi), 
                                 verbose = F)
```

```{r, language_grammar_stats}
t_language <- t.test(mf$es_hand_translation, 
                     none$es_hand_translation, 
                     paired = F)

d_language_grammar <- tes(t_language$statistic[[1]], 
                        nrow(mf[!is.na(mf$es_hand_translation),]), 
                        nrow(none[!is.na(none$es_hand_translation),]), 
                        verbose = F)
# Hindi and Hungarian not in hand-coded translations
```

```{r, include = F}

iat_means_with_grammar %>%
  ungroup() %>%
  ggplot(aes(x = wikipedia_grammar_type2, 
             y = es_hand_translation,
             color = wikipedia_grammar_type2)) +
  geom_boxplot(outlier.alpha = 0) +
  scale_color_manual(values = c("red", "blue")) +
  geom_point(color = "black", alpha = .4,
             aes(x = jitter(as.numeric(as.factor(wikipedia_grammar_type2)), .5)), 
             size = 2) +
  theme_minimal() +
  ggtitle("IAT Effect Size and Grammar Type") +
  ylab("Linguistic IAT Effect Size\n (Cohen's IAT)") +
  xlab("Grammar Type") +
  theme(legend.position = "none",
        text = element_text(size = TEXT_SIZE))

iat_means_with_grammar %>%
  group_by(wikipedia_grammar_type2) %>%
  multi_boot_standard(col = "es_hand_translation", na.rm = T) %>%
  ggplot(aes(x = wikipedia_grammar_type2,  
             y = mean,
             color = wikipedia_grammar_type2)) +
  scale_color_manual(values = c("red", "blue")) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_minimal() +
  ggtitle("IAT Linguistic Effect Size and Grammar Type") +
  ylab("Linguistic IAT Effect Size\n (Cohen's IAT)") +
  xlab("Grammar Type") +
  theme(legend.position = "none",
        text = element_text(size = TEXT_SIZE))

```

Languages that encode grammatical gender tended to have speakers with greater psychological gender bias (Study 1; _M_ = `r round(mean(mf$es_behavioral_iat_weighted), 2)`; _SD_ =  `r round(sd(mf$es_behavioral_iat_weighted), 2)`) compared to speakers of languages that do not grammatically encode gender (_M_ = `r round(mean(none$es_behavioral_iat_weighted), 2)`; _SD_ =  `r round(sd(none$es_behavioral_iat_weighted), 2)`), though this difference was not reliable (_d_ = `r gender_grammar_d$d` [`r gender_grammar_d$l.d`, `r gender_grammar_d$u.d`], _t_(`r round(with_hindi_t$parameter,2)`) = `r round(with_hindi_t$statistic,2)`; _p_ = `r round(with_hindi_t$p.value, 2)`; Fig. 4). In a post-hoc analysis, we excluded outliers located more than two standard deviations from the group mean (Hungarian and Hindi). With these exclusions, we find a reliable difference between language types (_d_ = `r gender_grammar_d_no_hindi$d` [`r gender_grammar_d_no_hindi$l.d`, `r gender_grammar_d_no_hindi$u.d`]; _t_(`r round(no_hindi_t$parameter,2)`) = `r round(no_hindi_t$statistic,2)`; _p_ < .01). In addition, we find the same pattern for language IAT (Study 2), with languages that encode gender grammatically tending to have larger language IAT gender biases, compared to those who do not (_t_(`r round(t_language$parameter,2)`) = `r round(t_language$statistic[[1]],2)`; _p_ = `r round(t_language$p.value, 2)`). [INCLUDE REGRESSION MODEL?]

## General Discussion
<!--
*not mutually exclusive
*grammar and embedding models show account for the same biases.
*we can do this with other biases
*can examine more directly the words driving the effect (more thatn just the IAT effect)
* Mechanism: implicit grammar (shared caregory with males); explicit grammar (essentialism); general semantics

Gatlet's problem (not random select, and gender biased to certain languages)
-->
# Conclusion 

<!--
 -> why these effects matter
 * "language causal hypothesis"

Chugh, D. (2004). Societal and managerial implications of implicit social cognition: Why milliseconds matter. Social Justice Research, 17, 203– 222. doi:10.1023/B:SORE.0000027410.26010.40
Rudman, L. A. (2004). Social justice in our minds, homes, and society: The nature, causes, and consequences of implicit bias. Social Justice Re- search, 17, 129–142. doi:10.1023/B:SORE.0000027406.32604.f6

correlations are small with explict (oswald 2013), but matter Greenwald (2015; https://faculty.washington.edu/agg/pdf/Greenwald,Banaji&Nosek.JPSP.2015.pdf)

-->

# References 

---
nocite: | 
  @caliskan2017semantics
  @wps
  @whorf1945grammatical

...

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
