---
title: "Language use shapes cultural norms: Large scale evidence from gender"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
header-includes: |
    \newcommand*\rot{\rotatebox{90}}
    \newcommand{\squeezeup}{\vspace{-2mm}}

author-information: > 
    \author{{\large \bf Molly Lewis} \\ \texttt{mollyllewis@gmail.com} \\ Department of Psychology  \\ University of Wisconsin-Madison
    \And {\large \bf Gary Lupyan} \\ \texttt{lupyan@wisc.edu} \\ Department of Psychology  \\ University of Wisconsin-Madison}

abstract: 
    "Cultural norms vary dramatically across social groups.  Here we examine the extent to which language semantics and structure might shape one such norm---the norm to associate men with careers and women with family. We use previously-collected cross-cultural estimates of gender bias from the Implicit Association Task (IAT; N = 663,709) as our dependent measure, and try to predict variability in this bias with variability in the way gender is encoded linguistically.  We find that the linguistic encoding of gender predicts variability in the degree of speakers' gender bias in the IAT, pointing to a causal role for language in shaping gender norms."
    
keywords:
    "cultural norms, IAT, gender, linguistic relativity"
    
output: cogsci2016::cogsci_paper

---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 3, fig.height = 3, 
                      fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(feather)
library(broom)
library(forcats)
library(png)
library(grid)
library(xtable)
library(ggrepel)
library(langcog)
library(compute.es)

TEXT_SIZE <- 10
```

# Introduction
The language we use to communicate a message shapes how our listener interprets that message  (Loftus \& Palmer, 1974; Tversky \& Kahneman, 1981; Fausey  \&  Boroditsky, 2010). A listener, for example, is more likely to infer that a person is at fault if the event is described actively (e.g., "she ignited the napkin"), as opposed to passively (e.g., "the napkin ignited"). The formative power of language is perhaps most potent in shaping meanings that necessarily must be learned from others: cultural norms. In the present paper, we consider one type of cultural norm---gender---and examine the extent to which differences in language use may lead to cross-cultural differences in understandings of gender. 

Gender provides a useful case study of the relationship between
language and thought for several reasons. First, more abstract domains like gender may be more subject to the influence of language relative to more perceptually grounded domains like natural kinds [@boroditsky2001does].  Second, many languages encode the gender of speakers and or addressees explicitly in their grammar. Third, a large body of evidence suggests that language plays a key role in transmitting social knowledge to children [e.g., @master2012thinking]. And, fourth, gender norms are highly variable across cultures and have clear and important social implications.

For our purposes, we define the hypothesis space of possible
relationships between language and gender norms with two broad
extremes:  (1) language reflects a pre-existing gender bias in its speakers (_language-as-reflection hypothesis_); (2) language causally influences gender biases (_language-as-causal hypothesis_).  We assume that the language-as-reflection hypothesis is true to some extent: some of the ways we talk about gender reflect our knowledge and biases acquired independently of language. For example, we may observe that most nurses are women, and therefore be more likely to use a female pronoun to refer
to a nurse of an unknown gender. Our goal here is to understand the extent to which language may also exert a causal influence on conceptualizations of gender.

In particular, we explore two possible mechanisms by which the way we
speak may influence notions of gender. \footnote{These mechanisms are what Whorf (1945) refers to as phenotypes (overt) and cryptotypes (covert).}  The first is through the overt grammatical marking of gender, particularly on nouns, which is obligatory in roughly one quarter of languages [e.g., in Spanish, “ni$\tilde n$a” (girl) and “enfermera” (nurse) both take the gender marker _-a_ to indicate grammatical femininity; @corbett1991]. Because grammatical gender has a natural link to the real world, speakers may assume that grammatical markers are meaningful even when applied to inanimate objects that do not have a biological sex.  In addition, the mere presence of obligatory marking of grammatical gender may  promote bias  by making the  dimension of gender more salient to speakers.

A second route by which language may shape gender norms is via word co-occurrences. Words that tend to occur in similar contexts in language may lead speakers to assume---either implicitly or explicitly---that they have similar meanings. For example, statistically, the word "nurse" occurs in many of the same contexts as
the pronoun "her," providing an implicit link between these two
concepts that may lead to a bias to assume that nurses are female. This
second route may be particularly influential because the bias is encoded
in language in a way that is more implicit than grammatical markers of
gender.

An existing body of experimental work points to a link between language and psychological gender bias in both adults [e.g.,@phillips2003can] and children [e.g., @sera1994grammatical]. For example, Phillips and Boroditsky (2003) asked Spanish-English and German-English adult bilinguals to make similarity judgements between pairs of pictures depicting an object with a natural gender (e.g., a bride) and one without (e.g., a toaster). They found that participants rated pairs as more similar when the pictures matched in grammatical gender in their native language. While these types of studies provide suggestive evidence for a causal link between language and psychological gender bias, they are limited by the fact that they typically only compare speakers of  2-3 different languages and measure bias in a way that is subject to demand characteristics.

In what follows, we ask whether the way gender is encoded linguistically
across 31 different languages predicts cross-cultural variability in a particular manifestation of a gender bias---the bias to associate men with careers and women with family. We begin in Study 1 by describing cross-cultural variability in psychological gender biases using an implicit measure. In Study 2, we use semantic-embedding models to examine whether variability in lexical semantics predicts variability in psychological gender biases. In Study 3, we ask whether the presence of grammatical gender in a language is associated with greater implicit gender bias. Together, our data suggest that both language statistics and language structure likely play a causal role in shaping culturally-specific notions of gender.

# Study 1: Gender bias across cultures
In Study 1, we describe how a widely studied gender bias---the propensity to associate women with family and men with careers---varies across cultures. To quantify this bias, we used data from a large-scale administration of an  Implicit  Association Task [IAT; @greenwald1998measuring] by Project Implicit [@nosek2002harvesting]. The IAT measures the strength of respondents' implicit associations between two pairs of concepts (e.g., male-career/female-family vs. male-family/female-career) accessed via verbal cues (i.e., words like “man”  and  “business”). The underlying assumption of the IAT  is that words (or images) denoting more similar meanings should be easier to pair together, compared to more dissimilar pairs. 

In the task, meanings are paired by assigning them to the same response keys in a 2AFC categorization task. In the critical blocks of the task, meanings are assigned to keys in a way that is either bias-congruent (i.e.\ Key A = male/career; Key B = female/family) or bias-incongruent (i.e.\ Key A = male/family; Key B = female/career). Participants are then presented with a word related to one of the four concepts and asked to classify it as quickly as possible. Slower reaction times in the bias-incongruent blocks relative to the bias-congruent blocks are interpreted as indicating an implicit association between the corresponding concepts (i.e.\ a bias to associate male with career and female with family).
 

## Method

```{r read_behavioral_iat_data}
raw_iat_behavioral <- read_feather("analysis/study1/data/Gender-Career IAT.public.2005-2016.feather") %>%
  select(D_biep.Male_Career_all,sex, countryres, PCT_error_3467, 
         Mn_RT_all_3467, Mn_RT_all_3, Mn_RT_all_4, Mn_RT_all_6, 
         Mn_RT_all_7, assocareer, assofamily, N_ERROR_3, N_ERROR_4,
         N_ERROR_6, N_ERROR_7, N_3, N_4, N_6, N_7) %>%
    rename(overall_iat_D_score = D_biep.Male_Career_all) 

# this file is "Gender-Career/Gender-Career IAT.public.2005-2016.sav" in feather form (taken from: https://osf.io/gmewy/) 
```

```{r get_complete_data}
MIN_PARTICIPANTS_PER_COUNTRY <- 400

raw_iat_behavioral_complete <- raw_iat_behavioral %>%
                                filter(sex %in% c("f", "m"),
                                       !is.na(countryres), 
                                       countryres != ".",
                                       countryres != "nu",  # not clear what this refers to (it's lower case and not in codebook- Niue seems unlikely)
                                       !is.na(overall_iat_D_score))

country_ns <- raw_iat_behavioral_complete %>%
  count(countryres)  %>%
  filter(n >= MIN_PARTICIPANTS_PER_COUNTRY) %>%
  arrange(-n)

raw_iat_behavioral_complete_dense_country <- raw_iat_behavioral_complete %>%
  filter(countryres %in% country_ns$countryres)
```

```{r behavioral_exclusions}
# same exclusions as Nosek, Banjali, & Greenwald (2002), pg. 104. 
iat_behavioral <- raw_iat_behavioral_complete_dense_country %>%
  filter(Mn_RT_all_3467 <= 1500, # RTs
         Mn_RT_all_3 <= 1800,
         Mn_RT_all_4 <= 1800,
         Mn_RT_all_6 <= 1800,
         Mn_RT_all_7 <= 1800) %>%
  filter(N_ERROR_3/N_3 <=.25, # errors
         N_ERROR_4/N_4 <=.25,
         N_ERROR_6/N_6 <=.25,
         N_ERROR_7/N_7 <=.25)
```

```{r tidy_names}
project_implicit_countries <- read_csv("analysis/study1/data/project_implicit_country_codes.csv") 
# these are from Gender-Career_IAT_public_2005-2016_codebook.xlsx; country_name are relabeled from original

iat_behavioral_tidy <- iat_behavioral %>% # join in country names
                          left_join(project_implicit_countries) 

country_ns_final <- iat_behavioral_tidy %>%
                           count(countryres)
```

We analyzed an existing IAT dataset collected online by Project Implicit [https://implicit.harvard.edu/implicit/; @nosek2002harvesting].\footnote{All data and analysis code can be found in an online repository: https://github.com/mllewis/IATLANG} Our analysis included all gender-career IAT scores collected from respondents between 2005 and 2016 who had complete data and  were located in countries with more than `r MIN_PARTICIPANTS_PER_COUNTRY` total respondents (_N_ = `r format(nrow(raw_iat_behavioral_complete_dense_country),big.mark=",")`). We further restricted our sample based on participants' reaction times and errors using the same criteria described in Nosek, Banjai, and Greenwald (2002, pg.\ 104). Our final sample included `r format(nrow(iat_behavioral), big.mark=",")` participants from `r nrow(country_ns_final)` countries, with a median of `r format(round(median(country_ns_final$n)), big.mark = ",")` participants per country. Note that although the respondents were from largely non-English speaking countries, the IAT was conducted in English. We do not have language background data from the participants, but we assume that most respondents from non-English speaking countries were native speakers of the dominant language of the country and L2 speakers of English. 

<!-- exclusion rate = nosek's (15%) --> 
Several measures have been used in the literature to quantify the strength of the bias from participants’ responses on congruent and incongruent blocks. Here, we used the most robust measure, D-score, which measures the difference between critical blocks for each participant while controlling for  individual differences in response time [@greenwald2003understanding]. For each country, we calculated an effect size as the mean D-score divided by its standard deviation (Cohen's _d_); larger values indicate greater bias.

In addition to the implicit measure, we also analyzed an explicit measure of gender bias. After completing the IAT, participants were asked, "How strongly do you associate the following with males and females?" for both the words "career" and "family." Participants indicated their response on a Likert scale ranging from female (1) to male (7).  We calculated an explicit gender bias score for each participant as the Career response minus the Family response, such that greater values indicate a greater bias to associate males with family.

We expected that countries with greater gender equality would have participants with lower implicit and explicit gender biases. As a measure of gender equality, we used
the Women, Peace, and Security Index (WPS, 2017), which measures inclusion, justice, and security of women by country, with larger values indicating higher gender equality. 


## Results

```{r country_gender_bias_map, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.8, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Study 1: IAT gender bias effect size for 48 countries with available data. All countries show a gender bias, with red and blue indicating above and below average bias, respectively."}
# overall effect size
es_behavioral_iat_by_participant <- mean(iat_behavioral_tidy$overall_iat_D_score)/
  sd(iat_behavioral_tidy$overall_iat_D_score)

#mes(mean(iat_behavioral_tidy$overall_iat_D_score), 0, sd(iat_behavioral_tidy$overall_iat_D_score), sd(iat_behavioral_tidy$overall_iat_D_score), 663709, 663709) # tiny CI

# By country effect size
country_means_career_implicit <- iat_behavioral_tidy %>%
  group_by(country_name) %>%
  summarise(es_behavioral_iat = mean(overall_iat_D_score)/sd(overall_iat_D_score))

map_world <- map_data(map = "world") %>%
  mutate(country_name = region) %>%
  mutate(country_name = fct_recode(country_name,
    `United States of America` = "USA", 
    `Russian Federation` = "Russia",
    `Republic of Korea` = "South Korea")) 

map_data <- country_means_career_implicit %>%
  full_join(map_world) %>%
  filter(lat > -57, 
         long > -165) 

SCALE_MIDPOINT <- mean(country_means_career_implicit$es_behavioral_iat, na.rm = T)

# Europe inset (note that Hong Kong is missing from this map)
ggplot(map_data, aes(x = long, y = lat, 
                            group = group, 
                            fill = es_behavioral_iat)) + 
  scale_fill_gradient2("", breaks =  c(.9, 1, 1.1, 1.2),
                       limits=c(SCALE_MIDPOINT - .2, SCALE_MIDPOINT + .2), 
                       midpoint = SCALE_MIDPOINT, 
                       low = "blue", high = "red") +
  geom_polygon(color = "black", size = .1) +
  ggtitle("IAT Gender Bias") +
  theme(text = element_text(size = TEXT_SIZE),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white"),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = c(.1, .35),
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(0.2, "in"),
        legend.text = element_text(size = 8.7))
```

```{r explicit_implicit_all}
# by-subject correlations (overall_iat_D_score)
subj_means_career_explicit <- iat_behavioral_tidy %>%
    mutate(explicit_dif = assocareer - assofamily) 

imp_exp_subjs <- cor.test(subj_means_career_explicit$overall_iat_D_score, 
         subj_means_career_explicit$explicit_dif)

# by country correlations (effect sizes)
country_means_career_explicit_es <- subj_means_career_explicit %>%
  group_by(country_name) %>%
  summarise(exp_mean_diff = mean(explicit_dif, na.rm = T),
            es_behavioral_iat = mean(overall_iat_D_score)/sd(overall_iat_D_score))

imp_exp_country_es <- cor.test(country_means_career_explicit_es$es_behavioral_iat,
                            country_means_career_explicit_es$exp_mean_diff) 
```

```{r participant_gender_bias}
men <- filter(iat_behavioral_tidy, sex == "m")
women <- filter(iat_behavioral_tidy, sex == "f")
t_test_gender <- t.test(women$overall_iat_D_score, men$overall_iat_D_score, paired = F)

n1 <- 470215 #nrow(women)# weirdly this doesn't work as variables
n2 <- 193494 #nrow(men) 
gender_participant_d <- tes(t_test_gender$statistic[[1]], n1, n2, verbose = F)
```

Our analyses confirm two key findings in the literature on the gender-career IAT [@nosek2002harvesting]. First, participants showed an overall bias to
associate men with career and females with family  (_d_ = `r round(es_behavioral_iat_by_participant,2)`). Figure 1a shows the mean effect size for each of the 48 countries in our sample, with participants from all countries showing a gender bias (_M_ = `r round(mean(country_means_career_implicit$es_behavioral_iat),2)`; _SD_ = `r round(sd(country_means_career_implicit$es_behavioral_iat), 2)`). Second, implicit and explicit bias measures were moderately correlated both at the level of individual participants (_r_ = `r round(as.numeric(imp_exp_subjs$estimate), 2)`; _p_ < .00001) and at the level of countries (_r_ = `r round(as.numeric(imp_exp_country_es$estimate), 2)`; _p_ = `r round(as.numeric(imp_exp_country_es$p.value), 2)`).

```{r WPS_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.4, fig.height=2.8, set.cap.width=T, num.cols.cap=1, fig.cap = "Study 1: Implicit gender bias (measured by the IAT) predicted by an independent measure of gender equality, Women, Peace, and Security Index (WPS).  Each point corresponds to a country with notable points labeled."}

wps <- read_csv("analysis/study1/data/WPS_index.csv") %>%
    mutate(country_name = fct_recode(country_name, 
                                   "Republic of Korea" = "Korea, Republic of",
                                   "UK" = "United Kingdom"))

country_means_career_implicit_with_index <- country_means_career_implicit %>%
  left_join(wps) %>%
  mutate(country_name_lab = ifelse(country_name %in% c("Afghanistan", "India", "Netherlands", 
                                                    "Portugal", "China", "United States of America",
                                                    "Sweden", "Hungary", "Philippines", "Colombia", 
                                                    "UK"), country_name, ""),
       country_name_lab = fct_recode(country_name_lab, 
                                   "USA" = "United States of America"))

country_means_career_implicit_index <- cor.test(country_means_career_implicit_with_index$es_behavioral_iat, 
         country_means_career_implicit_with_index$wps_index)

ggplot(country_means_career_implicit_with_index,
       aes(x = wps_index, y = es_behavioral_iat)) +
      geom_point(aes(fill=es_behavioral_iat),pch=21, size = 2) +
      geom_text_repel(aes(label = country_name_lab), size = 2.3) +
      geom_smooth(method = "lm", alpha = .2, color = "black") +
      scale_fill_gradient2("", breaks =  c(.9, 1, 1.1, 1.2),
                       limits = c(SCALE_MIDPOINT - .2, SCALE_MIDPOINT + .2), 
                       midpoint = SCALE_MIDPOINT, 
                       low = "blue", high = "red") +
      ggtitle("IAT Gender Bias and Gender Equality") +
      ylab("IAT Gender Bias (effect size)") +
      xlab("Women, Peace, and Security Equality Index") +
      #geom_label(aes(label = country_name), size = .9)
      annotate("text", x = .8, y = .87, 
           label = paste0("r = ", round(unlist(country_means_career_implicit_index$estimate, 
                                               use.names = F),2)),
           color = "red",
           size = 5) +
      theme_minimal() +
      theme(text = element_text(size = TEXT_SIZE), 
            legend.position = "none")

country_means_career_explicit_with_index <- country_means_career_explicit_es %>%
  left_join(wps) 

country_means_career_explicit_index <- cor.test(country_means_career_explicit_with_index$exp_mean_diff,
                                                country_means_career_explicit_with_index$wps_index)
```

Our independent measure of gender equality---the Women, Peace, and Security Index---was uncorrelated with explicit bias (_r_ = `r unlist(round(country_means_career_explicit_index$estimate, 2), use.names = F)`; _p_ = `r unlist(round(country_means_career_explicit_index$p.value, 2), use.names = F)`). Counter to our expectations, we found that countries such as the Netherlands, with allegedly greatest gender equality, have participants who show highest implicit gender bias according to the IAT (_r_ = `r unlist(round(country_means_career_implicit_index$estimate, 2), use.names = F)`; _p_ <.01; Fig.\ 1b).

```{r prop_female_confound}
prop_women <- iat_behavioral_tidy %>% 
  group_by(country_name) %>%
  summarize(prop_fem = length(which(sex == "f"))/n()) 

country_means_career_implicit_with_women <- country_means_career_implicit_with_index %>%
  left_join(prop_women)

confound_model_1 <- lm(es_behavioral_iat ~ wps_index + prop_fem, country_means_career_implicit_with_women)  %>%
  tidy()

#  WPS ($\beta$ = `r round(confound_model_1$estimate[confound_model_1$term == "wps_index"],2)`; _t_ = `r round(confound_model_1$statistic[confound_model_1$term == "wps_index"],2)`; _p_ < .01) 
```

We explored this surprising finding by testing two possible confounds with WPS. First, previous research has shown that women tend to have a larger implicit gender bias than men [@nosek2002harvesting]. If countries with more gender parity have more female participants, this would explain observed pattern. The data do not support this: In an additive linear model predicting IAT effect size with both WPS and the proportion female participants, proportion female participants was not a reliable predictor of IAT effect size ($\beta$ = `r round(confound_model_1$estimate[confound_model_1$term == "prop_fem"],2)`; _t_ =  `r round(confound_model_1$statistic[confound_model_1$term == "prop_fem"],2)`; _p_ = `r round(confound_model_1$p.value[confound_model_1$term == "prop_fem"],2)`).

```{r proficiency_confound}
epi <- read_csv("analysis/study1/data/epi.csv") %>%
  rename(ep_index = `2017 Score`) %>% 
  mutate(country_name = str_sub(country_name, 2, -1)) %>%
  select(country_name, ep_index)

country_means_career_implicit_with_index_and_epi <- country_means_career_implicit_with_index %>%
  left_join(epi) %>%
  mutate(ep_index = ifelse(country_name %in% c("UK",
                                               "United States of America",
                                               "Australia", 
                                               "Canada",
                                               "New Zealand",
                                               "Ireland",
                                               "Israel"), 100, ep_index)) 

confound_model_2 <- lm(es_behavioral_iat ~ wps_index + ep_index, country_means_career_implicit_with_index_and_epi)  %>%
  tidy()

# round(confound_model_2$estimate[confound_model_2$term == "ep_index"],2)
```

A second possibility is that participants in countries with greater gender parity have higher English proficiency and are thus relatively more influenced by the meaning of target words.  We tested this possibility by including a measure of English proficiency as a covariate with WPS (EF English Proficiency Index, 2017). We find that English proficiency does not predict IAT effect size ($\beta$ < .001; _t_ =  `r round(confound_model_2$statistic[confound_model_2$term == "ep_index"],2)`; _p_ = `r round(confound_model_2$p.value[confound_model_2$term == "ep_index"],2)`), inconsistent with this explanation.

## Discussion
In Study 1, we replicate previously reported patterns of gender bias in the gender-career IAT literature, with roughly comparable effect sizes (c.f.\ Nosek, et al., 2002: overall effect: _d_ = .72; explicit-implicit correlation: _r_ = .17; participant gender effect: _d_  =  .1). The weak correlation between explicit and implicit measures is consistent with claims that these two measures tap into different cognitive constructs [@forscher2016meta].

The novel finding from Study 1 is the direction of the correlation between the objective gender bias of a country (as measured by the WPS) and implicit gender bias---participants in countries with greater gender equality have _greater_ implicit gender bias, even after controlling for possible confounds. In the General Discussion, we speculate about possible reasons for this positive correlation.  

# Study 2: Gender bias and semantics
In Study 2, we ask whether participants' implicit and explicit gender biases are correlated with the biases in the semantic structure of their native languages. For example, are the semantics of the words “woman” and “family” more similar in Hungarian than in English? Both the language-as-reflection and language-as-causal hypotheses predict  a  positive correlation between psychological and semantic gender biases. Importantly, we expect psychological and semantic gender biases to be correlated regardless of the direction of the relationship between psychological and objective gender bias (WPS) found in Study 1. 

As a model of word meanings, we use large-scale distributional semantics models derived from auto-encoding neural networks trained on large corpora of text. The underlying assumption of these models is that the meaning of a word can be described by the words it tends to co-occur with---an approach known as distributional semantics 
[@firth1957synopsis]. Under this approach, a word like "dog" is represented as more similar to "hound" than to "banana" because "dog" co-occurs with words more in common
with "hound" than "banana" where co-occurrences are defined at multiple hierarchical levels, i.e., two words can come to be represented as similar because their contexts contain words that have similar contexts etc.

Recent developments in machine learning allow the idea of distributional
semantics to be implemented in a way that takes into account many
features of local language structure while remaining computationally
tractable. The best known of these word embedding models is
_word2vec_ [@mikolov2013efficient]. The model takes as input a corpus of text and outputs a vector for each word corresponding to its semantics. From these vectors, we can derive a measure of the semantic similarity between two words by taking the distance between their vectors (e.g., cosine distance). Similarity measures estimated from these models have been shown to be highly correlated with human judgements of word similarity [e.g., @hill2015simlex], though more for some forms of similarity than others [@chen2017evaluating].

As it turns out, the biases previously reported using IAT tests can be
predicted from distributional semantics models like word2vec using materials identical to those used in the IAT experiments. Caliskan, Bryson, and Narayanan (2017; henceforth _CBN_) measured the distance in vector space between the words 
presented to participants in the IAT task. CBN found that these distance
measures were highly correlated with reaction times in the behavioral IAT
task. For example,  CBN find a bias to associate males with career and females with family, in the career-gender IAT, suggesting that the biases measured by the IAT are also found in the lexical semantics of natural language.

In Study 2, we use the method described by CBN to measure the biases in the semantics of the natural languages spoken in the countries of participants in Study 1. While CBN only analyzed biases for models trained on English, we extend their method to compare biases across a wide number of languages. To do this, we take advantage of a set of models that have been pre-trained on a corpus of Wikipedia text in different languages [@bojanowski2016enriching]. In Study 2a, we replicate the CBN findings with the Wikipedia corpus; In Study 2b, we show that the implicit gender biases reported in Study 1 for individual countries are correlated with the biases found in the semantics of the natural language spoken by those participants.

## Study 2a: Replication of Caliskan, et al.\ (2017)
### Method
We use a word embedding model that has been pre-trained model on the corpus of English Wikipedia using the fastText algorithm [@bojanowski2016enriching]\footnote{Available here: https://github.com/facebookresearch/fastText/}. The model contains 2,519,370 words with each word represented by a 300 dimensional vector. 

Using the Wikipedia-trained model, we calculate an effect size for each of the 10 biases reported in CBN which correspond to behavioral IAT results existing in the literature:   flowers/insects--pleasant/unpleasant, instruments/weapons--pleasant/unpleasant, European-American/Afro-American--pleasant/unpleasant,\footnote{CBN test three versions of this bias.}  males/females--career/family, math/arts--male/female, science/arts--male/female, mental-disease/physical-disease--permanent/temporary, and young/old--pleasant/unpleasant (labeled as Word-Embedding Association Test (WEAT) 1-10 in CBN). We calculate the bias using the same effect size metric described in CBN, a  standardized difference score of the relative similarity of the target words to the target attributes (i.e.\ relative similarity of male to career vs.\ relative similarity of female to career). This measure is  analogous to the behavioral effect size measure in Study 1 where larger values indicate larger gender bias.

### Results
```{r WEAT_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.6, fig.height=3.6, set.cap.width=T, num.cols.cap=1, fig.cap = "Study 2a: Effect sizes for the 10 IAT biases types (WEAT 1-10) reported in Caliskan et al.\ (2017; CBN). CBN effect sizes  are plotted against effect sizes derived from the Wikipedia corpus.  Point color corresponds to  bias type, and point shape corresponds to the two CBN models trained on different corpora and with different algorithms."}

wiki_es <- read_csv("analysis/study2a/data/caliskan_wiki_es.csv",  
                    col_names = c("test", "study_name",
                                  "wiki", "study_name2",
                                  "WEAT_name"))
# from caliskan_2017_replication.R

caliskan_es  <- read_csv("analysis/study2a/data/caliskan_es.csv")

all_es <- wiki_es %>%
  left_join(caliskan_es) %>%
  gather("es_source", "d", 6:8)  %>%
  filter(es_source != "original") %>%
  mutate(study_name2 = fct_relevel(study_name2, 
                                   "flowers-insects",
                                   "instruments-weapons",
                                   "race", 
                                   "gender-career",
                                   "gender-math", 
                                   "gender-science",
                                   "mental-physical",
                                   "age"),
         point_color = study_name2,
         study_name2 = paste0(as.character(study_name2),
                              "\n", WEAT_name),
         study_name2 = ifelse(es_source == "AC_w2v" & 
                                test %in% c("WEAT_1", 
                                            "WEAT_2",
                                            "WEAT_3",
                                            "WEAT_7", 
                                            "WEAT_8", 
                                            "WEAT_9", 
                                            "WEAT_10", 
                                            "WEAT_6"),
                              study_name2, ""), 
         es_source = fct_recode(es_source,
                                `Common Crawl (GloVe)` = "AC_glove",
                                `Google News (word2vec)` = "AC_w2v"))
ggplot(all_es, aes(x = wiki, 
                   y = d, 
                   shape = es_source, 
                   color = point_color)) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = "darkgrey") +
  geom_point(size = 2.5) +
  ylim(-.6, 2.2) +
  xlim(-.6, 2.2) +
  ggtitle("Language-Embedding IAT biases") +
  xlab("Effect size (Wikipedia corpus)") +
  ylab("Effect size (Caliskan, et al., 2017)") +
  geom_text_repel(aes(label = study_name2), 
                             force = 5, 
                             color = "black", 
                             size = 2.3, 
                             fontface = 'bold', 
                             point.padding = 1) +
  theme_minimal() +
  scale_shape_discrete(name = "Caliskan et al. model") +
  scale_color_discrete(guide = FALSE) +
  theme(text = element_text(size = TEXT_SIZE),
        legend.position = c(.75, 0.15),
        legend.background = element_rect(fill = "white"),
        legend.text = element_text(size = TEXT_SIZE - 3),
        legend.title = element_text(size = TEXT_SIZE - 2),
        legend.margin = margin(3, 3, 0, 3, unit='pt'))
```

```{r english_language_career_es}
gender_bias_es_wiki <- filter(all_es, study_name == "gender-bias-career-family")$wiki[1]

gender_bias_es_cc <- filter(all_es, study_name == "gender-bias-career-family" & es_source == "Common Crawl (GloVe)" )$d[1]

gender_bias_es_gn <- filter(all_es, study_name == "gender-bias-career-family" & es_source == "Google News (word2vec)")$d[1]
```

Figure 2 shows the effect size measures derived from the Wikipedia corpus plotted against effect size estimates reported by CBN from two different models (trained on the Common Crawl and  Google News corpora). With the exception of biases related to race and age, effect sizes from the Wikipedia corpus are comparable to those reported by CBN. In particular, for the gender-career IAT---the bias relevant to our current purposes---we estimate the effect size to be  `r round(gender_bias_es_wiki,2)`, while CBN estimates it as approximately  `r round(mean(c(gender_bias_es_cc,gender_bias_es_gn)),2)`.

## Study 2b: Cross-linguistic gender semantics
With our corpus validated, we next turn toward examining the relationship between psychological and linguistic gender biases. In Study 2b, we estimate the magnitude of the gender-career bias in each of the languages spoken in the countries described in Study 1 and compare it with estimates of behavioral gender bias from Study 1. We predict these two measures should be positively correlated.

### Method

```{r read_language_bias_data}
# behavioral means by langauge
countries_to_langs <- read_csv("analysis/study2b/data/languages_with_percent.csv") %>% # this comes from get_language_percentage.R
  mutate(country_name = fct_recode(country_name,
                               "United States of America"= "United States", 
                                UK = "United Kingdom",
                                "Russian Federation" = "Russia",
                                "Republic of Korea" = "South Korea"),
         country_code = fct_recode(country_code, UK = "GB"), 
         wiki_language_code = fct_recode(wiki_language_code,
                               "zh"= "zh_yue")) # Cantonese isn't in gtranslate

# get one language per country
unique_langs_per_country <- countries_to_langs %>%
  group_by(country_name) %>%
  arrange(-prop_language)  %>%
  slice(1) %>%
  filter(!language_name %in% c("Niuean", "Cantonese"))

max_prop_es_translations <- unique_langs_per_country %>%
  select(country_name, country_code, language_name, wiki_language_code) 

# average across countries weighting by number of participatns
implicit_behavioral_means_by_country_with_language <- country_means_career_implicit %>%
  left_join(max_prop_es_translations) %>%
  left_join(country_ns_final, by = c("country_code"= "countryres")) %>% 
  group_by(language_name) %>%
  mutate(normalized_n = n/sum(n)) %>%
  select(wiki_language_code, language_name, country_name, 
         es_behavioral_iat, normalized_n) 

# get language means
language_means_career_implicit_hand <- read.csv("analysis/study2b/data/career_effect_sizes_hand_translations.csv", 
col.names = c("wiki_language_code", "test_id", "test_name", "es_hand_translation"), 
header = F,
                                                fill = TRUE)  %>%
  select(-test_id, -test_name)

all_hand_es <- implicit_behavioral_means_by_country_with_language %>%
  group_by(wiki_language_code, language_name) %>%
  summarise(es_behavioral_iat_weighted = weighted.mean(es_behavioral_iat, 
                                                       normalized_n, na.rm = T),
            es_behavioral_iat = mean(es_behavioral_iat)) %>%
  left_join(language_means_career_implicit_hand, by = "wiki_language_code")  %>%
  filter(!is.na(es_behavioral_iat_weighted))  %>%
  ungroup()
```


```{r behavior_vs_language_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.3, fig.height=3.1, set.cap.width=T, num.cols.cap=1, fig.cap = "Study 2b: Behavioral IAT gender bias (Study 1) by language versus language-embedding IAT gender bias, estimated from models trained on each language."}
behavior_lang_cor_imp <- cor.test(all_hand_es$es_hand_translation, all_hand_es$es_behavioral_iat_weighted)

ggplot(all_hand_es, aes(x = es_hand_translation, y = es_behavioral_iat_weighted)) +
  geom_smooth(method = "lm", alpha = .2, color = "black") +
  geom_text_repel(aes(label = language_name), size = 2.3) + 
  geom_point() +
  annotate("text", x = .75, y = .9, 
           label = paste0("r = ", 
                          round(unlist(behavior_lang_cor_imp$estimate, use.names = F),2)),
           color = "red",
           size = 5) +
  theme_minimal() +
  ggtitle("Behavioral and \nLanguage-Embedding IAT") +
  ylab("Behavioral IAT Gender Bias\n (effect size)") +
  xlab("Language-Embedding IAT Gender Bias\n (effect size)") +
  theme(text = element_text(size = TEXT_SIZE))
```

For each country included in Study 1, we identified the most frequently spoken language in each country using the CIA factbook (2017). This included a total of `r length(unique(unique_langs_per_country$language_name))` unique languages. For a sample of 20 of these languages (see Fig.\ 3), we had native speakers translate the set of 32 words from the gender-career IAT with a slight modification.\footnote{The language sample was determined by accessibility to native speakers, but included languages from a variety of language families.} The original gender-career IAT task  [@nosek2002harvesting] used proper names to cue the male and female categories (e.g.\ "John," "Amy"). Because there are not direct translation equivalents of proper names, we instead used a set of generic gendered words which had been previously used for a different version of the gender IAT [e.g., "man," "woman;" @nosek2002harvesting].

We used these translations to calculate an effect size from the models trained on Wikipedia in each language, using the same method as in Study 2a. We then compared the effect size of the linguistic gender bias to  the  behavioral IAT gender bias from Study 1, averaging across countries that speak the same language and weighting by sample size.  

### Results
```{r explicit_corrs_with_language}
country_means_career_explicit_es_with_weights <- country_means_career_explicit_es %>%
  select(-es_behavioral_iat) %>%
  left_join(max_prop_es_translations, by = "country_name") %>%
  left_join(country_ns_final, by = c("country_code"= "countryres")) %>%
  group_by(language_name) %>%
  mutate(normalized_n = n/sum(n))

language_means_career_explicit_language <- country_means_career_explicit_es_with_weights %>%
  group_by(wiki_language_code, language_name) %>%
  summarise(exp_mean_diff_weighted = 
              weighted.mean(exp_mean_diff, normalized_n, na.rm = T)) %>%
  left_join(language_means_career_implicit_hand, by = "wiki_language_code")

behavior_lang_cor_exp <- cor.test(language_means_career_explicit_language$es_hand_translation, language_means_career_explicit_language$exp_mean_diff_weighted)
```

Implicit IAT gender bias effect sizes were positively correlated with effect sizes of gender bias estimated from the native language embedding model (_r_ = `r unlist(round(behavior_lang_cor_imp$estimate, 2), use.names = F)`; _p_ = `r unlist(round(behavior_lang_cor_imp$p.value, 2), use.names = F)`; Fig.\ 3), suggesting that countries that have more gender bias encoded in their language have speakers with greater implicit bias. Explicit gender bias was not reliably correlated with language gender bias (_r_ = `r unlist(round(behavior_lang_cor_exp$estimate, 2), use.names = F)`; _p_ = `r unlist(round(behavior_lang_cor_exp$p.value, 2), use.names = F)`). 

## Discussion
Consistent with our prediction, we find that the magnitude of a country's implict gender bias is correlated with the degree of bias in the semantics of participants' native language.

# Study 3: Gender bias and grammar
The findings in Study 2 are consistent with both the language-as-causal and
language-as-reflection hypotheses. In Study 3, we try to distinguish between the two hypotheses by examining whether there is a relationship between implicit and explicit psychological gender bias and language along a linguistic dimension that is unlikely to be a subject of rapid change---namely, grammatical gender. While of course grammars do change over time, they are
less malleable than the meanings of individual words, and thus less likely to be
affected by psychological biases. We predict, therefore, that if
language causally influences psychological gender biases, languages that
encode gender grammatically will tend to have larger psychological
gender biases.


## Method
```{r read_gender_data}
gender_data <- read_csv("analysis/study3/data/gender_grammar.csv") %>%
  select(language_code, language_name, wikipedia_grammar_type) %>%
  mutate(wikipedia_grammar_type2 = ifelse(wikipedia_grammar_type == "none",
                                          "No Gender", 
                                          "Gender"))

# and `r nrow(filter(gender_data, wikipedia_grammar_type2 == "No Grammatical Gender"))` languages without grammatical gender.
```

We coded each of the `r length(unique(unique_langs_per_country$language_name))` languages in our sample (Study 1) for grammatical gender. We used a coarse binary coding scheme, categorizing a language as encoding
grammatical gender if it made any gender distinction on noun classes
(masculine, feminine, common or neuter), and as not encoding gender
grammatically otherwise. We coded this distinction on the basis of the
WALS typological database [Feature 32a; @wals] where
available, and consulted additional resources as necessary. Our sample
included `r nrow(filter(gender_data, wikipedia_grammar_type2 == "Gender"))`  languages with grammatical gender and `r nrow(filter(gender_data, wikipedia_grammar_type2 == "No Gender"))` without.

## Results

```{r grammatical_gender_plot, fig.env="figure", fig.pos = "t", fig.align='center', fig.width=3.5, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "Study 3: Behavioral IAT gender bias (Study 1) as a function of whether participants' native language encodes gender grammatically. Each point corresponds to a language with outliers shown as triangles (jittered for visibility)."}

## es behavior
iat_means_with_grammar <- all_hand_es %>%
  full_join(gender_data, by = c("wiki_language_code" = "language_code")) %>%
  select(-contains("test")) 

iat_means_with_grammar %>%
  ungroup() %>%
  mutate(outlier_status = ifelse(language_name.x %in% c("Hungarian", "Hindi"),
                                 "o", "i"),
         wikipedia_grammar_type2 = fct_rev(wikipedia_grammar_type2)) %>%
  ggplot(aes(x = wikipedia_grammar_type2, 
             y = es_behavioral_iat_weighted,
             color = wikipedia_grammar_type2)) +
  geom_boxplot(outlier.alpha = 0) +
  scale_color_manual(values = c("blue", "red")) +
  geom_point(color = "black", alpha = .4,
             aes(x = jitter(as.numeric(as.factor(wikipedia_grammar_type2)), .5), 
                 shape = outlier_status), 
             size = 2) +
  coord_flip() +
  theme_minimal() +
  ggtitle("Behavioral IAT and Grammar Type") +
  ylab("Behavioral IAT Gender Bias\n (effect size)") +
  xlab("Grammar Type") +
  theme(legend.position = "none",
        text = element_text(size = TEXT_SIZE),
        axis.text.y = element_text(angle = 90, hjust=0.5, size = 7))
```

```{r behavior_grammar_with_hindi_stats}
mf <- filter(iat_means_with_grammar, 
             es_behavioral_iat_weighted, 
             wikipedia_grammar_type2 == "Gender")
none <- filter(iat_means_with_grammar,
               es_behavioral_iat_weighted, 
               wikipedia_grammar_type2 == "No Gender")

with_hindi_t <- t.test(mf$es_behavioral_iat_weighted, 
                       none$es_behavioral_iat_weighted, 
                       paired = F)
gender_grammar_d <- tes(with_hindi_t$statistic[[1]], 
                        nrow(mf), 
                        nrow(none), 
                        verbose = F)
```

```{r behavior_grammar_no_hindi_stats}
# exclude outlier greater than 2 standard deviations past the mean of each group
upper_bound_mf <- mean(mf$es_behavioral_iat_weighted) + 2 * sd(mf$es_behavioral_iat_weighted)
lower_bound_mf <- mean(mf$es_behavioral_iat_weighted) - 2 * sd(mf$es_behavioral_iat_weighted)

upper_bound_none <- mean(none$es_behavioral_iat_weighted) + 2 * sd(none$es_behavioral_iat_weighted)
lower_bound_none <- mean(none$es_behavioral_iat_weighted) - 2 * sd(none$es_behavioral_iat_weighted)

iat_means_with_grammar_no_hindi <- iat_means_with_grammar %>%
  filter((wikipedia_grammar_type2 == "Gender" &
            es_behavioral_iat_weighted > lower_bound_mf & 
            es_behavioral_iat_weighted < upper_bound_mf) |
           (wikipedia_grammar_type2 == "No Gender" &
              es_behavioral_iat_weighted > lower_bound_none &
              es_behavioral_iat_weighted < upper_bound_none)) 

mf_no_hindi <-  iat_means_with_grammar_no_hindi %>%
                      filter(es_behavioral_iat_weighted, 
                            wikipedia_grammar_type2 == "Gender")
none_no_hindi <- iat_means_with_grammar_no_hindi %>%
                      filter(es_behavioral_iat_weighted, 
                            wikipedia_grammar_type2 == "No Gender")

no_hindi_t <- t.test(mf_no_hindi$es_behavioral_iat_weighted, 
       none_no_hindi$es_behavioral_iat_weighted, 
       paired = F)

gender_grammar_d_no_hindi <- tes(no_hindi_t$statistic[[1]], 
                                 nrow(mf_no_hindi), 
                                 nrow(none_no_hindi), 
                                 verbose = F)
```

```{r language_grammar_stats}
t_language <- t.test(mf$es_hand_translation, 
                     none$es_hand_translation, 
                     paired = F)

d_language_grammar <- tes(t_language$statistic[[1]], 
                        nrow(mf[!is.na(mf$es_hand_translation),]), 
                        nrow(none[!is.na(none$es_hand_translation),]), 
                        verbose = F)
# Hindi and Hungarian not in hand-coded translations
```

Languages that encode grammatical gender tended to have speakers with greater psychological gender bias (Study 1; _M_ = `r round(mean(mf$es_behavioral_iat_weighted), 2)`; _SD_ =  `r round(sd(mf$es_behavioral_iat_weighted), 2)`) compared to speakers of languages that do not grammatically encode gender (_M_ = `r round(mean(none$es_behavioral_iat_weighted), 2)`; _SD_ =  `r round(sd(none$es_behavioral_iat_weighted), 2)`), though this difference was not reliable (_d_ = `r gender_grammar_d$d` [`r gender_grammar_d$l.d`, `r gender_grammar_d$u.d`], _t_(`r round(with_hindi_t$parameter,2)`) = `r round(with_hindi_t$statistic,2)`; _p_ = `r round(with_hindi_t$p.value, 2)`; Fig. 4). In a post-hoc analysis, we excluded outliers located more than two standard deviations from the group mean (Hungarian and Hindi). With these exclusions, we find a reliable difference between language types (_d_ = `r gender_grammar_d_no_hindi$d` [`r gender_grammar_d_no_hindi$l.d`, `r gender_grammar_d_no_hindi$u.d`]; _t_(`r round(no_hindi_t$parameter,2)`) = `r round(no_hindi_t$statistic,2)`; _p_ < .01). In addition, we find the same pattern for language IAT (Study 2), with languages that encode gender grammatically tending to have larger language IAT gender biases, compared to those that do not (_t_(`r round(t_language$parameter,2)`) = `r round(t_language$statistic[[1]],2)`; _p_ = `r round(t_language$p.value, 2)`). 

## Discussion
Consistent with the language-as-causal account, Study 3 provides evidence that the presence of grammatical gender in a language predicts implicit gender bias. 

# General Discussion and Conclusion
Across three studies, we explore the relationship between a culturally-constructed norm---gender---and the the linguistic encoding of that norm. We find evidence for a close correspondence: Languages that have larger gender biases encoded in their lexical semantics (Study 2) and have grammatical gender markers (Study 3) tend to have speakers with larger implicit gender bias. Study 2 is consistent with both the language-as-reflection and the language-as-causal hypotheses, but Study 3 is most consistent with the language-as-causal hypothesis. Taken together, the most likely interpretation of our data is that both mechanisms are at play and act synergistically, such that the way we talk about gender shapes the way we think about it and the way we think about gender shapes the way we talk about it. 

In addition, our work is the first to report the surprising positive correlation between implicit gender bias and objective gender equality. The source of this correlation  is difficult to interpret in part because researchers do not agree on the nature of the construct that the IAT measures or its causal relationship to explicit bias and behavior [@forscher2016meta]. One provocative implication, however, that there is a causal relationship between individual and institutional sexism. We speculate that greater gender equality could emerge as a consequence of _increased_ attention to gender inequality, leading to both objective equality but also increased implicit bias at the individual level. An important next step for understanding this relationship will be to examine whether this pattern holds for other types of biases, such as race.

More generally, the task for future work will be to specify the dynamics of the causal mechanisms between language and cultural norms with more precision. The ultimate goal is to describe the relative influence of different aspects of language---statistics and structure---on cultural norms and vice versa, particularly when children are first acquiring cultural knowledge in development. The data here provide an early step toward this goal. 


```{r save_complete_df, eval = F}
all_measures_df <- country_means_career_implicit_with_index %>% # implicit
  select(-country_name_lab, -es_behavioral_iat) %>%
  left_join(country_means_career_explicit_es, by = "country_name") %>%  # explicit 
  left_join(implicit_behavioral_means_by_country_with_language %>% #implict n for weights
              select(country_name, wiki_language_code, normalized_n)) %>% 
  left_join(language_means_career_implicit_hand) %>% # iat language
  select(country_name, language_name, exp_mean_diff, 
         es_behavioral_iat, wps_index, normalized_n,  es_hand_translation) %>%
  left_join(prop_women)
# write_csv(all_measures_df, "analysis/all_measures_df.csv")
```

# References 

---
nocite: | 
  @caliskan2017semantics
  @wps
  @epi
  @whorf1945grammatical
  @loftus1974reconstruction
  @tversky1981framing
  @fausey2010subtle
  @ciafactbook

...

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
