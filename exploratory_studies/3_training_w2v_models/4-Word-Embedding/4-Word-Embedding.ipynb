{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings\n",
    "\n",
    "This week, we build on last week's topic modeling techniques by taking a text corpus we have developed, specifying an underlying number of dimensions, and training a model with a neural network auto-encoder (one of Google's word2vec  algorithms) that best describes corpus words in their local linguistic contexts, and exploring their locations in the resulting space to learn about the discursive culture that produced them. Documents here are represented as densely indexed locations in dimensions, rather than sparse mixtures of topics (as in LDA topic modeling), so that distances between those documents (and words) are consistently superior, though they require the full vector of dimension loadings (rather than just a few selected topic loadings) to describe. We will explore these spaces to understand complex, semantic relationships between words, index documents with descriptive words, identify the likelihood that a given document would have been produced by a given vector model, and explore how semantic categories can help us understand the cultures that produced them.\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our corpora\n",
    "\n",
    "Instead of downloading our corpora, we have download them in advance; a subset of the [senate press releases](https://github.com/lintool/GrimmerSenatePressReleases) are in `grimmerPressReleases`. We will load them into a DataFrame, but first we need to define a function to convert directories of text files into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDir(targetDir, category):\n",
    "    allFileNames = os.listdir(targetDir)\n",
    "    #We need to make them into useable paths and filter out hidden files\n",
    "    filePaths = [os.path.join(targetDir, fname) for fname in allFileNames if fname[0] != '.']\n",
    "\n",
    "    #The dict that will become the DataFrame\n",
    "    senDict = {\n",
    "        'category' : [category] * len(filePaths),\n",
    "        'filePath' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "\n",
    "    for fPath in filePaths:\n",
    "        with open(fPath) as f:\n",
    "            senDict['text'].append(f.read())\n",
    "            senDict['filePath'].append(fPath)\n",
    "\n",
    "    return pandas.DataFrame(senDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function in all the directories in `data/grimmerPressReleases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filePath</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Apr2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Dec2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Feb2006Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE      Fact sheet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Feb2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Jun2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  BOSTON  MA  Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Mar2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01May2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  The President ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Nov2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  Washington  DC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/02Aug2006Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/02Feb2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     The Preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           filePath  \\\n",
       "0   Kennedy  data/grimmerPressReleases/Kennedy/01Apr2005Ken...   \n",
       "10  Kennedy  data/grimmerPressReleases/Kennedy/01Dec2005Ken...   \n",
       "20  Kennedy  data/grimmerPressReleases/Kennedy/01Feb2006Ken...   \n",
       "30  Kennedy  data/grimmerPressReleases/Kennedy/01Feb2007Ken...   \n",
       "40  Kennedy  data/grimmerPressReleases/Kennedy/01Jun2007Ken...   \n",
       "50  Kennedy  data/grimmerPressReleases/Kennedy/01Mar2007Ken...   \n",
       "60  Kennedy  data/grimmerPressReleases/Kennedy/01May2007Ken...   \n",
       "70  Kennedy  data/grimmerPressReleases/Kennedy/01Nov2007Ken...   \n",
       "80  Kennedy  data/grimmerPressReleases/Kennedy/02Aug2006Ken...   \n",
       "90  Kennedy  data/grimmerPressReleases/Kennedy/02Feb2005Ken...   \n",
       "\n",
       "                                                 text  \n",
       "0            FOR IMMEDIATE RELEASE   FOR IMMEDIATE...  \n",
       "10           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "20           FOR IMMEDIATE RELEASE      Fact sheet...  \n",
       "30           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "40           FOR IMMEDIATE RELEASE  BOSTON  MA  Se...  \n",
       "50           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "60           FOR IMMEDIATE RELEASE  The President ...  \n",
       "70           FOR IMMEDIATE RELEASE  Washington  DC...  \n",
       "80           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...  \n",
       "90           FOR IMMEDIATE RELEASE     The Preside...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = 'data/grimmerPressReleases'\n",
    "\n",
    "senReleasesDF = pandas.DataFrame()\n",
    "\n",
    "for senatorName in [d for d in os.listdir(dataDir) if d[0] != '.']:\n",
    "    senPath = os.path.join(dataDir, senatorName)\n",
    "    senReleasesDF = senReleasesDF.append(loadDir(senPath, senatorName), ignore_index = True)\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove stop words and stem. Tokenizing requires two steps. Word2Vec needs to retain the sentence structure so as to capture a \"continuous bag of words (CBOW)\" and all of the skip-grams within a word window. The algorithm tries to preserve the distances induced by one of these two local structures. This is very different from clustering and LDA topic modeling which extract unordered words alone. As such, tokenizing is slightly more involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None, vocab = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "\n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "\n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "        \n",
    "    #We will return a list with the stopwords removed\n",
    "    if vocab is not None:\n",
    "        vocab_str = '|'.join(vocab)\n",
    "        workingIter = (w for w in workingIter if re.match(vocab_str, w))\n",
    "    \n",
    "    return list(workingIter)\n",
    "\n",
    "#initialize our stemmer and our stop words\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "wordnet = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filePath</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Apr2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Dec2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, Washington, D, C, T...</td>\n",
       "      <td>[[immediate, release, washington, c, today, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Feb2006Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE      Fact sheet...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, Fact, sheets, on, B...</td>\n",
       "      <td>[[immediate, release, fact, sheets, bush, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Feb2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, Washington, D, C, T...</td>\n",
       "      <td>[[immediate, release, washington, c, today, u,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Jun2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  BOSTON  MA  Se...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, BOSTON, MA, Senator...</td>\n",
       "      <td>[[immediate, release, boston, senator, edward,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Mar2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, Washington, DC, Tod...</td>\n",
       "      <td>[[immediate, release, washington, dc, today, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01May2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  The President ...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, The, President, is,...</td>\n",
       "      <td>[[immediate, release, president, wrong, veto, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/01Nov2007Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  Washington  DC...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, Washington, DC, Sen...</td>\n",
       "      <td>[[immediate, release, washington, dc, senators...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/02Aug2006Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>data/grimmerPressReleases/Kennedy/02Feb2005Ken...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     The Preside...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, The, President, gav...</td>\n",
       "      <td>[[immediate, release, president, gave, effecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           filePath  \\\n",
       "0   Kennedy  data/grimmerPressReleases/Kennedy/01Apr2005Ken...   \n",
       "10  Kennedy  data/grimmerPressReleases/Kennedy/01Dec2005Ken...   \n",
       "20  Kennedy  data/grimmerPressReleases/Kennedy/01Feb2006Ken...   \n",
       "30  Kennedy  data/grimmerPressReleases/Kennedy/01Feb2007Ken...   \n",
       "40  Kennedy  data/grimmerPressReleases/Kennedy/01Jun2007Ken...   \n",
       "50  Kennedy  data/grimmerPressReleases/Kennedy/01Mar2007Ken...   \n",
       "60  Kennedy  data/grimmerPressReleases/Kennedy/01May2007Ken...   \n",
       "70  Kennedy  data/grimmerPressReleases/Kennedy/01Nov2007Ken...   \n",
       "80  Kennedy  data/grimmerPressReleases/Kennedy/02Aug2006Ken...   \n",
       "90  Kennedy  data/grimmerPressReleases/Kennedy/02Feb2005Ken...   \n",
       "\n",
       "                                                 text  \\\n",
       "0            FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "10           FOR IMMEDIATE RELEASE     Washington ...   \n",
       "20           FOR IMMEDIATE RELEASE      Fact sheet...   \n",
       "30           FOR IMMEDIATE RELEASE     Washington ...   \n",
       "40           FOR IMMEDIATE RELEASE  BOSTON  MA  Se...   \n",
       "50           FOR IMMEDIATE RELEASE     Washington ...   \n",
       "60           FOR IMMEDIATE RELEASE  The President ...   \n",
       "70           FOR IMMEDIATE RELEASE  Washington  DC...   \n",
       "80           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...   \n",
       "90           FOR IMMEDIATE RELEASE     The Preside...   \n",
       "\n",
       "                                      tokenized_sents  \\\n",
       "0   [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "10  [[FOR, IMMEDIATE, RELEASE, Washington, D, C, T...   \n",
       "20  [[FOR, IMMEDIATE, RELEASE, Fact, sheets, on, B...   \n",
       "30  [[FOR, IMMEDIATE, RELEASE, Washington, D, C, T...   \n",
       "40  [[FOR, IMMEDIATE, RELEASE, BOSTON, MA, Senator...   \n",
       "50  [[FOR, IMMEDIATE, RELEASE, Washington, DC, Tod...   \n",
       "60  [[FOR, IMMEDIATE, RELEASE, The, President, is,...   \n",
       "70  [[FOR, IMMEDIATE, RELEASE, Washington, DC, Sen...   \n",
       "80  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "90  [[FOR, IMMEDIATE, RELEASE, The, President, gav...   \n",
       "\n",
       "                                     normalized_sents  \n",
       "0   [[immediate, release, immediate, release, cont...  \n",
       "10  [[immediate, release, washington, c, today, se...  \n",
       "20  [[immediate, release, fact, sheets, bush, plan...  \n",
       "30  [[immediate, release, washington, c, today, u,...  \n",
       "40  [[immediate, release, boston, senator, edward,...  \n",
       "50  [[immediate, release, washington, dc, today, s...  \n",
       "60  [[immediate, release, president, wrong, veto, ...  \n",
       "70  [[immediate, release, washington, dc, senators...  \n",
       "80  [[immediate, release, immediate, release, impo...  \n",
       "90  [[immediate, release, president, gave, effecti...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [normlizeTokens(s, stopwordLst = stop_words_nltk, stemmer = None) for s in x])\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "We will be using the gensim implementation of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec).\n",
    "\n",
    "To load our data our data we give all the sentences to the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the word2vec object the words each have a vector. To access the vector directly, use the square braces (`__getitem__`) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63332963,  1.62318957, -0.47924739,  0.07098044, -0.45479971,\n",
       "        0.40752095, -0.50096846, -0.07689824,  1.77117872,  1.08833802], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V['president'][:10] #Shortening because it's very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the full matrix, `syn0` stores all the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.14442146e-01,  -1.34408742e-01,  -6.52093351e-01, ...,\n",
       "         -7.52535105e-01,  -4.10799444e-01,   8.67611110e-01],\n",
       "       [  5.93297482e-01,  -1.37054849e+00,   6.75345540e-01, ...,\n",
       "          7.82384217e-01,   8.24719787e-01,   8.95615891e-02],\n",
       "       [  1.07347357e+00,   1.11068761e+00,  -3.11134100e+00, ...,\n",
       "          9.96571407e-02,   1.58239529e-02,   1.40019035e+00],\n",
       "       ..., \n",
       "       [ -1.93243716e-02,   5.98129891e-02,   2.62835771e-02, ...,\n",
       "          3.65377069e-02,  -4.75026146e-02,  -3.23274098e-02],\n",
       "       [ -4.53822464e-02,  -2.21091677e-02,   1.64702665e-02, ...,\n",
       "          7.39889070e-02,  -6.29690140e-02,   3.01235300e-02],\n",
       "       [ -6.70455545e-02,   1.60855870e-03,   1.52522353e-02, ...,\n",
       "          8.36551841e-03,   8.87496863e-03,  -1.36727588e-02]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `index2word` lets you translate from the matrix to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.wv.index2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few things that come from the word vectors. The first is to find similar vectors (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('administration', 0.7875275611877441),\n",
       " ('presidents', 0.7459065318107605),\n",
       " ('administrations', 0.6830694675445557),\n",
       " ('george', 0.6143300533294678),\n",
       " ('cheney', 0.5935467481613159),\n",
       " ('ronald', 0.5507928133010864),\n",
       " ('responds', 0.5303289890289307),\n",
       " ('republican', 0.5133267641067505),\n",
       " ('gore', 0.5069808959960938),\n",
       " ('democrats', 0.5040360689163208)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wars', 0.6811537742614746),\n",
       " ('disobedience', 0.6585363745689392),\n",
       " ('afghanistan', 0.6442499160766602),\n",
       " ('unresisted', 0.6400057673454285),\n",
       " ('quagmire', 0.6329987049102783),\n",
       " ('battle', 0.6274481415748596),\n",
       " ('descending', 0.622592568397522),\n",
       " ('invasion', 0.6055372953414917),\n",
       " ('chaos', 0.5968828201293945),\n",
       " ('troop', 0.5751866698265076)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar('war')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word least matches the others within a word set (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'washington'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.doesnt_match(['administration', 'administrations', 'presidents', 'president', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word best matches the result of a semantic *equation* (here, we seek the words whose vectors best fit the missing entry from the equation: **X + Y - Z = _**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vetoed', 0.6914553642272949),\n",
       " ('bush', 0.6894548535346985),\n",
       " ('veto', 0.6585812568664551),\n",
       " ('bushs', 0.6451351046562195),\n",
       " ('blocked', 0.6428378820419312),\n",
       " ('signed', 0.6387166380882263),\n",
       " ('vetoes', 0.6350974440574646),\n",
       " ('sign', 0.6205121278762817),\n",
       " ('signature', 0.6123530268669128),\n",
       " ('proposing', 0.6040381193161011)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senReleasesW2V.most_similar(positive=['clinton', 'republican'], negative = ['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that **Clinton + Republican - Democrat = Bush**. In other words, in this dataset, **Clinton** is to **Democrat** as **Bush** is to **Republican**. Whoah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the vectors for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.save(\"data/senpressreleasesWORD2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dimension reduction to visulize the vectors. We will start by selecting a subset we want to plot. Let's look at the top words from the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWords = 50\n",
    "targetWords = senReleasesW2V.wv.index2word[:numWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract their vectors and create our own smaller matrix that preserved the distances from the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51444215, -0.13440874, -0.65209335, ..., -0.7525351 ,\n",
       "        -0.41079944,  0.86761111],\n",
       "       [ 0.59329748, -1.37054849,  0.67534554, ...,  0.78238422,\n",
       "         0.82471979,  0.08956159],\n",
       "       [ 1.07347357,  1.11068761, -3.111341  , ...,  0.09965714,\n",
       "         0.01582395,  1.40019035],\n",
       "       ..., \n",
       "       [-0.5765599 , -0.58712918,  0.83634526, ...,  1.93434584,\n",
       "         0.31177869,  0.66381794],\n",
       "       [ 1.07949841, -4.51863813, -0.71407819, ...,  0.02297206,\n",
       "         0.72050518, -0.54594749],\n",
       "       [-0.51721627, -0.63158733, -0.23220488, ...,  0.08494563,\n",
       "         0.21976326,  1.84752452]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(senReleasesW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use PCA to reduce the dimesions (e.g., to 50), and T-SNE to project them down to the two we will visualize. We note that this is nondeterministic process, and so you can repeat and achieve alternative projectsions/visualizations of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can plot the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAFUCAYAAADvbtLzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSEmSJHX0/wEI9UKCSQr2CQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119376510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, (tsneWords[:, 0][i],tsneWords[:, 1][i]), size =  20 * (numWords - i) / numWords)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My visualization above puts ``said`` next to ``congress`` and ``bill`` near ``act``. ``health`` is beside ``care`` and ``national`` abuts ``security``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a word2vec model with your corpus. Interrogate word relationships in the resulting space. Plot a subset of your words. What do these word relationships reveal about the *social* and *cultural game* underlying your corpus? What was surprising--what violated your prior understanding of the corpus? What was expected--what confirmed your knowledge about this domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instead of just looking at just how words embed within in the space, we can look at how the different documents relate to each other within the space. First lets load our data--abstracts of most U.S. physics papers from the 1950s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>copyrightYear</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950</td>\n",
       "      <td>10.1103/RevModPhys.22.221</td>\n",
       "      <td>A summarizing account is given of the research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.147</td>\n",
       "      <td>New tables of coulomb functions are presented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.185</td>\n",
       "      <td>Ionization by electron impact in diatomic gase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.203</td>\n",
       "      <td>It is shown that the conductivity in the ohmic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.21</td>\n",
       "      <td>The factorization method is an operational pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.311</td>\n",
       "      <td>A brief account is given of Dyson's proof of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.315</td>\n",
       "      <td>A systematics is given of all transitions for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.322</td>\n",
       "      <td>A systematics of the -transitions of even A nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1951</td>\n",
       "      <td>10.1103/RevModPhys.23.328</td>\n",
       "      <td>The available experiments on the absorption sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1952</td>\n",
       "      <td>10.1103/RevModPhys.24.108</td>\n",
       "      <td>The classical theory of the dynamics of viscou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   copyrightYear                        doi  \\\n",
       "0           1950  10.1103/RevModPhys.22.221   \n",
       "1           1951  10.1103/RevModPhys.23.147   \n",
       "2           1951  10.1103/RevModPhys.23.185   \n",
       "3           1951  10.1103/RevModPhys.23.203   \n",
       "4           1951   10.1103/RevModPhys.23.21   \n",
       "5           1951  10.1103/RevModPhys.23.311   \n",
       "6           1951  10.1103/RevModPhys.23.315   \n",
       "7           1951  10.1103/RevModPhys.23.322   \n",
       "8           1951  10.1103/RevModPhys.23.328   \n",
       "9           1952  10.1103/RevModPhys.24.108   \n",
       "\n",
       "                                            abstract  \n",
       "0  A summarizing account is given of the research...  \n",
       "1  New tables of coulomb functions are presented ...  \n",
       "2  Ionization by electron impact in diatomic gase...  \n",
       "3  It is shown that the conductivity in the ohmic...  \n",
       "4  The factorization method is an operational pro...  \n",
       "5  A brief account is given of Dyson's proof of t...  \n",
       "6  A systematics is given of all transitions for ...  \n",
       "7  A systematics of the -transitions of even A nu...  \n",
       "8  The available experiments on the absorption sp...  \n",
       "9  The classical theory of the dynamics of viscou...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apsDF = pandas.read_csv('data/APSabstracts1950s.csv', index_col = 0)\n",
    "apsDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load these as documents into Word2Vec, but first we need to normalize and pick some tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['photomagnetoelectric', 'quantum', 'boltzmann', 'proton', 'positron', 'feynman', 'classical', 'relativity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsDF['tokenized_words'] = apsDF['abstract'].apply(lambda x: nltk.word_tokenize(x))\n",
    "apsDF['normalized_words'] = apsDF['tokenized_words'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedDocs = []\n",
    "for index, row in apsDF.iterrows():\n",
    "    #Just doing a simple keyword assignment\n",
    "    docKeywords = [s for s in keywords if s in row['normalized_words']]\n",
    "    docKeywords.append(row['copyrightYear'])\n",
    "    docKeywords.append(row['doi']) #This lets us extract individual documnets since doi's are unique\n",
    "    taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "apsDF['TaggedAbstracts'] = taggedDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a Doc2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V = gensim.models.doc2vec.Doc2Vec(apsDF['TaggedAbstracts'], size = 100) #Limiting to 100 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get vectors for the tags/documents, just as we did with words. Documents are actually the centroids (high dimensional average points) of their words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs[1952]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words can still be accessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V['atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the ``most_similar`` command to perform simple semantic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['atom','electrons'], negative = ['electron'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. **Electron** is to **electrons** as **atom** is to **atoms**. Another way to understand this, developed below is: **electrons - electron** induces a singular to plural dimension, so when we subtract **electron** from **atom** and add **electrons**, we get **atoms**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['einstein','law'], negative = ['equation'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words **Einstein** minus **equation** plus **law** equals **Meissner**--Walthur Meissner studied mechanical engineering and physics ... and was more likely to produce a \"law\" than a \"equation\", like the Meissner effect, the damping of the magnetic field in superconductors. If we built our word-embedding with a bigger corpus like the entire arXiv, a massive repository of physics preprints, we would see many more such relationships like **gravity - Newton + Einstein = relativity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute all of these *by hand*--explicitly wth vector algebra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise.cosine_similarity(apsD2V['electron'].reshape(1,-1), apsD2V['positron'].reshape(1,-1))\n",
    "#We reorient the vectors with .reshape(1, -1) so that they can be computed without a warning in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the doc2vec model, the documents have vectors just as the words do, so that we can compare documents with each other and also with words (similar to how a search engine locates a webpage with a query). First, we will calculate the distance between a word and documents in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron'] ], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search for the first of these on the web (these are doi codes), we find the following...a pretty good match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"data/PhysRev.98.875.jpg\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go the other way around and find words most similar to this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar( [ apsD2V.docvecs['10.1103/PhysRev.98.875'] ], topn=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even look for documents most like a query composed of multiple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron']+apsD2V['positron']+apsD2V['neutron']], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some words and documents against one another with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrix = []\n",
    "for tagOuter in keywords:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrix.append(column)\n",
    "heatmapMatrix = np.array(heatmapMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmapMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrix, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrix.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrix.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(keywords, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDocs = apsDF['doi'][:10]\n",
    "\n",
    "heatmapMatrixD = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in targetDocs:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixD.append(column)\n",
    "heatmapMatrixD = np.array(heatmapMatrixD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents and our keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrixC = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixC.append(column)\n",
    "heatmapMatrixC = np.array(heatmapMatrixC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixC, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the model in case we would like to use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.save('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apsD2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a doc2vec model with your corpus. Interrogate document and word relationships in the resulting space. Construct a heatmap that plots the distances between a subset of your documents against each other, and against a set of informative words. Find distances between *every* document in your corpus and a word or query of interest. What do these doc-doc proximities reveal about your corpus? What do these word-doc proximities highlight? Demonstrate and document one reasonable way to select a defensible subset of query-relevant documents for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Score Function\n",
    "\n",
    "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. \n",
    "\n",
    "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_model  = gensim.models.word2vec.Word2Vec.load('data/resume.model') #Sorry we can't make this public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the vacabularies of this model by building a word-index map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = resume_model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's load a few job ads. Here, we only use a small sample of all of them. Uncomment this cell if you want to load more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/joblistings.merged.parsed.unique.grpbyyear.2010-2015.02.tsv','r') as tsv:\n",
    "#     ads = [line.strip().split('\\t') for line in tsv]\n",
    "    \n",
    "# adsDF = pandas.DataFrame(ads, columns = ads[0])\n",
    "# reducedDF = adsDF[['hiringOrganization_organizationName', 'jobDescription', 'jobLocation_address_region', 'jobLocation_geo_latitude', 'jobLocation_geo_longitude', 'qualifications', 'responsibilities']][1:]\n",
    "# N = reducedDF.shape[0]\n",
    "# indices = random.sample(range(1, N+1), 100)\n",
    "# sampleDF = reducedDF.iloc[indices]\n",
    "# sampleDF.to_csv('data/SampleJobAds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = pandas.read_csv('data/SampleJobAds.csv', index_col = False)\n",
    "#We need to convert the last couple columns from strings to lists\n",
    "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to every job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 job descriptions that have the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for phrases corresponding to job skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adprob([[\"python\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adprob([[\"basic\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic programming appears to be more likely in this pool of resumes than python programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean likelihood of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would increase the sample size if you want to do a more serious study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate the scores for a small sample of documents from outside your corpus to identify which are *closest* to your corpus. Then calculate the scores for a few phrases or sentences to identify the ones most likely to have appeared in your corpus. Interrogate patterns associated with these document/phrase scores (e.g., which companies produced job ads most or least likely to find jobseekers in the resume corpus?) What do these patterns suggest about the boundaries of your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also project word vectors to an arbitray semantic dimension. To demonstrate this possibility, let's first load a model trained with New York Times news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes_model = gensim.models.word2vec.Word2Vec.load_word2vec_format('data/nytimes_cbow.reduced.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can visualize with dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to create dimensions\n",
    "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', 'black','blacks','African', 'white', 'whites', 'Caucasian', 'rich', 'richer', 'richest', 'expensive', 'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
    "#words we will be mapping\n",
    "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
    "\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in tnytTargetWords:\n",
    "    wordsSubMatrix.append(nytimes_model[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaWordsNYT = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWordsNYT = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneWordsNYT[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
    "for i, word in enumerate(tnytTargetWords):\n",
    "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  20 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some convenient functions for getting dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate three dimensions: gender, race, and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = dimension(nytimes_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
    "Race = dimension(nytimes_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
    "Class = dimension(nytimes_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Occupations = [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\"]\n",
    "\n",
    "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
    "\n",
    "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to project words in a word list to each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDF(model, word_list):\n",
    "    g = []\n",
    "    r = []\n",
    "    c = []\n",
    "    for word in word_list:\n",
    "        g.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
    "        r.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
    "        c.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
    "    df = pandas.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCCdf = makeDF(nytimes_model, Occupations) \n",
    "Fooddf = makeDF(nytimes_model, Foods)\n",
    "Sportsdf = makeDF(nytimes_model, Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the occupational words in each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, OCCdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, OCCdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, OCCdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Fooddf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Fooddf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Fooddf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Sportsdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Sportsdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Sportsdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that identify semantic dimensions of interest from your data (e.g., gender: man-woman) and project words onto these dimensions. Plot the array of relevant words along each semantic dimension. Which words are most different. Which dimensions are most different? On which dimension are your words most different? Print three short textual examples from the corpus that illustrate the association you have explored.\n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Project documents from your corpus along a dimension of interest. Sample relevant documents from your corpus with this functionality and explain your rationale? Calculate the cosine of the angle between two dimensions (encoded as vectors) of interest. What does this suggest about the relationship between them within your corpus? \n",
    "\n",
    "<span style=\"color:red\">***Super stretch***: Create 90% bootstrap confidence intervals around your word projections onto a given dimension by generating 10 separate word2vec models, sampling $n$ documents (the total number in your corpus) for each, but with replacement. The bounds will be defined as the highest and lowest projection across your 10 samples. Which words are *significantly* different on your semantic dimension of interest?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
